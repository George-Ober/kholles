% Ceci est un fichier généré automatiquement.
% Ne pas le modifier directement. Exécuter $ make pour le générer.
% Il rassemble les questions de khôlles de toutes les semaines.

\documentclass{article}

\usepackage{kholles}
\usepackage{braket}

\renewenvironment{question_kholle}[2][ ]
{
	\subsection{\texorpdfstring{#2}{}}
	\notblank{#1}
	{
		\noindent #1
		\bigbreak
	}
	{}
	\begin{proof}
}
{
	\end{proof}
}
\renewcommand{\setnbquestion}[1]{
	\setcounter{subsection}{\numexpr#1-1\relax}
}

\begin{document}
\maketitle

\begin{abstract}
  Bienvenue très chers camarades sereins, ce document contient les questions de khôlles de mathématiques de la MP1 de Fermat. Il est coécrit par Kylian Boyet, George Ober, Hugo \textit{Vangi}lluwen (qui maintient la structure du projet, la compilation et le paquet kholles.sty) avec la contribution de Jérémie Menard.
  Il n'est malheureusement pas exhaustif. Si vous voulez nous aider, lisez CONTRIBUER.md et envoyez-nous votre code \LaTeX \ ou plus simplement dites-nous quand vous rencontrez une erreur.
\end{abstract}

\pagenumbering{roman}
\tableofcontents
\clearpage
\pagenumbering{arabic}

\pagebreak\section{Semaine 1}


\allowdisplaybreaks[4]
\begin{question_kholle}{Preuve formelle de la somme des entiers et des termes d'une suite géométrique}
  \begin{itemize}[label=$\lozenge$]
    \item Soit $n \in \mathbb{N}$ fixé quelconque. Posons $$S_{n} = \sum_{k=0}^{n}k$$
          En posant la symétrie d'indice $i = n-k$, on a aussi
          $$
            S_{n}= \sum_{i=0}^{n}(n-i)=\sum_{i=0}^{n}n - \sum_{i=0}^{n}i=(n \times \mathrm{card}[ \! [ 0, n ] \!]) - \sum_{i=0}^{n} i
          $$
          Or, puisque $\mathrm{card}[ \! [ 0, n ] \!] = n + 1$ et que $\sum_{i=0}^{n} i = S_{n}$
          $$
            S_{n} = n \times (n+1) + S_{n}
          $$
          Donc
          $$
            S_{n} = \frac{n(n+1)}{2}
          $$
    \item Soient $q \in \mathbb{R}$ , $k \in \mathbb{N}$ fixés quelconques.
          \begin{itemize}[label=$\star$]
            \item Si $q = 1$,
                  $$
                    \sum_{i=0}^{k}q^{i} = \sum_{i=0}^{k}1 = k+1
                  $$
            \item Sinon, avec l'identité algébrique, on a
                  $$
                    q^{k+1}-1^{k+1} = (q-1) \sum_{i=0}^{k}q^{i}\times 1^{k-i}
                  $$
                  Ainsi, puisque $q \neq 1$ on a, par multiplication par $(q-1)^{-1}$
                  $$
                    \sum_{i=0}^{k}q^{i} = \frac{q^{k+1}-1}{q-1}
                  $$
                  Nous avons donc établi que
                  $$
                    \sum_{i=0}^{k} q^{i} = \left\{ \begin{array}{ll}
                      \displaystyle\frac{1-q^{k+1}}{1-q} & \text{ si } q \neq 1 \\
                      k + 1                              & \text{ sinon}
                    \end{array}\right.
                  $$
          \end{itemize}
  \end{itemize}
\end{question_kholle}

\pagebreak
\begin{question_kholle}{Preuve de la factorisation de $a^n - b^n$ puis de celle de $a^{2m+1} + b^{2m+1} $}
  Soient $(a,b)\in\C^2$ et $n\in\N$ fixés quelconques.

  \begin{align*}
    (a-b)\sum_{k=0}^{m-1}a^{k}b^{m-1-k}
     & =a \sum_{k=0}^{m-1}a^{k}b^{m-1-k} -b \sum_{k=0}^{m-1}a^{k}b^{m-1-k} \\
     & = \sum_{k=0}^{m-1}a^{k+1}b^{m-1-k} - \sum_{k=0}^{m-1}a^{k}b^{m-k}   \\
  \end{align*}
  Si bien qu'en posant le changement d'indice $j = k + 1$ on reconnait le téléscopage.
  $$
    \sum_{j=1}^{m}a^{j}b^{m-j} - \sum_{k=0}^{m-1}a^{k}b^{m-k} = a^{m} - b ^{m}
  $$
  Soit $m$ un entier naturel fixé quelconque. En particularisant la relation pour $n\leftarrow 2m+1$ et $b\leftarrow (-b)$, on obtient
  \begin{align*}
    a^{2m+1} - (-b)^{2m+1} = a^{2m+1} + b^{2m+1} & = (a-(-b))\sum_{k=0}^{2m}{a^k (-b)^{2m-k}}             \\
                                                 & = (a+b)\sum_{k=0}^{2m}{a^k (-1)^{2m}(-1)^{-k}b^{2m-k}} \\
                                                 & = (a+b)\sum_{k=0}^{2m}{(-1)^k a^k b^{2n-k}}
  \end{align*}
\end{question_kholle}

\begin{question_kholle}[{Pour tout $(a, b) \in \C^2$, $n \in \N$: $$(a+b)^{n} = \sum_{k=0}^{n}\binom n k a^{k} b^{n-k}$$}]{Preuve de la formule du binôme de Newton}
  Soient $(a, b) \in \mathbb{C}^{2}$ fixés quelconques.
  Posons le prédicat $\prop(\cdot)$ défini pour tout $n \in \mathbb{N}$ par
  $$
    \prop(n): (a+b)^{n} = \sum_{k=0}^{n}\binom n k a^{k} b^{n-k}
  $$
  \begin{itemize}[label=$\star$]
    \item Initialisation, $n \leftarrow 0$
          D'une part $(a+b)^{0} = 0$, même si les deux sont nuls (par convention $0^{0} =0$)
          D'autre part
          $$
            \sum_{k=0}^{0}\binom 0 k a^{k}b^{n-k} = \binom 0 0 a^{0} b^{0} = 0
          $$
          Donc $\prop(0)$ est vérifée.

    \item Soit $n \in \mathbb{N}$ fixé quelconque tel que $\prop(n)$ est vraie

          \begin{align*}
            (a+b)^{n+1}                                 & = (a+b)\times(a+b)^{n}                                                                                        \\
                                                        & = (a+b) \times \sum_{k=0}^{n}\binom n k a^{k} b^{n-k}                                                         \\
                                                        & = a\sum_{k=0}^{n}\binom n k a^{k} b^{n-k} + b\sum_{k=0}^{n}\binom n k a^{k} b^{n-k}                           \\
                                                        & = \sum_{k=0}^{n}\binom n k a^{k+1} b^{n-k} + \sum_{k=0}^{n}\binom n k a^{k} b^{n+1-k}                         \\
                                                        & = \sum_{j=1}^{n+1}\binom n {j-1} a^{j} b^{n+1-j} + \sum_{k=0}^{n}\binom n k a^{k} b^{n+1-k}                   \\
            (\text{en posant } j = k + 1)               & = a^{n+1} + \sum_{j=1}^{n}\binom n {j-1} a^{j} b^{n+1-j} + \sum_{k=1}^{n}\binom n k a^{k} b^{n+1-k} + b^{n+1} \\
                                                        & = a^{n+1} + \left( \sum_{k=1}^{n} \left(\binom n {k-1} + \binom n k \right) a^{k} b^{n+1-k} \right) + b^{n+1} \\
            (\text{en utilisant la relation de Pascal}) & = a^{n+1} + \sum_{k=1}^{n} \binom {n+1} k a^{k} b^{n+1-k} + b^{n+1}                                           \\
                                                        & =\sum_{k=0}^{n+1} \binom {n+1} k a^{k} b^{n+1-k}
          \end{align*}

          Donc $\prop(n+1)$ est vraie.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[
  {$$\left( \sum_{k=1}^{n}x_{k} \right)^{2} = \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j} = 2 \sum_{1\leqslant k < j \leqslant n} x_{k}x_{j}+\sum_{k=1}^{n}x_{i}^{2}$$}
  ]{Développement d'une somme}

  \begin{align*}
    \left( \sum_{k=0}^{n}x_{k} \right)^{2}
     & = \left( \sum_{k=1}^{n} x_{k} \right) \times \left( \sum_{j=1}^{n} x_{j} \right) \\
     & = \sum_{k=1}^{n}\left[ x_{k} \times \sum_{j=1}^{n}x_{j} \right]                  \\
     & = \sum_{k=1}^{n}\left( \sum_{j=1}^{n} x_{k}\times x_{j} \right)                  \\
     & = \sum_{\substack{1\leqslant k \leqslant n                                       \\ 1 \leqslant j \leqslant n}} x_{k}x_{j}
  \end{align*}


  On peut aussi séparer cette somme

  \begin{align*}
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n}} x_{k}x_{j}
                                                                                                          & =
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} &+
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n\\k=j}} x_{k}x_{j} &+
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}\\
                                                                                                          & =
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} & +
    \underbrace{ \sum_{k=1}^{n}x_{k}^{2} }_{ \text{somme sur les indices }(k,j) \text{ tels que } k = j } & +
    \sum_{\substack{1\leqslant k \leqslant n                                                                  \\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}
  \end{align*}

  On remarque aussi qu'en permutant les indices des deux sommes (les variables sont muettes)
  $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} = \sum_{\substack{1\leqslant j \leqslant n\\ 1 \leqslant k \leqslant n\\j<k}} x_{j}x_{k}
  $$
  Qui, par commutativité du produit dans $\mathbb{C}$ nous donne cette égalité
  $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} =
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}
  $$
  On a donc bien l'identité attendue :
  $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j} = 2 \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} +
    \sum_{k=1}^{n}x_{k}^{2}
  $$
\end{question_kholle}

\begin{question_kholle}{Montrer que tout entier $n > 2$ admet un diviseur premier}
  Raisonnons par récurrence forte avec la propriété $\prop(\cdot)$ définie pour tout $n > 2$ par
  $$
    \prop(n) : \frquote{\forall k \in [ \! [ 2, n ] \!], k \text{ admet un diviseur premier}}
  $$
  \begin{itemize}
    \item Initialisation: $n \leftarrow 2$

          Soit $k \in [ \! [ 2, 2 ] \!]$ fixé quelconque. Nécéssairement, $k = 2$.
          or, $2$ admet $2$ pour diviseur premier.

          Donc $\forall k \in [ \! [ 2, 2 ] \!], k \text{ admet un diviseur premier}$, ce qui prouve $\prop(2)$.

    \item Hérédité: Soit $n \in \mathbb{N} \setminus \{ 1, 0 \}$ fixé quelconque tel que $\prop(n)$ est vraie.

          Pour montrer $\prop(n+1)$, il nous faudra montrer que $\forall k \in [ \! [ 2, n+1 ] \!], k \text{ admet un diviseur premier }$

          Soit $k \in [ \! [ 2, n+1 ] \!]$ fixé quelconque.
          \begin{itemize}[label=$\star$]
            \item    Si $k \in [ \! [ 2, n ] \!]$, alors la véracité de $\prop(n)$ nous permet de conclure, et de dire que k admet un diviseur premier.

            \item Sinon $k = n + 1$
                  \begin{itemize}[label=$\lozenge$]
                    \item Si $n+1$ est premier, alors il admet $k$ comme diviseur premier
                    \item Sinon, $\exists d \in [ \! [ 2, n ] \! ]: d \mid n+1$

                          Mais, puisque $d \in [ \! [ 2, n ] \! ]$, la véracité de $\prop(n)$ nous permet d'affirmer que $d$ admet un diviseur premier $p$. Donc par transitivité de la relation de divisibilité $$(p \mid d \text{ et } d \mid n) \implies p \mid n$$
                  \end{itemize}
          \end{itemize}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer par récurrence qu'une fonction polynomiale à coefficients réels est nulle si et seulement si tous ses coefficients sont nuls}
  Considérons le prédicat $\prop(\cdot)$ défini pour tout $n \in \mathbb{N}$
  $$
    \prop(n) : \text{toute fonction polynômiale identiquement nulle sur }\mathbb{R} \text{ a tous ses coefficients nuls}
  $$
  Autrement dit
  $$
    \prop(n) : \forall(a_{0}, \dots ,a_{n}) \in \mathbb{R}^{n+1}\left( \forall x \in \mathbb{R}, \sum_{k=0}^{n}a_{k}x^{k} = 0 \right) \implies \forall k \in [ \! [ 0, n ] \!], a_{k} = 0
  $$
  \begin{itemize}[label=$\lozenge$]
    \item Pour $n \leftarrow 0$
          Soit $a_{0} \in \mathbb{R}$ fixé quelconque tel que $\forall x \in \mathbb{R}, a_{0}x^{0} = 0$ Alors $a_{0} = 0$

    \item Soit $n \in \mathbb{N}$ fixé quelconque tel que $\prop(n)$ est vraie
          Soient $(a_{0},\dots, a_{n+1}) \in \mathbb{R}^{n+2}$
          Posons $Q(x) = \sum_{k=0}^{n+1}a_{k}x^{k}$ tel que $\forall x \in \mathbb{R}, Q(x) = 0$
          D'une part
          $$
            \forall x \in \R, \underbrace{ Q(2x) }_{ =0 } - 2^{n+1}\underbrace{ Q(x) }_{ =0 } = 0
          $$
          D'autre part

          \begin{align*}
            \forall x \in \mathbb{R}, Q(2x) - 2^{n+1}Q(x) & = \sum_{k=0}^{n+1}a_{k}(2x)^{k}-2^{n+1}\sum_{k=0}^{n+1}a_{k}x^{k} \\
                                                          & = \sum_{k=0}^{n+1}a_{k}(2^{k}-2^{n+1})x^{k}
          \end{align*}

          Le terme d'indice $n+1$ s'annule, si bien que l'on peut écrire
          $$\forall x \in \mathbb{R}, Q(2x) - 2^{n+1}Q(x) = \sum_{k=0}^{n}a_{k}(2^{k}-2^{n+1})x^{k}$$
          Qui est une fonction polynômiale de degré $\leqslant n$, ce qui permet d'appliquer $\prop(n)$ pour $(a_{k})_{k \in[ \! [ 0, n ] \!]} \leftarrow (a_{k}(2^{k}-2^{n+1}))_{k \in[ \! [ 0, n ] \!]}$.

          Donc $\forall x \in [ \! [ 0, n ] \!]: a_{k}(2^{k}-2^{n+1}) = 0$ et puisque $2^{k}-2^{n+1} \neq 0$, on en déduit que $$\forall k \in [ \! [ 0, n ] \!], a_{k} = 0$$

          L'expression de $Q$ devient:
          $$
            \forall x \in \mathbb{R}, \underbrace{ \sum_{k=0}^{n}a_{k}x^{k} }_{ =0 }+ a_{n+1}x^{n+1} = 0
          $$
          Donc en particularisant pour $x\leftarrow 1$, on en déduit que $a_{n+1} = 0$

          Donc $\prop(n+1)$ est vraie.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer par analyse/synthèse qu'une fonction réelle d'une variable réelle s'écrit de manière unique comme somme d'une fonction paire et d'une fonction impaire}
  Soit $f \in \mathcal{F}(\mathbb{R}, \mathbb{R})$ fixée quelconque.
  \begin{itemize}[label=$\lozenge$]


    \item \underline{Analyse}:
          Supposons que $f : \mathbb{R} \to \mathbb{R}$ se décompose de manière unique en $f = g+h$ avec $g$ paire et $h$ impaire (i.e. $\forall x \in \mathbb{R}, g(-x)= g(x)$ et $h(-x)= -h(x)$).
          Soit $x \in \mathbb{R}$ fixé quelconque
          Calculons $f(-x)$:
          $$f(-x) = g(-x) + h(-x) = g(x) -h(x)$$
          Par demi somme, nous avons donc
          $$
            \left\{ \begin{array}{ll}
              2g(x)  = f(x)+f(-x) \\
              2h(x) = f(x)-f(-x)
            \end{array}\right.
          $$
          Ainsi, si une telle décomposition existe, c'est
          $$
            \left\{ \begin{array}{ll}
              g : x \mapsto \frac{f(x)+f(-x)}{2} \\
              h : x \mapsto \frac{f(x)-f(-x)}{2}
            \end{array}\right.
          $$

    \item \underline{Synthèse}:
          Posons

          \begin{align}
            g\left|\begin{array}{ll} \mathbb{R} &\to \mathbb{R} \\ x &\mapsto \frac{f(x)+f(-x)}{2} \end{array}\right.
            \text{ et } h \left|\begin{array}{ll} \mathbb{R} &\to \mathbb{R} \\ x &\mapsto \frac{f(x)-f(-x)}{2} \end{array}\right.
          \end{align}


          Remarquons, d'une part que:
          $$
            \forall x \in \mathbb{R}, g(x)+h(x) = \frac{f(x)+f(-x)}{2} + \frac{f(x)-f(-x)}{2} = f(x)
          $$
          Vérifions si les fonctions $g$ et $h$ vérifient les conditions de parité:
          $$
            \forall x \in \mathbb{R}, g(-x) = \frac{f(-x)+f(-(-x))}{2}= \frac{f(x)+f(-x)}{2} = g(x) \text{ ainsi } g \text{ est paire.}
          $$
          $$
            \forall x \in \mathbb{R}, h(-x) = \frac{f(-x)-f(-(-x))}{2} = -\frac{f(x)-f(-x)}{2} = -h(x) \text{ ainsi } h \text{ est impaire}
          $$
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[]{Illustration graphique de certaines identités trigonométriques}
  \hfill\\
  \begin{minipage}{0.5\textwidth}
      
    \begin{figure}[H]
      \centering
    
      \begin{tikzpicture}
        
        \draw[thick] (0,0) circle (2);
        
        
        \draw[->] (-2.5,0) -- (2.5,0) node[right] {$x$};
        \draw[->] (0,-2.5) -- (0,2.5) node[above] {$y$};
        

        \def\angle{30}
        \def\compAngle{60}
        

        \draw[thick, ->, blue] (0,0) -- ({2*cos(\angle)}, {2*sin(\angle)});

        

        \draw[thick, ->, green!60!black] (0,0) -- ({2*sin(\angle)}, {2*cos(\angle)});

        
        
        \draw[thick,->, blue] (0.5,0) arc (0:\angle:0.5);
        \node[blue] at (0.7,0.2) {$x$};
        
        \draw[->, thick, green!60!black] (1,0) arc (0:\compAngle:1);
        \node[green!60!black] at (1.4,0.3) {$\frac \pi 2 - x$};
        
        \draw[thick,->,  blue] (0,0.5) arc (90:\compAngle:0.5);
        \node[blue] at (0.2,0.7) {$x$};
        
        \filldraw[blue] ({2*cos(\angle)}, {2*sin(\angle)}) circle (2pt);
        \filldraw[green!60!black] ({2*sin(\angle)}, {2*cos(\angle)}) circle (2pt);
        
        \draw[thick, ->, red] (-0.1, 0) -- (-0.1, {2*cos(\angle)}) node[pos=0.3, left]{$\sin(\frac \pi 2 - x)$};
        \draw[thick, ->, red] (0, -0.1) -- ({2*cos(\angle)}, -0.1) node[pos=0.5, below]{$\cos x$};

        \draw[dotted] (0, 0) -- (2, 2);

      \end{tikzpicture}
      \caption{Illustration de $\sin (\frac \pi 2 -  x) = \cos x$. On retrouve l'égalité par réflexion sur la première bissectrice.}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
      
    \begin{figure}[H]
      \centering
    
      \begin{tikzpicture}

        \draw[thick] (0,0) circle (2);
        

        \draw[->] (-2.5,0) -- (2.5,0) node[right] {$x$};
        \draw[->] (0,-2.5) -- (0,2.5) node[above] {$y$};
        

        \def\angle{30} 
        \def\compAngle{210}
        

        \draw[thick, ->, blue] (0,0) -- ({2*cos(\angle)}, {2*sin(\angle)});

        

        \draw[thick, ->, green!60!black] (0,0) -- ({-2*cos(\angle)}, {-2*sin(\angle)});

        
        
        \draw[thick,->, blue] (0.5,0) arc (0:\angle:0.5);
        
        \draw[dotted] (0, {2*sin(\angle)}) -- ({2*cos(\angle)}, {2*sin(\angle)});
        \draw[dotted] (0, {-2*sin(\angle)}) -- ({-2*cos(\angle)}, {-2*sin(\angle)});
        
        \draw[->, thick, green!60!black] (1.5,0) arc (0:\compAngle:1.5);
        
        
        
        
        \filldraw[blue] ({2*cos(\angle)}, {2*sin(\angle)}) circle (2pt) node[above right] {$x$};
        \filldraw[green!60!black] ({-2*cos(\angle)}, {-2*sin(\angle)}) circle (2pt)node[below left] {$x + \pi$};
        
        \draw[thick, ->, red] (-0.1, 0) -- (-0.1, {2*sin(\angle)}) node[pos=0.3, left]{$\sin x$};
        \draw[thick, ->, red] (0.1, 0) -- (0.1, -{2*sin(\angle)}) node[pos=0.5, right]{$-\sin x$};

      \end{tikzpicture}
      \caption{Illustration de $\sin (x + \pi) = - \sin x$. On retrouve l'égalité par symétrie centrale.}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
      
    \begin{figure}[H]
      \centering
    
      \begin{tikzpicture}

        \draw[thick] (0,0) circle (2);
        

        \draw[->] (-2.5,0) -- (2.5,0) node[right] {$x$};
        \draw[->] (0,-2.5) -- (0,2.5) node[above] {$y$};
        \draw[thick] (2, 3) -- (2, -3);

        \def\angle{45}
        \def\compAngle{-45}
        

        \draw[dotted, blue] (0,0) -- ({2}, {2*tan(\angle)});
        \draw[thick, ->, blue] (0,0) -- ({2*cos(\angle)}, {2*sin(\angle)});
        
        
        
        \draw[dotted, green!60!black] (0,0) -- ({2}, {-2*tan(\angle)});
        \draw[thick, ->, green!60!black] (0,0) -- ({2*cos(\angle)}, {-2*sin(\angle)});

        
        
        \draw[thick,->, blue] (0.5,0) arc (0:\angle:0.5) node[pos=0.5, right] {$x$};
        
        
        \draw[->, thick, green!60!black] (0.5,0) arc (0:\compAngle:0.5) node[pos=0.5, right] {$-x$};
        
        
        
        
        \filldraw[blue] (2, {2*tan(\angle)}) circle (2pt) node[above right] {$\tan x$};
        \filldraw[green!60!black] (2, {-2*tan(\angle)}) circle (2pt) node[below right] {$\tan (-x)$};
        
        

      \end{tikzpicture}
      \caption{Illustration de $\tan (-x) = - \tan x$. On retrouve l'égalité par réflexion sur l'axe des abscisses.}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
      
    \begin{figure}[H]
      \centering
    
      \begin{tikzpicture}
        
        \draw[thick] (0,0) circle (2);
        
        
        \draw[->] (-2.5,0) -- (2.5,0) node[right] {$x$};
        \draw[->] (0,-2.5) -- (0,2.5) node[above] {$y$};
        

        \def\angle{30} 
        \def\compAngle{120}
        

        \draw[thick, ->, blue] (0,0) -- ({2*cos(\angle)}, {2*sin(\angle)});

        

        \draw[thick, ->, green!60!black] (0,0) -- ({2*cos(\compAngle)}, {2*sin(\compAngle)});

        
        
        \draw[thick,->, blue] (0.5,0) arc (0:\angle:0.5) node[pos=0.5, right] {$x$};
        
        \draw[dotted] (0, {2*sin(\angle)}) -- ({2*cos(\angle)}, {2*sin(\angle)});
        \draw[dotted] ({2*cos(\compAngle)}, {2*sin(\compAngle)}) -- ({2*cos(\compAngle)}, 0);
        
        \draw[->, thick, green!60!black] (1.5,0) arc (0:\compAngle:1.5) node[pos=1, above right] {$x + \frac \pi 2$};
        
        
        
        
        \filldraw[blue] ({2*cos(\angle)}, {2*sin(\angle)}) circle (2pt) node[above right] {$x$};
        \filldraw[green!60!black] ({2*cos(\compAngle)}, {2*sin(\compAngle)}) circle (2pt)node[above left] {$x + \frac \pi 2$};
        
        \draw[thick, ->, red] (0, 0) -- (0, {2*sin(\angle)}) node[pos=0.7, right]{$\sin x$};
        \draw[thick, ->, red] (0, 0) -- ({2*cos(\compAngle)},0) node[pos=0.9, below]{$\cos (x + \frac \pi 2)$};
        
        \draw[dotted] (0, 0) -- (-2, 2);

      \end{tikzpicture}
      \caption{Illustration de $\cos (x + \frac \pi 2) = - \sin x$. On retrouve l'égalité par réflexion sur la deuxième bissectrice.}
    \end{figure}
  \end{minipage}
\end{question_kholle}

\begin{question_kholle}{Technique de résolution des équations trigonométriques du type $A \cos x + B \sin x = C$}
  Étudions l'équation d'inconnue $x$

  $$
    A \cos x + B \sin x = C
  $$
  \begin{itemize}[label=$\star$]
    \item Si $A = 0$ et $B = 0$
          \begin{itemize}[label=$\lozenge$]
            \item Si $C = 0$ l'équation admet $\mathbb{R}$ pour ensemble de solutions
            \item Sinon, l'équation n'admet pas de solutions
          \end{itemize}
    \item Sinon,

          Factorisons par $\sqrt{ A^{2}+B^{2} }$ (ce qui a un sens car $(A, B) \neq (0, 0) \implies \sqrt{ A^{2}+ B^{2} } \neq 0$)

          $$
            \frac{A}{\sqrt{ A^{2}+B^{2} }}\cos x + \frac{B}{\sqrt{ A^{2}+B^{2} }} \sin x = \frac{C}{ \sqrt{ A^{2}+B^{2} }}
          $$
          Le nombre complexe $\frac{A}{\sqrt{ A^{2}+B^{2}} }+i \frac{B}{\sqrt{ A^{2} +B^{2}}}$ est de module $1$, donc $\exists \varphi \in \mathbb{R}$ tel que

          $$
            e^{i\varphi} = \underbrace{ \frac{A}{\sqrt{ A^{2}+B^{2}} } }_{ \cos \varphi }+i \underbrace{ \frac{B}{\sqrt{ A^{2} +B^{2}}} }_{ \sin \varphi }
          $$

          Ainsi,
          $$
            (\cos \varphi \cos x-\sin \varphi \sin x) = \frac{C}{ \sqrt{ A^{2}+B^{2} }}
          $$
          donc
          $$
            \cos(\varphi+x) = \frac{C}{\sqrt{ A^{2}+B^{2} }}
          $$
          \begin{itemize}[label=$\lozenge$]
            \item Si $\displaystyle \frac{C}{\sqrt{ A^{2}+B^{2} }}\leqslant 1$

                  \begin{align*}
                    \cos(\varphi+x) = \frac{C}{\sqrt{ A^{2}+B^{2} }} & \iff
                    \left\{ \begin{array}{ll}
                              \phi + x \equiv \arccos\frac{C}{\sqrt{ A^{2}+B^{2} }}  [2\pi] \\
                              \text{ou}                                                     \\
                              \phi + x \equiv - \arccos\frac{C}{\sqrt{ A^{2}+B^{2} }} [2\pi]
                            \end{array}\right.                                                                                                                                                                                                 \\
                                                                     & \iff x \in \begin{array}{ll}\left\{ \arccos \frac{C}{\sqrt{ A^{2}+B^{2} }} + 2k\pi \mid k \in \mathbb{Z} \right\} \\ \cup \\ \left\{- \arccos \frac{C}{\sqrt{ A^{2}+B^{2} }} + 2k\pi \mid k \in \mathbb{Z} \right\}
                                                                                  \end{array}
                  \end{align*}


            \item Sinon, l'équation n'admet aucune solution
          \end{itemize}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Étude complète de la fonction tangente, tracé du graphe et en déduire celui de cotangente.}
\end{question_kholle}

\begin{question_kholle}{Expression de $\sin \theta$, $\cos \theta$, $\tan \theta$ en fonction de $\tan \frac \theta 2$}
  Soit $\theta \in \mathbb{R} \setminus \pi \mathbb{Z}$.
  Posons $\displaystyle u = \tan \frac{\theta}{2}$
  \begin{itemize}[label=$\lozenge$]
    \item $\displaystyle \tan \theta = \frac{2u}{1-u^{2}}$

          En utilisant la formule classique de trigonométrie

          $$\tan (a+b) = \frac{\tan a+\tan b}{1-\tan a\tan b}$$

          On obtient, avec $(a,b) \leftarrow (\frac{\theta}{2}, \frac{\theta}{2})$
          $$
            \tan \theta = \frac{2 \tan \frac{\theta}{2}}{1-\tan ^{2} \frac{\theta}{2}}=\frac{2u}{1-u^{2}}
          $$


    \item $\displaystyle \cos \theta = \frac{1-u^{2}}{1+u^{2}}$
          \begin{align*}
            \cos \theta & = 2\cos ^{2} \frac{\theta}{2} -1             \\
                        & = \frac{2}{1+\tan ^{2} \frac{\theta}{2}} - 1 \\
                        & = \frac{2}{1+u^{2}}-1                        \\
                        & = \frac{1-u^{2}}{1+u^{2}}
          \end{align*}

    \item $\displaystyle \sin \theta \frac{2u}{1+u^{2}}$

          \begin{align*}
            \sin \theta & = \cos \theta \tan \theta                           \\
                        & = \frac{1-u^{2}}{1+u^{2}} \times \frac{2u}{1-u^{2}} \\
                        & = \frac{2u}{1+u^{2}}
          \end{align*}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Preuve des formules du type $\cos p + \cos q = \dots$}
  Partons des formules d'addition

  $$
    \cos (a+b) = \cos a\cos b-\sin a\sin b\\
  $$
  $$
    \cos (a-b) = \cos a\cos b+\sin a\sin b
  $$

  \begin{align*}
    \cos(a+b) + \cos (a-b) = 2\cos a\cos b
    \tag{$\spadesuit$}\label{S1:Q12:1}
  \end{align*}


  Si bien qu'en posant
  $$
    \left\{ \begin{array}{ll}
      p = a+b \\
      q= a-b
    \end{array}\right.
    \iff
    \left\{ \begin{array}{ll}
      a = \frac{p+q}{2} \\
      b= \frac{p-q}{2}
    \end{array}\right.
  $$
  D'où, en injectant dans \eqref{S1:Q12:1}
  $$
    \cos p + \cos q = 2\cos \frac{p+q}{2} \cos \frac{p-q}{2}
  $$

\end{question_kholle}
\pagebreak\section{Semaine 2}

\begin{question_kholle}{Montrer qu'une composée d'applications inj/surj/bij est inj/surj/bij}
  Soient $u : E \to F$ et $v: F \to G$
  \begin{itemize}[label = $\lozenge$]
    \item Supposons $u$ et $v$ injectives.

          Soient $(x_{1}, x_{2}) \in E^{2}$ fq tels que $(v \circ u) (x_{1}) =(v\circ u)(x_{2})$.

          Alors $v(u(x_{1})) = v(u(x_{2}))$, mais $v$ est injective donc, $u(x_{1})= u(x_{2})$ mais $u$ est injective donc $x_{1} = x_{2}$.
          Ainsi $v \circ u$ est injective.
    \item Supposons $u$ et $v$ surjectives
          Soit $y \in G$ fixé quelconque.

          $v$ est surjective donc $\exists t \in F : v(t) = y$

          $u$ est surjective, donc $\exists x \in E : u(x) = t$
          Ainsi,
          $$
            (v \circ u) (x) = v(u(x)) = v(t) = y
          $$
          Donc $v \circ u$ est surjective.

    \item Supposons $u$ et $v$ bijectives.

          Le fait que $v \circ u$ est une bijection est une conséquence des deux points précédents.

  \end{itemize}

\end{question_kholle}
\begin{question_kholle}{Montrer que, si $u$ est une application de $E$ dans $F$, si $v$ est une application de $F$ dans $E$ telle que $v \circ u = \mathrm{Id}_E$ et $u \circ v = \mathrm{Id}_F$ alors $u$ est bijective ($v$ aussi) et sa bijection réciproque est $v$}
  Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, E)$ qui satisfont les conditions de l'énnoncé.
  \begin{itemize}[label=$\lozenge$]
    \item $u$ est injective

          Soient $(x_{1}, x_{2}) \in E^{2}$ fixés quelconques tels que $u(x_{1}) = u(x_{2})$. Alors $v(u(x_{1})) = v(u(x_{2}))$. Donc $x_{1} = x_{2}$ puisque $v \circ u = \mathrm{Id}_{E}$

    \item $u$ est surjective
          Soit $y \in F$ fixé quelconque. Posons $t = v(y)$. Ainsi, $u(t) = u(v(y))= y$ car $u \circ v = \mathrm{Id}_{F}$


  \end{itemize}
  Ainsi, $u$ est bijective, notons $u^{-1}$ sa bijection réciproque
  \begin{align*}
    u^{-1} \circ (u \circ v )    & = (u^{-1} \circ u) \circ v \\
    u^{-1} \circ \mathrm{Id}_{F} & = \mathrm{Id}_{E}\circ v   \\
    u^{-1}                       & = v
  \end{align*}

\end{question_kholle}

\begin{question_kholle}{Montrer que $v \circ u$ injective implique $u$ injective + montrer que cela n'implique pas $v$ injective.}
  Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, G)$.

  Supposons $v \circ u$ est injective
  Soient $(x_1, x_2) \in E^2$ fixés quelconques tels que $u(x_1) = u(x_2)$.
  Composons par $v$ à gauche : $v \circ u ( x_1 ) = v \circ u (x_2)$
  Puisque $v \circ u$ est injective, cela implique que $x_1 = x_2$.


  \begin{figure}[!h]
    \centering
    \begin{tikzpicture}[ele/.style={fill=black,circle,minimum width=.8pt,inner sep=1pt},every fit/.style={ellipse,draw,inner sep=-2pt}]
      \node[label=above:$E$] at (0, 4.5) {};
      \node[label=above:$F$] at (4, 4.5) {};
      \node[label=above:$G$] at (8, 4.5) {};

      \node[label=above:$u$] at (2, 4) {};
      \node[label=above:$v$] at (6, 4) {};


      \node[ele,label=left:$a$] (a1) at (0,4) {};
      \node[ele,label=left:$b$] (a2) at (0,3) {};

      \node[ele,,label=below:$1$] (b1) at (4,4) {};
      \node[ele,,label=above:$2$] (b2) at (4,3) {};
      \node[ele,,label=above:$3$] (b3) at (4,2) {};

      \node[ele,,label=right:$\star$] (c1) at (8,4) {};
      \node[ele,,label=right:$\triangle$] (c2) at (8,3) {};

      \node[draw,fit= (a1) (a2) ,minimum width=2cm] {} ;
      \node[draw,fit= (b1) (b2) (b3) ,minimum width=2cm] {} ;
      \node[draw,fit= (c1) (c2) ,minimum width=2cm] {} ;

      \draw[->,thick,shorten <=2pt,shorten >=2pt] (a1) -- (b1);
      \draw[->,thick,shorten <=2pt,shorten >=2] (a2) -- (b2);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b1) -- (c1);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b2) -- (c2);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b3) -- (c2);
    \end{tikzpicture}
  \end{figure}
  Ici, $v \circ u$ est injective, on a montré que cela impliquait $u$ injective. Pourtant, $v$ n'est pas injective.

\end{question_kholle}

\begin{question_kholle}{Montrer que $v \circ u$ surjective implique $v$ surjective + montrer que cela n'implique pas $u$ surjective.}
  Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, G)$.

  Supposons $v \circ u$ est surjective
  Soit $y \in G$ fixé quelconque.
  Puisque $v \circ u$ est surjective, $\exists x \in E : (v \circ u)(x) = y$
  Donc $v(u(x)) = y$.
  Donc, en posant $t = u(x)$, on a $v(t) = y$.
  Ainsi, $v$ est surjective.

  \begin{figure}[!h]
    \centering
    \begin{tikzpicture}[ele/.style={fill=black,circle,minimum width=.8pt,inner sep=1pt},every fit/.style={ellipse,draw,inner sep=-2pt}]
      \node[label=above:$E$] at (0, 4.5) {};
      \node[label=above:$F$] at (4, 4.5) {};
      \node[label=above:$G$] at (8, 4.5) {};

      \node[label=above:$u$] at (2, 4) {};
      \node[label=above:$v$] at (6, 4) {};


      \node[ele,label=left:$a$] (a1) at (0,4) {};
      \node[ele,label=left:$b$] (a2) at (0,3) {};

      \node[ele,,label=below:$1$] (b1) at (4,4) {};
      \node[ele,,label=above:$2$] (b2) at (4,3) {};
      \node[ele,,label=above:$3$] (b3) at (4,2) {};

      \node[ele,,label=right:$\star$] (c1) at (8,4) {};
      \node[ele,,label=right:$\triangle$] (c2) at (8,3) {};

      \node[draw,fit= (a1) (a2) ,minimum width=2cm] {} ;
      \node[draw,fit= (b1) (b2) (b3) ,minimum width=2cm] {} ;
      \node[draw,fit= (c1) (c2) ,minimum width=2cm] {} ;

      \draw[->,thick,shorten <=2pt,shorten >=2pt] (a1) -- (b1);
      \draw[->,thick,shorten <=2pt,shorten >=2] (a2) -- (b2);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b1) -- (c1);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b2) -- (c2);
      \draw[->,thick,shorten <=2pt,shorten >=2] (b3) -- (c2);
    \end{tikzpicture}
  \end{figure}

  Ici, $v \circ u$ est surjective, on a montré que cela impliquait $v$ surjective. Pourtant, $u$ n'est pas surjective.

\end{question_kholle}

\textbf{Remarque} : Les deux contre contre-exemples exhibés ici sont les mêmes, mais il y en a bien d'autres où $v \circ u$ n'est pas bijective.

\begin{question_kholle}{Soit $u$ une application de $E$ dans $F$. Si $A$ et $A'$ sont des parties de $E$, y'a-t-il égalité entre $u(A \cap A')$ et $u(A) \cap u(A')$? (On justifiera les réponses aux deux inclusions suggérées par la question)}
  Soit $u \in \mathcal{F}(E, F)$ fixée quelconque, $(A, A') \in \mathcal{P}(E)^{2}$, deux parties de $E$.
  \begin{itemize}[label=$\lozenge$]
    \item Soit $y \in u(A \cap A')$ fixé quelconque.
          Par définition $\exists x \in (A \cap A') : u(x) = y$.
          Ainsi, $x \in A \implies u(x) \in u(A)$
          $x \in A' \implies u(x) \in u(A')$

          $$
            \left.
            \begin{array}{ll}
              u(x) \in u(A) \\
              u(x) \in u(A')
            \end{array}\right\} \implies u(x) \in u(A) \cap u(A')
          $$
          Donc $u(A \cap A') \subset u(A) \cap u(A')$.

    \item En revanche l'inclusion réciproque est fausse: considérons
          $$
            u \left|\begin{array}{ll} \{ 1, 2, 3, 4 \} & \to \{ a, b, c, d \} \\ 1 &\mapsto a \\
             2                     & \mapsto b            \\
             3                     & \mapsto a            \\
             4                     & \mapsto d\end{array}\right.
          $$
          Si on choisit $A = \{ 1, 2 \}$ et $A' = \{ 2, 3 \}$.

          Alors, $u(A) = \{ a, b \}$, et $u(A') = \{ a, b \}$
          $u(A \cap A') = u(\{ 2 \}) = \{ b \}$

          et $u(A) \cap u(A') = \{ a, b \} \not\subset \{  b \}$
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer que, si $u$ est une application de $E$ dans $F$. Si $B$ est une partie de $F$, alors $u^{-1}(F\setminus B) = E \setminus u^{-1}(B)$.}
  Soit $x \in u^{-1}(F\setminus B)$. Raisonnons par équivalences.

  \begin{align*}
    x \in u^{-1}(F \setminus B)
     & \iff u(x) \in F \setminus B         \\
     & \iff \mathrm{non} (u(x) \in B)      \\
     & \iff \mathrm{non} (x \in u^{-1}(B)) \\
     & \iff x \in E \setminus u^{-1}(B)
  \end{align*}

\end{question_kholle}
\begin{question_kholle}{Montrer que, parmi les entiers ne s'écrivant qu'avec des 7, il existe au moins un multiple de 61.}

  Posons $s$ la suite des entiers tels que $\forall n \in \mathbb{N}^{*}, s_{n} = \underbrace{ 7\dots 7 }_{ n \text{ fois} }$.
  Considérons les $62$ premiers termes de la suite.

  Puisqu'il y a $61$ classes de congruences modulo $61$, le principe des tiroirs de Dirichlet nous permet d'affirmer que $\exists (k, l) \in [ \! [ 1, 62 ] \!]^{2}, k < l:s_{k} \equiv s_{l}[61]$.

  Remarquons maintenant que $s_{l}-s_{k} \equiv 0 [61]$, autrement dit que $61\mid s_{l} - s_{k}$.

  Cependant, $$
    s_{l} - s_{k} = \underbrace{ 7\dots7 }_{ l \text{ fois} } - \underbrace{ 7 \dots 7 }_{ k \text{ fois} } = \underbrace{ 7 \dots 7 }_{ l-k \text{ fois} } \underbrace{ 0 \dots 0 }_{ k \text{ fois} } = s_{l-k} \times 10^{k}
  $$
  Donc $61 \mid 10^{k} \times s_{l-k}$, mais $\mathrm{pgcd}(61, 10^{k}) = 1$ donc le théorème de Gauss donne $61\mid s_{l-k}$.

  Ainsi, parmi les entiers ne s'écrivant qu'avec des 7, il existe au moins un multiple de 61.
\end{question_kholle}
\pagebreak\section{Semaine 3}

\allowdisplaybreaks[4]
\begin{question_kholle}
  [Pour tout $(z_{1}, z_{2})\in \mathbb{C}^{2}$,\begin{propositions}

      \item $\lvert z_{1}+z_{2} \rvert \leqslant \lvert z_{1} \rvert + \lvert z_{2} \rvert$

      \item $\bigg| \lvert z_{1} \rvert-\lvert z_{2} \rvert \bigg|\leqslant \lvert z_{1}-z_{2} \rvert$

    \end{propositions}]
  {Preuve de l'inégalité triangulaire et de l'inégalité montrant que le module est 1-lipschitzien + dessin et interprétation géométrique}
  Soient $(z_1,z_2) \in \C^2$ fixés quelconques.
  \begin{itemize}[label=$\lozenge$]
    \item

          Si $z_{2} = 0$ l'inégalité est évidente.\\
          Sinon, $z_{2} \neq 0$ alors $\lvert z_{1}+z_{2} \rvert \leqslant \lvert z_{1} \rvert + \lvert z_{2} \rvert \iff \left| 1+\frac{z_{1}}{z_{2}} \right|\leqslant 1 + \left\lvert  \frac{z_{1}}{z_{2}}  \right\rvert$.\\
          Posons $u = \frac{z_{1}}{z_{2}}$
          \begin{align*}
            \lvert 1+u \rvert ^{2} - (1+\lvert u \rvert )^{2} & = (1+u)(\overline{1+u})-(1+2\lvert u \rvert +\lvert u \rvert ^{2})     \\
                                                              & = (1+u)(1+ \overline u) - 1 - 2\lvert u \rvert  - \lvert u \rvert ^{2} \\
                                                              & = u + \overline u -2 \lvert u \rvert                                   \\
                                                              & = 2(\mathrm{Re}(u)-u) \leqslant 0
          \end{align*}

    \item Appliquons l'inégalité triangulaire
          $$
            \lvert  z_{1} \rvert  = \lvert z_{1} - z_{2} + z_{2} \rvert  \leqslant \lvert z_{1}-z_{2} \rvert +\lvert z_{2} \rvert  \implies \lvert z_{1} \rvert - \lvert z_{2} \rvert \leqslant \lvert z_{1}-z_{2} \rvert
          $$
          Puisque $z_{1}$ et $z_{2}$ jouent de rôles symétriques on a aussi
          $$
            \lvert z_{2} \rvert - \lvert z_{1} \rvert \leqslant \lvert z_{2}-z_{1} \rvert =\lvert z_{1}-z_{2} \rvert
          $$
          Donc
          $$
            \bigg| \lvert z_{1} \rvert-\lvert z_{2} \rvert \bigg|\leqslant \lvert z_{1}-z_{2} \rvert
          $$
          \begin{figure}[H]
            \centering
            \begin{tikzpicture}
              \draw [->] (-0.5,0) -- (10,0);
              \draw [->] (0,-0.5) -- (0,4);
              \draw [-Stealth][line width=1.5] (0,0) -- (10,4) node[midway, above left] {$z_1+z_2$};
              \draw [-Stealth][color=purple,line width=1.5] (0,0) -- (7,1) node[midway, above left] {$z_1$};
              \draw [-Stealth][color=teal,line width=1.5] (7,1) -- (10,4) node[midway, below right] {$z_2$};
            \end{tikzpicture}
            \caption{Interprétation géométrique de l'inégalité triangulaire.}
          \end{figure}
  \end{itemize}

\end{question_kholle}
\begin{question_kholle}{Caractérisation du cas d'égalité de l'inégalité triangulaire dans $\mathbb C$}
  \;\\
  \begin{itemize}[label=$\star$]
    \item ($\implies$) Supposons qu'il y ait égalité dans l'inégalité triangulaire
          \begin{itemize}[label=$\lozenge$]
            \item Si $z_{2} = 0$ alors $z_{1}$ et $z_{2}$ sont positivement liés
            \item Sinon, $\lvert 1+u \rvert ^{2} - (1+\lvert u \rvert )^{2}  = 0$ donc $\mathrm{Re}(u) - \lvert u \rvert = 0$.\\
                  Donc $u \in \mathbb{R}_{+}$, et comme $z_{1} = uz_{2}$, alors $z_{1}$ et $z_{2}$ sont positivement liés.
          \end{itemize}
    \item ($\impliedby$) Supposons que $z_{1}$ et $z_{2}$ sont positivement liés. Alors il existe $\lambda \in \mathbb{R}_{+}$ tel que $z_{1} = \lambda z_{2}$.
          Si $z_{1} = \lambda z_{2}$,
          $$
            \lvert z_{1}+z_{2} \rvert  = \lvert (\lambda+1) z_{2} \rvert  = \lvert \lambda + 1  \rvert \lvert z_{2} \rvert = (\lambda+1)\lvert z_{2} \rvert = \lambda \lvert z_{2} \rvert +\lvert z_{2} \rvert  = \lvert \lambda z_{2} \rvert +\lvert z_{2} \rvert = \lvert z_{1} \rvert + \lvert z_{2} \rvert
          $$
          Donc l'inégalité est une égalité.

          Si $z_{2} = \lambda z_{1}$, en échangeant les rôles joués par $z_{1}$ et $z_{2}$ on obtient que l'inégalité est une égalité.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Calcul de $\sum_{k=0}^{n}\cos(k\theta)$ pour tout $\theta \in \mathbb R$}
  Soit $\theta \in \mathbb{R}$ fixé quelconque, $n \in \mathbb{N}$ fixé quelconque.


  \begin{align*}
    C_{n}(\theta)=\sum_{k=0}^{n} \cos(k\theta) & = \sum_{k=0}^{n} \mathrm{Re}(e^{i k \theta})                                                      \\
                                               & = \mathrm{Re} \left(\sum_{k=0}^{n} e^{i k \theta}\right)                                          \\
                                               & = \mathrm{Re} \left( \sum_{k=0}^{n} (e^{i \theta})^{k} \right) \text{ par les formules de moivre} \\
  \end{align*}
  Ainsi, si $e^{i\theta} = 1 \iff \theta \equiv 0 [2\pi]$,
  $$
    C_{n}(\theta) = \mathrm{Re}\left( \sum_{k=0}^{n}(1)^{k} \right) = \mathrm{Re}(n+1) = n+1
  $$
  Sinon,
  $$
    C_{n}(\theta) = \mathrm{Re}\left( \frac{1-(e^{i\theta})^{n+1}}{1-e^{i\theta}} \right)
  $$
  Simplifions donc ce quotient.

  \begin{align*}
    \frac{1-(e^{i\theta})^{n+1}}{1-e^{i\theta}} = \frac{1-e^{i\theta(n+1)}}{1-e^{i\theta}}
     & =\frac{e^{\frac{i\theta(n+1)}2{}}\left( e^{-\frac{i\theta(n+1)}2{}}-e^{\frac{i\theta(n+1)}2{}} \right)}{e^{i\frac{\theta}{2}}\left( e^{-i\frac{\theta}{2}}- e^{i \frac{\theta}{2}} \right)} \\
     & = e^{i \frac{\theta n}2}\left( \frac{-2i \sin\left( \frac{\theta (n+1)}{2} \right)}{-2i\sin \frac{\theta}{2}} \right)                                                                       \\
     & = \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}}\left( \cos \frac{\theta n}{2} + i \sin \frac{\theta n}{2} \right) \tag{$\clubsuit$}\label{S3:Q3}
  \end{align*}


  En prenant la partie réelle de ce résultat, on a
  $$
    C_{n}(\theta) = \mathrm{Re}\left[ \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}}\left( \cos \frac{\theta n}{2} + i \sin \frac{\theta n}{2} \right) \right] = \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \cos \frac{n\theta}{2}
  $$

  Donc
  $$
    C_{n}(\theta) = \left\{ \begin{array}{ll}
      n+1                                                                                           & \text{ si } \theta \equiv 0 [2 \pi] \\
      \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \cos \frac{n\theta}{2} & \text{ sinon}
    \end{array}\right.
  $$
\end{question_kholle}
\textbf{Remarque}
En prenant la partie imaginaire de \eqref{S3:Q3}, on peut retrouver la somme $S_n(\theta)$:
$$
  S_{n}(\theta)= \sum_{k=0}^n \sin(k \theta) = \left\{ \begin{array}{ll}
    0                                                                                             & \text{ si } \theta \equiv 0 [2 \pi] \\
    \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \sin \frac{n\theta}{2} & \text{ sinon}
  \end{array}\right.
$$
\begin{question_kholle}
  [{Soient $n \in \mathbb{N}, (a_{0}, \dots, a_{n})\in\mathbb{C}^{n+1}$ et $z_{0} \in \mathbb{C}$ Posons pour tout $z \in \mathbb{C}, P(z) = \sum_{k=0}^{n}a_{k}z^{k}$
  \begin{propositions}
    \item Si $P(z_{0}) = 0$, alors $\exists Q \in\mathbb{C}[z]:\forall z \in \mathbb{C}, P(z)=(z-z_{0})Q(z)$
  \end{propositions}
  }]
  {Si $z_0$ est racine de la fonction polynômiale $P$, alors $P$ se factorise par $(z-z_0)$}
  Soit $z \in \mathbb{C}$ fixé quelconque,
  \begin{align*}
    P(z) & = P(z) - P(z_{0})                                                                                                        \\
         & = \sum_{k=0}^{n}a_{k}z^{k} - \sum_{k=0}^{n}a_{k}z_{0}^{k}                                                                \\
         & = \sum_{k=0}^{n}a_{k}(z^{k}-z_{0}^{k})                                                          & \text{ nul pour }k = 0 \\
         & = \sum_{k=1}^{n}\left( a_{k}(z-z_{0})\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right) \right)                          \\
         & = (z-z_{0}) \sum_{k=1}^{n}a_{k}\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right)
  \end{align*}

  Donc en posant $Q(z) = \sum_{k=1}^{n}a_{k}\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right), \in \mathbb{C}[z]$, on a montré que $P$ se factorise.
\end{question_kholle}
\begin{question_kholle}{Si $z_1, \dots , z_n$ sont $n$ racines distinctes de la fonction polynômiale $P$ de degré $n$, alors $P(z)$ se factorise en ... }
  Soient $n\in\N$ et $(a_0,\dots,a_n)\in\C^n\times\C^*$ fixés quelconques. Posons, pour tout $z\in\C$,
  \begin{align}\label{eqn:S3:5.1}
    P(z)=\sum_{k=0}^{n}{a_kz^k}
  \end{align}
  Supposons que $(z_1,\dots,z_n)\in\C^n$ sont $n$ racines deux à deux distinctes de la fonction polynomiale $P$. Alors, il existe $Q\in\C[z]$ tel que pour tout $z\in\C$,
  \begin{align*}
    P(z)=Q(z)\prod_{i=1}^{n}{(z-z_i)}
  \end{align*}
  On note $d$ le degré de $Q$ et $(b_0, b_d)\in\C^d\times\C^*$ ses coefficients. On a alors
  \begin{align*}
    Q(z)=\sum_{k=0}^{d}{b_dz^d}
  \end{align*}
  Ainsi, $P(z)$ s'écrit
  \begin{align}\label{eqn:S3:5.2}
    P(z)=\sum_{k=0}^{d}{b_dz^d}\prod_{i=1}^{n}{(z-z_i)} = b_dz^{n+d} + \textit{termes de degré inférieur à $n+d$.}
  \end{align}
  Par unicité des coefficients d'une fonction polynomiale, $n+d=n$ (sinon, $z^{n+d}$ aurait un coefficient $b_d$ non nul à droite mais un coefficient nul à gauche).\\
  Donc $d=0$ d'où $Q$ est une fonction constante de valeur $b_d=b_0$, et en identifiant les termes en $z^n$ de \eqref{eqn:S3:5.1} et \eqref{eqn:S3:5.2}, on obtient $a_n=b_0$. Ainsi, pour tout $z\in\C$,
  \begin{align*}
    P(z)=a_n\prod_{i=1}^{n}{(z-z_i)}
  \end{align*}
\end{question_kholle}
\begin{question_kholle}{Calculer le module et un argument de $z=1+ e^{i \theta}$ en fonction de $\theta \in [0, 2 \pi[$}
  Soit $\theta \in [0, 2\pi[$
  $$
    z = 1+ e^{i \theta} = e^{i\times0}+e^{i\theta} = e^{i \frac{\theta}{2}}\left( e^{-i\frac{\theta}{2}} + e^{i \frac{\theta}{2}} \right)= 2 \cos \frac{\theta}{2}e^{i\frac{\theta}{2}}
  $$
  Cette dernière notation est une notation exponentielle seulement si $2 \cos \frac{\theta}{2} \geqslant 0$.
  \begin{itemize}[label=$\star$]
    \item Si $\theta \in [0, \pi[$,
          $$\left\{ \begin{array}{ll}
              |z| = 2\cos \frac{\theta}{2} \\
              \frac{\theta}{2} \in \mathrm{Arg}  (z)
            \end{array}\right.$$
    \item Si $\theta = \pi$, $z = 0$ donc $\lvert z \rvert = 0$

    \item Si $\theta \in ]\pi, 2\pi[$,

          \begin{align*}
            z = 2 \cos \frac{\theta}{2}e^{i\frac{\theta}{2}} & = -2 \left\lvert  \cos \frac{\theta}{2}  \right\rvert e^{i\frac{\theta}{2}}                      \\
                                                             & =-2 \left\lvert  \cos \frac{\theta}{2}  \right\rvert e^{i \left( \frac{\theta}{2} + \pi \right)}
          \end{align*}

          Donc
          $$
            \left\{ \begin{array}{ll}
              |z| = -2 \lvert  \cos \frac{\theta}{2}  \rvert \\
              \frac{\theta}{2} + \pi \in \mathrm{Arg}  (z)
            \end{array}\right.
          $$
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{Considérons l'équation algébrique de degré 2:
        $$az^{2}+bz+c=0$$
        Où $z\in L$ est l'inconnue et $(a,b,c)\in\mathbb{C}^*\times\mathbb{C}^2$ sont des paramètres. Posons $\Delta = b^{2}-4ac$ que l'on appelle le discriminant de l'équation.
        \begin{itemize}
          \item Si $\Delta=0$, l'équation admet une unique solution dite double qui est $-\frac b{2a}$ et la forme factorisée du trinôme est
                $$
                  az^2+bz+c = a \left( z + \frac b {2a} \right)^2
                $$
          \item Si $\Delta\neq0$, notons $\delta$ une racine carrée de $\Delta$, l'équation admet deux solutions distinctes $\frac{-b-\delta}{2a}$ et $\frac{-b+\delta}{2a}$ dites simples et la forme factorisée du trinôme est
                $$
                  az^2+bz+c = a\left(z - \frac{-b-\delta}{2a}\right)\left(z - \frac{-b+\delta}{2a}\right)
                $$
        \end{itemize}
      }]{Résolution des équations algébriques de degré $2$ dans $\C$ et algorithme de recherche d'une racine carrée sous forme cartésienne (sur un exemple explicite).}
  La preuve est immédiate à partir de la forme canonique du trinôme du second degré :

  La preuve est immédiate à partir de la forme canonique du trinôme du second degré :

  \begin{align*}
    az^{2}+bz+c = a\left[ \underbrace{ z^{2}+\frac{b}{a}z }_{ \text{But: Absorber ces termes dans un carré} }+\frac{c}{a} \right] & = a\left[ z^{2}+2\frac{b}{2a}z +\frac{b^{2}}{4a^{2}}-\frac{b^{2}}{4a^{2}}+\frac{c}{a} \right] \\
                                                                                                                                  & =a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{b^{2}}{4a^{2}}+\frac{c}{a} \right]        \\
                                                                                                                                  & =a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{b^{2}-4ac}{4a^{2}}\right]                 \\
                                                                                                                                  & =a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{\Delta}{4a^{2}}\right]
  \end{align*}
  \begin{itemize}
    \item Si $\Delta = 0$
          $$
            az^{2}+bz+c = a\left( z-\frac{-b}{2a} \right)^{2}
          $$
          de sorte que
          $$
            az^{2}+bz+c = 0 \iff a\left( z-\frac{-b}{2a} \right)^{2} = 0 \iff z = -\frac{b}{2a}
          $$

    \item Sinon
          \begin{align*}
            az^{2}+bz+c = a \left[ \left( z+\frac{b}{2a} \right)^{2}-\left( \frac{\delta}{2a} \right)^{2} \right] & = a \left( z + \frac{b}{2a}-\frac{\delta}{2a} \right)\left( z+\frac{b}{2a}+\frac{\delta}{2a} \right) \\ &= a \left( z - \frac{-z+\delta}{2a} \right)\left( z- \frac{-z - \delta}{2a} \right)
          \end{align*}
          de sorte que

          \begin{align*}
            az^{2}+bz+c =0 & \iff a \left( z - \frac{-z+\delta}{2a} \right)\left( z- \frac{-z - \delta}{2a} \right) = 0 \\ \\
                           & \iff \left\{ \begin{array}{ll}
                                            z -  \frac{-z-\delta}{2a}  = 0 \\
                                            \text{ou}                      \\
                                            z- \frac{-z+\delta}{2a} =0
                                          \end{array}\right.                                                \\
                           & \iff \left\{ \begin{array}{ll}
                                            z = \frac{-z-\delta}{2a} \\ \text{ou} \\
                                            z = \frac{-z+\delta}{2a}
                                          \end{array}\right.
          \end{align*}

  \end{itemize}
\end{question_kholle}

\pagebreak
\begin{question_kholle}[{
  Pour tout $n \in \mathbb N ^{*} \setminus \{ 1 \}$,
  $$
    \mathbb U _n = \left\{ e^{\frac{2ik\pi}{n}}\mid k \in [ \! [ 0, n - 1 ] \!] \right\}
  $$
  }]{Décrire (avec preuve) l'ensemble des racines $n$-ièmes de l'unité et les localiser géométriquement dans le plan complexe.}
  \begin{itemize}
    \item \underline{Description de l'ensemble $\mathbb U _n$}
          \begin{align*}
            \left\{ \begin{array}{ll}
                      z^{n}=1 \\
                      z \in \mathbb{C}
                    \end{array}\right.
             & \iff
            \left\{ \begin{array}{ll}
                      z^{n} = 1 \\
                      z \in \mathbb{C}^{*}
                    \end{array}\right. \text{ ou }
            \left\{ \begin{array}{ll}
                      z^{n} = 1 \\
                      z = 0
                    \end{array}\right.                                               \\
             & \iff
            \left\{ \begin{array}{ll}
                      \rho^{n}e^{in \theta}  = 1 \\
                      z = \rho e^{i \theta}      \\
                      (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
                    \end{array}\right.
            \\ & \iff
            \left\{ \begin{array}{ll}
                      \rho^{n} = 1              \\
                      n \theta \equiv 0 [2 \pi] \\
                      z = \rho e^{i \theta}     \\
                      (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
                    \end{array}\right.
            \\ & \iff
            \left\{ \begin{array}{ll}
                      \rho = 1 \text{ car } \rho > 0                \\
                      \theta \equiv 0 \left[ \frac{2\pi}{n} \right] \\
                      z = \rho e^{i \theta}                         \\
                      (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
                    \end{array}\right.
            \\ & \iff
            \left\{ \begin{array}{ll}
                      \rho = 1                                            \\
                      \exists k \in \mathbb{Z} : \theta = \frac{2k\pi}{n} \\
                      z = \rho e^{i \theta}                               \\
                      (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
                    \end{array}\right.
            \\  & \iff \exists k \in \mathbb{Z}: z=e^{\frac{2ik\pi}{n}} \\
             & \iff z \in \left\{ e^{\frac{2ik\pi}{n}}\mid k \in \mathbb{Z} \right\}
          \end{align*}


          L'ensemble des solutions est paramétré par l'entier $k$ qui parcourt un ensemble infini. Toutefois, en représentant graphiquement les solutions, il semblerait que "tous les $n$", on fait un tour de cercle trigonométrique de plus, en redécrivant les solutions déjà obtenues pour $k \in [ \! [ 0, n-1 ] \!]$.
    \item \underline{Localisation géométrique}
          \begin{itemize}[label=$\star$]
            \item $\mathbb U _3$ est l'ensemble des sommets du triangle équilatéral inscrit dans le cercle unité, et dont $1$ est l'un des sommets

            \item $\mathbb U _4$ est l'ensemble des sommets du carré inscrit dans le cercle unité et dont 1 est l'un des sommets. Le côté du carré vaut $\lvert 1 - i\rvert = \sqrt 2$.
            \item $\mathbb U _5$ est l'ensemble des sommets du pentagone régulier inscrit dans le cercle unité et dont 1 est l'un des sommets.
                  \begin{figure}[!h]
                    \hfill
                    \begin{minipage}{.45\textwidth}
                      \centering
                      \begin{tikzpicture}[
                          dot/.style={draw,fill,circle,inner sep=1pt}
                        ]
                        \draw[->] (-2,0) -- (2,0) node[above] {$\Re$};
                        \draw[->] (0,-2) -- (0,2) node[left] {$\Im$};
                        \draw[help lines] (0,0) circle (1);

                        \node[dot,label={below right:$O$}] (O) at (0,0) {};
                        \foreach \i in {1,...,3} {
                            \node[dot,label={\i*360/3-(\i==3)*45:$e^\frac{2 \times {\i} i \pi}{3}$}] (w\i) at (\i*360/3:1) {};
                            \draw[->] (O) -- (w\i);
                          }
                        \draw[->] (0:.3) arc (0:360/3:.3);
                        \node at (360/3/2:.5) {$\alpha$};
                      \end{tikzpicture}
                      \caption{Racines cubiques de l'unité.}
                    \end{minipage}
                    \begin{minipage}{.45\textwidth}
                      \centering
                      \begin{tikzpicture}[
                          dot/.style={draw,fill,circle,inner sep=1pt}
                        ]
                        \draw[->] (-2,0) -- (2,0) node[above] {$\Re$};
                        \draw[->] (0,-2) -- (0,2) node[left] {$\Im$};
                        \draw[help lines] (0,0) circle (1);

                        \node[dot,label={below right:$O$}] (O) at (0,0) {};
                        \foreach \i in {1,...,5} {
                            \node[dot,label={\i*360/5-(\i==5)*45:$e^\frac{2 \times {\i} i \pi}{5}$}] (w\i) at (\i*360/5:1) {};
                            \draw[->] (O) -- (w\i);
                          }
                        \draw[->] (0:.3) arc (0:360/5:.3);
                        \node at (360/5/2:.5) {$\alpha$};
                      \end{tikzpicture}
                      \caption{Racines $5^{\mathrm{èmes}}$ de l'unité.}
                    \end{minipage}
                  \end{figure}
          \end{itemize}
  \end{itemize}
\end{question_kholle}
\pagebreak
\begin{question_kholle}{Somme et Produit des racines $n$-ièmes}
  \;\\
  \begin{itemize}[label=$\lozenge$]
    \item \textbf{Méthode 1} : En utilisant les relations coefficients racines.

          $\mathbb{U}_{n}$ sont les n racines disctinctes de $z^{n}-1$
          $$S_{n} = - \frac{1}{\text{coefficient dominant}}\times(\text{coefficient de }z^{n-1} \text{ dans }z^{n}-1)= \left\{ \begin{array}{ll}
              -0    & \text{ si }  n\geqslant 2 \\
              -(-1) & \text{ sinon}
            \end{array}\right.$$
          $$
            P_{n} = (-1)^{n} \frac{\text{coefficient constant}}{\text{coefficient dominant}} = (-1) ^{n}\times \frac{-1}{1} = (-1)^{n+1}
          $$

    \item \textbf{Méthode 2} : Manipulation des symboles sommatoires

          \begin{align*}
            S_{n} = \sum_{\omega \in \mathbb{U}_{n}}\omega & = \sum_{k=0}^{n-1}\omega_{0}^{k}                                                                    \\
                                                           & = \left\{ \begin{array}{ll}
                                                                         1                                                & \text{si }  n =1 \\
                                                                         1 \times \frac{1 - \omega_{0}^{n}}{1-\omega_{0}} & \text{sinon}
                                                                       \end{array}\right.
          \end{align*}

          Puisqu'on ne peut appliquer la formule de la somme des termes d'une suite géométrique seulement si $\omega_{0} = 1 \iff e^{\frac{2i\pi}{n}} = 1 \iff \frac{2\pi}{n} \equiv 0 [2\pi] \iff n = 1$

          De même

          \begin{align*}
            P_{n} = \prod_{\omega \in \mathbb{U}_{n}}\omega = \prod_{k=0}^{n-1}\omega_{0}^{k}= \omega_{0}^{\sum_{k=0}^{n-1}k} & = \omega_{0} ^{\frac{n(n-1)}{2}}                                                                                  \\ \\
                                                                                                                              & =\left\{ \begin{array}{ll}
                                                                                                                                           (\omega_{0}^{n})^{\frac{n-1}{2}} = 1^{\frac{n-1}{2}} = 1 & \text{si } n \equiv 1 [2] \\
                                                                                                                                           e^{\frac{2i\pi n(n-1)}{2n}} = e^{i \pi(n-1)} = (-1)^{n-1}
                                                                                                                                         \end{array}\right.
            \\ &= (-1)^{n-1}
          \end{align*}
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}
  [{Soient $n \in \mathbb{N}, (a_{0}, \dots, a_{n})\in\mathbb{C}^{n+1}$ et $z_{0} \in \mathbb{C}$.\\
  Posons pour tout $z \in \mathbb{C}, P(z) = \sum_{k=0}^{n}a_{k}z^{k}$.
  \begin{propositions}
    \item Si $\exists p \in \mathbb{N}^{*}: \exists(z_{1},\dots ,z_{p})\in\mathbb{C}^{p}$ deux à deux distincts tels que $\forall k \in [ \! [ 1, p ] \!], P(z_{k}) = 0$ alors, $\exists Q \in \mathbb{C}[x]:\forall z \in \mathbb{C}, P(z) = Q(z) \times \prod_{k=1}^{p}(z-z_{k})$.
  \end{propositions}
  }]
  {\textit{[non demandée]} Factorisation d'une fonction polynomiale connaissant $p$ racines.}
  Considérons la propriété $\mathcal{P}(\cdot)$ définie pour tout $p \in \mathbb{N}^{*}$ par
  \begin{multline*}
    \mathcal{P}(p) : \forall P \in \mathbb{C}[z], (\exists (z_{1}, \dots, z_{p}) \in \mathbb{C}^{p}, \text{ 2 à 2 distincts }: \forall i \in [ \! [ 1, p ] \!], P(z_{i}) = 0)\\
    \implies \exists Q \in \mathbb{C}[z]: P(z) = Q(z) \prod_{i=1}^{p}(z-z_{i})
  \end{multline*}
  \begin{itemize}[label=$\lozenge$]
    \item $\mathcal{P}(1)$ est vraie d'après la preuve précédente.
    \item Soit $p \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{P}(p)$ est vraie.\\
          Soit $P \in \mathbb{C}[z]$ fixés quelconques tels que $\exists (z_{1}, \dots, z_{p+1}) \in \mathbb{C} ^{p+1}$ deux à deux distincts tels que $\forall i \in [ \! [ 1, p + 1 ] \!], P(z_{i}) = 0$.\\
          Appliquons $\mathcal{P}(p)$ à $P \in \mathbb{C}[z]$ dont $(z_{1}, \dots, z_{p})$ sont les $p$ racines deux à deux distinctes.

          $$\exists Q_{1} \in \mathbb{C}[z]:\forall z \in \mathbb{C}, P(z) = Q_{1}(z)\prod_{i=1}^{p}(z-z_{i})$$
          Évaluons cette expression en $z_{p+1}$
          $$\underbrace{ P(z_{p+1}) }_{ =0 } = Q_{1}(z_{p+1}) \prod_{i=1}^{p}\underbrace{ (z_{p+1}-z_{i}) }_{ \neq 0 \text{ car distincts} }$$
          Donc $Q_{1}(z_{p+1}) = 0$, ce qui permet d'appliquer (i) pour $P \leftarrow Q_{1}$, $z_{0} \leftarrow z_{p+1}$.
          $$
            \exists Q \in \mathbb{C}[z]:\forall z \in \mathbb{C}, Q_{1}(z)=(z-z_{p+1})Q(z)
          $$
          Donc
          $$
            \forall z \in \mathbb{C}, P(z) = (z-z_{p+1})Q(z) \prod_{i=1}^{p}(z-z_{i}) = Q(z) \prod_{i=1}^{p+1}(z-z_{i})
          $$
          Donc $\mathcal{P}(p+1)$ est vraie.
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 4}
\begin{question_kholle}{Montrer que l'ensemble des similitudes directes du plan complexe est un groupe pour la composition (la preuve de la bijectivité des similitudes fait partie de la question).}
  Soient $s$ et $s'$ deux similitudes directes. Alors, il existe $(a,a',b,b')\in(\C^*)^2\times \C^2$ tels que
  \[
    s:z\longmapsto az+b \quad\text{ et }\quad s':z\longmapsto a'z+b'
  \]
  \begin{itemize}[label=$\star$]
    \item $s\circ s': z\longmapsto a(a'z+b')+b = \underbrace{aa'}_{\in\C^*}z+\underbrace{ab'+b}_{\in\C}$ donc $s\circ s'$ est une similitude directe.\\
          Ainsi, la composition est une LCI sur l'ensemble des similitudes directes.
    \item La composition est associative
    \item La composition admet $id_{\C}:z\longmapsto 1z+0$ (qui est une similitude) comme neutre.
    \item Les similitudes directes sont des bijection du plan complexe car si $f$ est une similitude directe ($f:z\longmapsto az+b$ avec $(a,b)\in\C*\times\C$), pour tout $u\in\C$, l'équation d'inconnue $z\in\C$
          \[
            f(z)=u \iff az+b=u \iff z=\frac{1}{a}u-\frac{b}{a}
          \]
          admet une unique solution. De plus, la bijection réciproque $f^{-1}$ d'une similitude directe $f$ (vérifiant $f\circ f^{-1} = f^{-1}\circ f = id_{\C}$) est une similitude directe.\\
          Ainsi, toute similitude directe est symétrisable pour la loi de composition.
  \end{itemize}
  L'ensemble des similitudes directes du plan complexe muni de la loi de composition est donc bien un groupe.
\end{question_kholle}
\begin{question_kholle}{Classifier et interpréter une similitude directe donnée sous la forme $z \mapsto a z + b$ sur un exemple, donner l'expression complexe d'une similitude dont on connaît les éléments caractéristiques.}
  Soient $(a, b) \in \mathbb{C}^*$ fixés quelconques. Posons la similitude
  \begin{align*}
    s \left| \begin{array}{ll}
               \mathbb{C} & \to \mathbb{C}  \\
               z          & \mapsto a z + b
             \end{array}\right.
  \end{align*}
  \begin{itemize}[label=$\lozenge$]
    \item Si $a = 1$, c'est la translation de vecteur d'affixe $b$.
    \item Si $a \neq 1$, $s$ admet un unique point fixe appelé «~centre de la similitude~» $\omega = \frac{b}{1-a}$
          \begin{itemize}[label=$\star$]
            \item Si $a \in \mathbb{R}^*$, $s$ est l'homothétie de centre $\omega$ et de rapport $a$.
            \item Si $a \in (\mathbb{C}^* \setminus \mathbb{R})$, $s$ est la composée commutative de :
                  \begin{itemize}
                    \item La rotation de centre $\omega$ et d'angle $\alpha$, où $\alpha$ est un argument de $a$.
                    \item L'homothétie de centre $\omega$ et de rapport $|a|$.
                  \end{itemize}
                  On nommera alors $|a|$ le rapport de $s$ et $\alpha$ une mesure de l'angle de $s$.
          \end{itemize}
  \end{itemize}
  \textbf{Exemple} : Prenons la similitude $s: z \mapsto (1 - i) z - 1$.
  \begin{align*}
    s(z) = z & \iff (1 - i)z - 1 = z \\
             & \iff -iz = 1          \\
             & \iff z = i
  \end{align*}
  De plus,
  \begin{align*}
    (1 - i)z - 1 = \sqrt{2}e^{-i \frac{\pi}{4}}z - 1
  \end{align*}
  On en déduit que $s$ est la similitude directe de centre d'affixe $i$, de rapport $\sqrt{2}$, et d'angle $-\frac{\pi}{4}$.
\end{question_kholle}
\begin{question_kholle}
  [{L'exponentielle complexe a pour image $\mathbb{C}^{*}$ et, pour tout $z_{0} \in \mathbb{C}^{*}$,
        $$
          \exp_{\mathbb{C}}^{-1}(\{ z_{0} \}) = \{ \ln \lvert z_{0} \rvert +i\theta_{0} +2ik\pi \mid k \in \mathbb{Z}\}
        $$
        où $\theta_{0} \in \arg(z_{0})$}]{Résolution de $e^z = z_0$ où $z_0 \in \C^*$}

  La propriété: $\forall{z} \in \mathbb{C}$, $\lvert e^{z} \rvert = \lvert e^{\mathrm{Re}(z)} \rvert>0$ montre que $0 \not\in \exp_{\mathbb{C}}(\mathbb{C})$.\\
  $z_{0}\neq 0$ donc $\exists \theta_{0} \in \arg(z_{0}): z_{0} = \lvert z_{0} \rvert e^{i\theta_{0}}$.
  Résolvons l'équation d'inconnue $z \in \mathbb{C}$
  \begin{align*}
    \exp_{\mathbb{C}}(z) = z_{0} & \iff e^{\mathrm{Re}(z)}e^{i\mathrm{Im}(z)} = \lvert z_{0} \rvert e^{i\theta_{0}}                \\
                                 & \iff \left\{ \begin{array}{ll}
                                                  e^{\mathrm{Re}(z)} = \lvert z_{0} \rvert \\
                                                  \text{et}                                \\
                                                  \mathrm{Im}(z) \equiv \theta_{0} [2\pi]
                                                \end{array}\right.                                    \\
                                 & \iff \left\{ \begin{array}{ll}
                                                  \mathrm{Re}(z)  = \ln \lvert  z_{0} \rvert \\
                                                  \text{et}                                  \\
                                                  \mathrm{Im}(z) \equiv \theta_{0} [2\pi]
                                                \end{array}\right.                                         \\
                                 & \iff z \in \left\{ \ln \lvert z_{0} \rvert +i \theta_{0} +2ik\pi \mid k \in \mathbb{Z} \right\}
  \end{align*}

\end{question_kholle}
\begin{question_kholle}{Montrer l'unicité de l'élément neutre et du symétrique d'un élément sous des hypothèses sur la loi à préciser.}
  \;\\
  \begin{itemize}[label=$\lozenge$]
    \item Unicité de l'élément neutre bilatère

          Soient $(e_{1}, e_{2}) \in E^{2}$ fixés quelconques tels que $\left\{ \begin{array}{ll} \forall x \in E, x * e_{1} = e_{1} * x = x \\ \forall x \in E, x*e_{2}=e_{2}*x = x\end{array}\right.$.\\
          Particularisons la première relation pour $x \leftarrow e_{2}$:
          $$
            e_{2}*e_{1} = e_{1}*e_{2} = e_{2}
          $$
          En particularisant de même la deuxième relation pour $x \leftarrow e_{1}$ :
          $$
            e_{1}*e_{2} = e_{2}*e_{1} = e_{1}
          $$
          D'où, par transitivité de l'égalité : $e_{1} = e_{2}$

    \item Unicité du symétrique sous réserve d'existence (LCI associative d'unité $e$).\\
          Soit $a \in E$ symétrisable
          $$
            \exists z \in E : a * z = z*a = e
          $$
          Fixons un tel $z$ pour la suite de la preuve
          \begin{itemize}
            \item L'ensemble $\{ y \in E \mid a * y = y * a = e \}$ n'est pas vide puisqu'il contient $z$.

            \item Soit $b \in \{ y \in E \mid a * y = y * a = e \}$ fixé quelconque.
                  Alors
                  \begin{align*}
                    a * b = e & \implies z * ( a * b ) = z * e                                          \\
                              & \implies \underbrace{ z*a }_{ e } * b = z * e \text{ par associativité} \\
                              & \implies b = z
                  \end{align*}
          \end{itemize}
          Donc l'ensemble $\{ y \in E \mid a * y = y * a = e \}$ contient au plus un élément qui est $z$.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{Soit $(G, *)$ un groupe, et $H$ une partie de $G$ $$
          H\text{ est un sous-groupe de }G \iff \left\{ \begin{array}{ll}
            H \neq \emptyset \\
            \forall (x, y) \in H^{2}, x * y^{-1} \in H
          \end{array}\right.
        $$}]{Preuve de la caractérisation d'un sous-groupe, application au fait que $(\mathbb U _n, \times)$ est un sous-groupe de $(\mathbb U, \times)$.}
  \;\\
  \begin{itemize}[label=$\star$]
    \item Supposons que $H$ est un sous-groupe de $G$.
          Par définition d'un sous-groupe, $H \neq \emptyset$.

          Soient $(x, y) \in H^{2}$ fixés quelconques.\\
          $H$ est un sous-groupe donc $y$ est symétrisable dans $H$: $y^{-1} \in H$.\\
          De plus, c'est un groupe, donc stable pour la loi $*$, donc $x*y^{-1} \in H$

    \item Supposons que $\left\{ \begin{array}{ll}H \neq \emptyset &(1) \\\forall (x, y) \in H^{2}, x * y^{-1} \in H & (2)\end{array}\right.$
          \begin{itemize}[label=$\lozenge$]
            \item $H$ est non vide par hypothèse
            \item Puisque $H \neq \emptyset$, $\exists h \in H$.\\
                  Ainsi, en appliquant (2) pour $(x,y)\leftarrow (h,h)$, on obtient $h * h^{-1} \in H$ donc $H$ possède un élément neutre $e$.
            \item Soit $h \in H$ fixé quelconque.
                  $h \in H$ permet d'appliquer (2) pour $(x,y) \leftarrow (h,e)$:
                  $$
                    e * h^{-1} \in H
                  $$
                  Donc $h^{-1} \in H$. Ainsi, tout élément est symétrisable dans $H$
            \item Soient $(x, y) \in H^{2}$ fixés quelconques. On a montré que $y$ est symétrisable dans H, donc en appliquant (2) pour $x \leftarrow x$ et $y \leftarrow y^{-1}$:
                  $$
                    x * (y^{-1})^{-1} \in H \implies x* y \in H
                  $$
                  Donc $H$ est stable pour la loi $H$.
          \end{itemize}
          Donc $H$ est un sous-groupe de $G$.
  \end{itemize}
\end{question_kholle}
\textbf{Application aux racines n-ièmes de l'unité}

Soit $n \in \mathbb{N}^{*}$ fixé quelconque
\begin{itemize}[label=$\star$]
  \item $\forall z \in \mathbb{U}_{n}, z^{n} = 1$ donc $1 = \lvert  z^{n} \rvert = \lvert z \rvert^{n}$. Or $\lvert z \rvert \geqslant 0$ donc $\lvert z \rvert = 1$, si bien que $\mathbb{U}_{n} \subset \mathbb{U}$

  \item $\mathbb{U}_{n} \neq \emptyset$ car $1 \in \mathbb{U}_{n}$

  \item Soient $(z_{1}, z_{2}) \in \mathbb{U}_{n}$ fixés quelconques.
        Calculons
        $$
          (z_{1}z_{2}^{-1})^{n} = \left( \frac{z_{1}}{z_{2}} \right)^{n} = \frac{z_{1}^{n}}{z_{2}^{n}} = \frac{1}{1} = 1
        $$
\end{itemize}
Donc $z_{1}z_{2}^{-1} \in \mathbb{U}_{n}$. On a donc montré que $(\mathbb{U}_{n, \times})$ est un sous-groupe de $(\mathbb{U}, \times)$.

\begin{question_kholle}{Si $\varphi$ est un morphisme de groupes de $G_1$ de neutre $e_1$ dans $G_2$ de neutre $e_2$, calculer $\varphi(e_1)$ et $\varphi(x^{-1})$}
  \;\\
  \begin{itemize}[label=$\star$,nosep]
    \item Soit $f$ un morphisme de groupe de $(G_{1}, *_{1})$ dans $(G_{2}, *_{2})$

          D'une part $f(e_{1} *_{1} e_{1}) = f(e_{1})$.\\
          D'autre part, par propriété de morphisme, $f(e_{1} *_{1} e_{1}) =f(e_{1})*_{2} f(e_{1})$, donc
          $$
            f(e_{1}) *_{2} f(e_{1}) = f(e_{1})
          $$
          Si l'on compose à gauche par $f(e_{1})^{-1}$,
          \begin{align*}
            f(e_{1})^{-1} *_{2} f(e_{1}) *_{2} f(e_{1}) = f(e_{1})^{-1} *_{2}f(e_{1}) \implies f(e_{1}) = e_{2}
          \end{align*}


    \item Soit $x \in G_{1}$ fixé quelconque.
          $$
            f(x)*_{2}f(x^{-1}) = f(x*_{1}x^{-1})=f(e_{1}) = e_{2}
          $$
          Composons les deux membres à gauche par $f(x)^{-1}$ :
          $$
            f(x)^{-1} *_{2} f(x)*_{2}f(x^{-1}) = f(x)^{-1} *_{2} * e_{2}
          $$
          Donc
          $$
            f(x^{-1}) = f(x)^{-1}
          $$

  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer que l'image directe d'un sous-groupe par un morphisme de groupes est un sous-groupe du groupe d'arrivée}
  Soit $f$ un morphisme de groupe de $(G_{1},*_{1})$ dans $(G_{2}, *_{2})$.\\
  Notons $e_{1}$ et $e_{2}$ les neutres respectifs de $G_{1}$ et $G_{2}$.

  Soit $H_{1}$ un sous-groupe de $G_{1}$ fixé quelconque
  \begin{itemize}[label=$\star$]
    \item $f(H_{1})$ est par définition une partie de $G_{2}$.
    \item $f(H_{1}) \neq \emptyset$ car $H_{1}$ est un groupe qui contient $e_{1}$ et $f(e_{1}) = e_{2}$ donc $e_{2} \in f(H_{1})$.
    \item Soient $(g_{2},h_{2}) \in f(H_{1})^{2}$ fixés quelconques, alors $\exists (g_{1}, h_{1}) \in H_{1} : f(g_{1})= g_{2} \text{ et } f(h_{1})=h_{2}$.
          Par conséquent,
          $$
            g_{2}*_{2}h_{2}^{-1} = f(g_{1}) *_{2} f(h_{1}^{-1}) = f(\underbrace{ g_{1} *_{1} h_{1}^{-1} }_{ \in H_{1} \text{ car sous-groupe de }G_{1} })
          $$
          Ainsi, $g_{2} *_{2} h_{2}^{-1} \in f(H_{1})$
          d'où $f(H_{1})$ est un sous-groupe de $G_{2}$
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer que l'image réciproque par un morphisme de groupes d'un sous-groupe est toujours un sous-groupe du groupe de départ, }
  Soit $f$ un morphisme de groupe de $(G_{1},*_{1})$ dans $(G_{2}, *_{2})$.\\
  Notons $e_{1}$ et $e_{2}$ les neutres respectifs de $G_{1}$ et $G_{2}$.

  Soit $H_{2}$ un sous-groupe de $G_{2}$ fixé quelconque.
  \begin{itemize}[label=$\star$]
    \item $f^{-1}(H_{2})$ est par définition une partie de $G_{1}$.
    \item $f(H_{2}) \neq \emptyset$ car $H_{2}$ est un groupe qui contient $e_{2}$ et $f(e_{1}) = e_{2}$ donc $e_{1} \in f^{-1}(H_{2})$.
    \item Soient $(g_{1},h_{1}) \in f^{-1}(H_{2})^{2}$ fixés quelconques, alors $f(g_{1}) \in H_{2}$ et $f(h_{1}) \in H_{2}$, donc
          $$
            f(g_{1} *_{1} h_{1}^{-1}) = \underbrace{ f(g_{1}) }_{ \in H_{2} }*_{2}\underbrace{ f(h_{1})^{-1} }_{ \in H_{2} } \in H_{2} \quad\text{ car c'est un sous-groupe}
          $$
          Ainsi, $f(g_{1} *_{1} h_{1}^{-1}) \in H_{2}$ d'où $g_{1} *_{1} h_{1}^{-1} \in f^{-1}(H_{2})$
  \end{itemize}
  $f^{-1}(H_{2})$ est donc un sous-groupe de $G_{1}$.
\end{question_kholle}

\begin{itemize}[label=$\vartriangleright$]
  \item \textbf{Application 1:} Le noyau d'un morphisme est un sous-groupe.\\
        Le noyau, noté $\ker f$ est par définition égal à $f^{-1}(\{e_2\})$ c'est donc un sous-groupe de $G_1$.

  \item \textbf{Application 2:} Pour tout $n \in \mathbb{N}$, l'application
        \[
          \phi _{n}\left|\begin{array}{ll} (\mathbb{C}^{*},\times) &\to (\mathbb{C}^{*},\times) \\ z &\mapsto z^{n} \end{array}\right.
        \]
        est un morphisme de groupes. Son noyau est $\ker \phi _n = \{ z \in \mathbb{C}^{*} | z^{n} = 1 \} = \mathbb{U}_{n}$.\\
        D'après l'application 1, $\ker \phi_{n}$ est un sous-groupe de $(\mathbb{C}^{*}, \times)$, donc $(\mathbb{U}_{n}, \times)$ est un sous-groupe de $(\mathbb{C}^{*},\times)$.
\end{itemize}

\begin{question_kholle}{\textit{[non demandée]} Montrer que l'ensemble des similitudes directes du plan complexe est un groupe pour la composition (démonstration alternative)}
  Montrons donc que $(S, \circ)$ est un sous-groupe de $(\mathcal{S}(\mathbb{C}), \circ)$
  \begin{itemize}[label=$\lozenge$]
    \item D'une part, $S \subset \mathcal{S}(\mathbb{C})$. Or l'ensemble des permutations $(\mathcal{S}(\mathbb{C}), \circ)$ est un groupe. En effet, les similitudes sont des bijections de $\mathbb{C} \to \mathbb{C}$.
    \item De plus, $S$ est non vide, par exemple l'application $\mathrm{Id(\mathbb{C}})$ est une similitude pour $a \leftarrow 1$ et $b \leftarrow 1$.
    \item Prenons finalement $a$ et $c$ dans $\mathbb{C}^*$ puis $b$ et $d$ dans $\mathbb{C}$, et posons les deux applications suivantes:
          \begin{align*}
            s \left| \begin{array}{ll}
                       \mathbb{C} & \to \mathbb{C}  \\
                       z          & \mapsto a z + b
                     \end{array}\right.
            \text{ et }
            s' \left| \begin{array}{ll}
                        \mathbb{C} & \to \mathbb{C}  \\
                        z          & \mapsto a z + b
                      \end{array}\right.
          \end{align*}
          Ainsi, comme toute similitude directe est une bijection, en particulier $s'$ en est une, et
          \begin{align*}
            s'^{-1} \left| \begin{array}{ll}
                             \mathbb{C} & \to \mathbb{C}                    \\
                             z          & \mapsto \frac{z}{c} - \frac{d}{c}
                           \end{array}\right.
          \end{align*}
          Soit $z \in \mathbb{C}$ fixé quelconque:
          \begin{align*}
            (s \circ s'^{-1})(z) & = s(s'^{-1}(z))                                  \\
                                 & = s\left(\frac{z}{c} - \frac{d}{c}\right)        \\
                                 & = a\left(\frac{z}{c} - \frac{d}{c}\right) + b    \\
                                 & = \frac{a}{c}z + \left( b - \frac{ad}{c} \right)
          \end{align*}
          Qui est une similitude directe, puisque $\frac{a}{c} \neq  0$ donc $s \circ s'^{-1} \in S$. Donc $(S, \circ)$ est bien un sous-groupe de $(\mathcal{S}(\mathbb{C}), \circ)$.
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 5}

\begin{question_kholle}{Montrer qu'une combinaison linéaire de deux fonctions bornées (respectivement lipschitziennes) est bornée (resp. lipschitzienne)}
  Soit $I$ un intervalle réel.
  Soient $f$ et $g$ deux fonctions de $I$ dans $\mathbb{R}$ et $(\lambda, \mu) \in \mathbb{R}^2$.
  \begin{itemize}[label=$\lozenge$]
    \item Supposons que $f$ et $g$ sont respectivement bornées par $A$ et par $B$.
          Soit $x \in I$ fixé quelconque.
          \begin{align*}
            \Big| (\lambda.f + \mu.g)(x) \Big| & = \Big| \lambda.f(x) + \mu.g(x) \Big|                                             \\
                                               & \leqslant \big| \lambda \big|  \big|f(x)\big| + \big| \mu \big|  \big| g(x) \big| \\
                                               & \leqslant \big| \lambda \big| A + \big| \mu \big| B
          \end{align*}
          Donc $\lambda.f + \mu.g$ est bornée.
    \item Supposons que $f$ et $g$ sont respectivement $K$ et $L$ lipschitziennes.\\
          Soient $(x, y) \in I^2$ fixés quelconques.
          \begin{align*}
            \Big| (\lambda.f + \mu.g)(x) - (\lambda.f + \mu.g)(y)\Big| & = \Big| \lambda.f(x) + \mu.g(x) - \lambda.f(y) - \mu.g(y) \Big|                                   \\
                                                                       & = \Big| \lambda(f(x) - f(y)) + \mu(g(x) - g(y)) \Big|                                             \\
                                                                       & \leqslant \Big| \lambda  \Big|  \Big| f(x) - f(y) \Big| + \Big| \mu \Big|  \Big|g(x) - g(y) \Big| \\
                                                                       & \leqslant \Big| \lambda  \Big|  K \Big| x-y \Big| + \Big| \mu  \Big| L  \Big| x - y \Big|         \\
                                                                       & \leqslant (|\lambda| K + |\mu|L ) |x - y|
          \end{align*}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer que si $f$ est impaire et bijective, alors $f^{-1}$ est aussi impaire. Donnez un/des exemples.}
  Soient $I$ et $J$ deux parties non-vides de $\R$ et $f$ une application bijective impaire de $I$ dans $J$. Notons $f^{-1}$ sa bijection réciproque.\\
  L'imparité de $f$ impose la symétrie de $I$ par rapport à l'origine. De plus, pour tout $y\in J$,
  \[
    \exists{x}\in I: f(x)=y
  \]
  donc par imparité de la fonction $f$, le domaine $I$ étant centré en $0$,
  \[
    f(-x)=-f(x)=-y
  \]
  Ainsi, $J$ est centré en $0$. On a alors, pour tout $y\in J$,
  \begin{align*}
    f^{-1}(-y) & = f^{-1}(-f(f^{-1}(y))) \\
               & = f^{-1}(f(-f^{-1}(y))) \\
               & = -f^{-1}(y).
  \end{align*}
  D'où l'imparité de $f^{-1}$.

  \noindent $\vartriangleright$\textbf{Exemple :} Prenons notre fonction bijective impaire préférée, la fonction $\bigl.\sin\bigr|_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]}^{[-1,1]}$ que l'on notera $\widetilde{\sin}$. Sa bijection réciproque est $\arcsin : [-1,1] \to \left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]$.\\
  Comme dans la démonstration, prenons $y\in [-1, 1]$. Comme $[-1,1]$ est centré en $0$, $-y\in [-1,1]$, et dès lors,
  \begin{align*}
    \arcsin(-y) & = \arcsin(-\widetilde{\sin}(\arcsin(y))) \\
                & = \arcsin(\widetilde{\sin}(-\arcsin(y))) \\
                & = -\arcsin(y).
  \end{align*}
  Ce qui prouve l'imparité de la fonction $\arcsin$.
\end{question_kholle}

\begin{question_kholle}{Montrer que les graphes d'une fonction et de sa bijection réciproque sont symétriques par rapport à la première bissectrice.}
  Calculons les coordonnées $(x',y')$ du point $M'$, image de $M$ de coordonnées $(x,y)$ par la réflexion $r$ d'axe la première bissectrice.
  \begin{align*}
    \begin{cases}
      \vv{MM'}\perp (\vv{i}+\vv{j}) \\
      \text{le milieu de $[MM']$ appartient à $\Delta$.}
    \end{cases}
     & \iff
    \begin{cases}
      \vv{MM'}\cdot (\vv{i}+\vv{j})=0 \\
      \text{le milieu de $[MM']$ vérifie l'équation $y=x$.}
    \end{cases} \\
     & \iff
    \begin{cases}
      \begin{pmatrix}x'-x\\y'-y\end{pmatrix}\cdot \begin{pmatrix}1\\1\end{pmatrix}=0 \\[12pt]
      \left(\frac{x+x'}{2},\frac{y+y'}{2}\right) \text{ vérifie l'équation $y=x$.}
    \end{cases}           \\
     & \iff
    \begin{cases}
      x'-x+y'-y=0 \\
      \frac{x+x'}{2}=\frac{y+y'}{2}
    \end{cases}                                                          \\
     & \iff
    \begin{cases}
      x'+y'=x+y \\
      x'-y'=-x+y
    \end{cases}
    \iff
    \begin{cases}
      x'=y \\
      y'=x
    \end{cases}
  \end{align*}
  L'expression de la réflexion $r$ d'axe la première bissectrice est ainsi
  \[
    r:\left|
    \begin{array}{rcl}
      \R^2  & \longrightarrow & \R^2  \\
      (x,y) & \longmapsto     & (y,x) \\
    \end{array}
    \right.
  \]
  Les graphes de $f$ et $f^{-1}$ étant respectivement
  \[
    G_f = \{(x,f(x))\in\R^2 \mid x\in I\} \quad\text{ et }\quad G_{f^{-1}}=\{(y,f^{-1}(y)) \mid y\in J\}
  \]
  on a bien
  \begin{align*}
    r(G_f) & =\{(f(x),x) \mid x\in I\}                                                      \\
           & =\{(y,f^{-1}(y)) \mid y\in J\}\quad\text{ en posant $y=f(x) \iff x=f^{-1}(y)$} \\
           & = G_{f^{-1}}
  \end{align*}
\end{question_kholle}

\begin{question_kholle}{Limite (et preuve) lorsque $x$ tend vers $+\infty$ de $\displaystyle\frac{(\ln x)^{\alpha}}{x^{\beta}}$ pour $\alpha ,\beta \in \left( \mathbb{R}_+^*\right) ^2$.}
  La fonction $\ln$ est concave sur $\R_+^*$, donc au dessous de toutes ses tangentes sur cet intervalle, et en particulier à celle au point d'abscisse $1$ qui a pour équation $y=x-1$. On a donc
  \[
    \forall x \in [1,+\infty[, \quad 0 \; \leq \; \ln (x) \; \leq \; x-1.
  \]
  Ce qui permet d'affirmer, en divisant par $x^2$, que
  \[
    \forall{x}\in [1,+\infty[, \quad 0 \; \leq \; \frac{\ln x}{x^2} \; \leq \; \underbrace{\frac{1}{x}}_{\arrowlim{x}{+\infty}0} - \underbrace{\frac{1}{x^2}}_{\arrowlim{x}{+\infty}0}
  \]
  Ainsi, le théorème d'existence de limite par encadrement permet de conclure que $\frac{\ln x}{x^2}$ admet une limite en $+\infty$ et que cette limite est nulle :
  \[
    \lim_{x\to+\infty}\frac{\ln x}{x^2} = 0
  \]
  On en déduit alors le cas général :
  \\
  \begin{minipage}{0.7\textwidth}
    \begin{align*}
      \frac{(\ln x)^\alpha}{x^\beta} & = \left(\frac{\ln x}{x^{\frac{\beta}{\alpha}}}\right)^{\alpha}                                                                                                                                              \\
                                     & = \left(\frac{\frac{2\alpha}{\beta}\ln\left(x^{\frac{\beta}{2\alpha}}\right)}{x^{\frac{\beta}{\alpha}}}\right)^{\alpha}                                                                                     \\
                                     & = \left(\frac{2\alpha}{\beta}\right)^{\alpha} \underbrace{\left[\underbrace{\frac{\ln\left(x^{\frac{\beta}{2\alpha}}\right)}{\left(x^{\frac{\beta}{2\alpha}}\right)^{2}}}_{\substack{\arrowlim{x}{+\infty}0 \\\text{\scriptsize v. ci-dessus}}}\right]^{\alpha}}_{\substack{\arrowlim{x}{+\infty}0\\\text{par composition des limites}}} \arrowlim{x}{+\infty} 0
    \end{align*}
  \end{minipage}
  \begin{minipage}{0.3\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \draw [->] (-0.5,0) -- (3,0);
        \draw [->] (0,-2) -- (0,2);
        \draw (1,-0.1) -- (1,0.1) node[below, yshift=-2mm] {\footnotesize $1$};
        \draw (-0.1,1) -- (0.1,1) node[left, xshift=-2mm] {\footnotesize $1$};
        \draw[thick, domain=0.15:3, smooth, variable=\x, teal] plot ({\x}, {ln(\x)});
        \draw[thick, domain=-0.5:3, smooth, variable=\x, purple] plot ({\x}, {\x-1});
      \end{tikzpicture}
      \caption{$\ln$ en bleu et $y=x-1$ en violet.}
    \end{figure}
  \end{minipage}
\end{question_kholle}

\begin{question_kholle}{Limite en $0$ de $\displaystyle\frac{(1+x)^{\alpha}-1}{x}$ et de $\displaystyle\frac{1-\cos x}{x^2}$.}
  \hfill
  \begin{itemize}[label=$*$]
    \item Le taux d'accroissement en $x_0$ d'une fonction $f$ dérivable en $x_0$ est
          \[\tag{$\star$}
            \lim_{x\to x_0}\frac{f(x)-f(x_0)}{x-x_0}=f'(x_0)
          \]
          \begin{itemize}
            \item En appliquant $(\star)$ pour $f\leftarrow \bigl(x\mapsto (1+x)^{\alpha}\bigr)$ et $x_0\leftarrow 0$, on a
                  \[
                    \lim_{x\to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x\to x_0}\frac{(1+x)^{\alpha}-(1+0)^{\alpha}}{x-0} = \lim_{x\to 0}\frac{(1+x)^{\alpha}-1}{x}=f'(0)=\alpha
                  \]
                  car $f': x\mapsto 1\cdot \alpha\cdot (1+x)^{\alpha - 1}$ vaut $\alpha$ en $0$.
            \item De même, en appliquant $(\star)$ pour $f\leftarrow \sin$ et $x_0\leftarrow 0$, on a
                  \[
                    \lim_{x\to 0}\frac{f(x)-f(0)}{x-0}=\lim_{x\to x_0}\frac{\sin x-\sin 0}{x-0} = \lim_{x\to 0}\frac{\sin x}{x}=\sin' 0=\cos 0=1
                  \]
          \end{itemize}
    \item Soit $x\in\R^*$ fixé quelconque.
          \[
            \frac{1 - \cos x}{x^2} = \frac{(1-\cos x)(1+\cos x)}{x^2(1+\cos x)} = \frac{1-\cos^2 x}{x^2(1+\cos x)}=\biggl(\!\!\!\smash{\underbrace{\frac{\sin x}{x}}_{\arrowlim{x}{0}1}}\!\!\!\biggr)^2\cdot\underbrace{\left(\frac{1}{1+\cos x}\right)}_{\arrowlim{x}{0}\frac{1}{2}} \arrowlim{x}{0}\frac{1}{2}
          \]
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arcsin$.}
  Premièrement, ladite fonction est la bijection réciproque de la fonction $\bigl.\sin\bigr|_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]}^{[-1,1]}$ (voir \textbf{1}.). D'où :
  \begin{equation*}
    \arcsin = \left\{
    \begin{array}{c c c}
      [-1,1] & \to     & [-\frac{\pi}{2} , \frac{\pi}{2}]                                                             \\ [1ex]
      x      & \mapsto & \left(\bigl.\sin\bigr|_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]}^{[-1,1]}\right)^{-1}(x)
    \end{array}
    \right.
  \end{equation*}
  Ainsi, pour $x\in [-1,1]$, $\arcsin (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left[-\frac{\pi}{2} , \frac{\pi}{2}\right]$ :
  \[
    \sin(\theta) = x
  \]
  .
  \noindent Il découle alors naturellement des propriétés héréditairement acquises de $\bigl.\sin\bigr|_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]}^{[-1,1]}$ :

  \begin{enumerate}
    \item $\arcsin$ est impaire.
    \item $\arcsin$ est strictement croissante sur $[-1,1]$.
    \item $\arcsin \in \mathcal{C}^0\left([-1,1],[-\frac{\pi}{2} , \frac{\pi}{2}] \right)$.
    \item $\arcsin \in \mathcal{D}^1\left(]-1,1[,\left]-\frac{\pi}{2} , \frac{\pi}{2}\right[ \right)$.
    \item $\arcsin'(x) = \frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
    \item $\arcsin$ admet deux demi-tangentes verticales en $-1$ et $1$.
  \end{enumerate}

  \

  Graphe de $\arcsin$ :
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=2]
      \draw [->] (-pi/2-0.5,0) -- (pi/2+0.5,0);
      \draw [->] (0,-pi/2-0.5) -- (0,pi/2+0.5);

      \draw (-pi/2,-0.05) -- (-pi/2,0.05) node[below, yshift=-3mm] {\footnotesize $-\frac{\pi}{2}$};
      \draw (-1,-0.05) -- (-1,0.05) node[below, yshift=-3mm] {\footnotesize $-1$};
      \draw (1,-0.05) -- (1,0.05) node[below, yshift=-3mm] {\footnotesize $1$};
      \draw (pi/2,-0.05) -- (pi/2,0.05) node[below, yshift=-3mm] {\footnotesize $\frac{\pi}{2}$};

      \draw (-0.05,-pi/2) -- (0.05,-pi/2) node[left, xshift=-3mm] {\footnotesize $-\frac{\pi}{2}$};
      \draw (-0.05,-1) -- (0.05,-1) node[left, xshift=-3mm] {\footnotesize $-1$};
      \draw (-0.05,1) -- (0.05,1) node[left, xshift=-3mm] {\footnotesize $1$};
      \draw (-0.05,pi/2) -- (0.05,pi/2) node[left, xshift=-3mm] {\footnotesize $\frac{\pi}{2}$};

      \draw[thick, domain=-1.5:1.5, smooth, variable=\x, lime] plot ({\x}, {sin(\x*180/pi)});
      \draw[thick, domain=-1.5:1.5, smooth, variable=\x, teal] plot ({\x}, {\x});
      \draw[thick, domain=-1:1, samples=100, variable=\x, purple] plot ({\x}, {pi*asin(\x)/180});
      \draw [->] (-1,-pi/2) -- (-1,-pi/2+0.5);
      \draw [->] (1,pi/2) -- (1,pi/2-0.5);
      \draw [->] (pi/2,1) -- (pi/2-0.5,1);
      \draw [->] (-pi/2,-1) -- (-pi/2+0.5,-1);
    \end{tikzpicture}
    \caption{$\arcsin$ en violet, $\sin$ en vert et la première bissectrice en bleu.}
  \end{figure}
  On a aussi, grâce au taux d'accroissement en 0 d'$\arcsin$ :
  \[
    \lim_{x\to0} \frac{\arcsin(x)}{x} \ = \ 1.
  \]

  \

  Puis finalement (visible sur le graphe) :
  \[
    \forall x \in [0,1], \quad \arcsin(x) \geq x.
  \]
\end{question_kholle}

\begin{question_kholle}{Étude et tracé de $\arcsin\circ\sin$ (avec réduction du domaine d'étude à $[0,\pi/2]$).}\hfill
  \begin{itemize}[label=$*$]
    \item La fonction $\sin$ est définie sur $\R$ et $\sin(\R)=[-1,1] = \mathcal{D}_{\arcsin}$ donc $\arcsin\circ\sin$ est définie sur $\R$.
    \item La fonction $\arcsin\circ\sin$ est $2\pi$-périodique car $\sin$ est $2\pi$-périodique. On peut donc restreindre le domaine d'étude à un intervalle d'amplitude $2\pi$, par exemple $[-\pi,\pi]$.
    \item La fonction $\arcsin\circ\sin$ est impaire comme composée de fonctions impaires. L'intervalle $[-\pi,\pi]$ étant centré en $0$, on peut restreindre le domaine d'étude à $[0,\pi]$.
    \item De plus, pour tout $x\in [0,\pi]$, $\sin(\pi - x)=\sin x$ donc $(\arcsin\circ\sin)(\pi-x)=(\arcsin\circ\sin)(x)$ .
  \end{itemize}
  Il suffit donc d'étudier le graphe fonction $\arcsin\circ\sin$ sur l'intervalle $\left[0,\frac{\pi}{2}\right]$, son graphe sur $[0,\pi]$ s'en déduisant par une réflexion d'axe la droite d'équation $x=\frac{\pi}{2}$, ce qui nous permet, par imparité et $2\pi$-périodicité, de trouver son graphe sur $\R$ par réflexion sur l'axe des ordonnées et translations successives de vecteur $2\pi\vv{i}$.\\
  Sachant que pour tout $x\in\left[0,\frac{\pi}{2}\right], (\arcsin\circ\sin)(x) = x$, on a le graphe suivant~:

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.5]
      \draw [->] (-3*pi/2,0) -- (3*pi/2,0);
      \draw [->] (0,-pi/2-0.5) -- (0,pi/2+0.5);
      \draw [dashed] (pi/2,-pi/2-0.5) -- (pi/2,pi/2+0.5);
      \draw [thick, ->] (0,0) -- (1,0) node[midway, below] {$\vv{i}$};

      % Graduations
      \draw (pi/2,-0.05) -- (pi/2,0.05) node[below, yshift=-3mm, fill=white] {\footnotesize $\frac{\pi}{2}$};
      \draw (pi,-0.05) -- (pi,0.05) node[below, yshift=-3mm] {\footnotesize $\pi$};
      \draw (-0.05,pi/2) -- (0.05,pi/2) node[left, xshift=-3mm] {\footnotesize $\frac{\pi}{2}$};

      % Courbes
      \draw[thick, dashed, domain=-3*pi/2:-3*pi/2+0.3, variable=\x, gray] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
      \draw[thick, dashed, domain=3*pi/2-0.3:3*pi/2, variable=\x, gray] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
      \draw[thick, domain=-3*pi/2+0.3:3*pi/2-0.3, samples=100, variable=\x, gray] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
      \draw[thick, domain=0:pi/2, variable=\x, purple] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
      \draw[thick, domain=pi/2:pi, variable=\x, teal] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
      \draw[thick, domain=-pi:0, variable=\x, lime] plot ({\x}, {pi*asin(sin(\x*180/pi))/180});
    \end{tikzpicture}
    \caption{Graphe de $\arcsin\circ\sin$.}
  \end{figure}
\end{question_kholle}
\pagebreak\section{Semaine 6}

\begin{question_kholle}{Liens entre le graphe de $f$ et ceux de $g$ et $h$ définies par $g(x)=af(x)$ et $h(x)=f(x+a)$.}
  \hfill\\
  \begin{minipage}{0.5\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node at (3.3,2.5) {$\mathcal{D}_g=\mathcal{D}_f$};
        \draw [->] (-1,0) -- (4,0);
        \draw [->] (0,-2) -- (0,3);

        \coordinate (A) at (1.5, 2);
        \coordinate (B) at (1.5, -1);

        \draw [dashed, gray] (0, 2) -- (A) -- (1.5,0);
        \draw [dashed, gray] (0, -1) -- (B) -- (1.5,0);

        \fill [radius=1.5pt] (A) circle node[above=2pt] {\small $(x,g(x))$};
        \fill [radius=1.5pt] (B) circle node[below=2pt] {\small $(x,f(x))$};

        \draw[-Stealth, thick] (1.7,-1) arc (-90:90:1.5) node [midway, right] {$\bm{\times a}$};

        \draw (1.5,-0.05) -- (1.5,0.05) node[fill=white, above] {\footnotesize $x$};
        \draw (-0.05,2) -- (0.05,2) node[left, xshift=-3mm] {\footnotesize $g(x)$};
        \draw (-0.05,-1) -- (0.05,-1) node[left, xshift=-3mm] {\footnotesize $f(x)$};
      \end{tikzpicture}
      \caption{Lien entre le graphe de $f$ et de $g$.}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node at (2.5,3.5) {$\mathcal{D}_h=\{x-a\mid x\in\mathcal{D}_f\}$};
        \draw [->] (-1,0) -- (5,0);
        \draw [->] (0,-1) -- (0,4);

        \coordinate (A) at (1.2, 2);
        \coordinate (B) at (3.5, 2);

        \draw (1.2,-0.05) -- (1.2,0.05) node[below, yshift=-1mm] {\footnotesize $x$};
        \draw (3.5,-0.05) -- (3.5,0.05) node[below, yshift=-1mm] {\footnotesize $x+a$};
        \draw (-0.05,2) -- (0.05,2) node[left, xshift=-3mm] {\footnotesize $f(x+a)$};

        \draw [dashed, gray] (0,2) -- (A) -- (1.2, 0);
        \draw [dashed, gray] (0,2) -- (B) -- (3.5, 0);

        \fill [radius=1.5pt] (A) circle node[above=2pt] {\small $(x,h(x))$};
        \fill [radius=1.5pt] (B) circle node[below=2pt, fill=white] {\small $(x+a, f(x+a))$};

        \draw (A) node [below=1ex, white, fill=white] {a}; % TODO: Dirty fix. Change it.
        \draw [-Stealth, thick] (B) -- (A) node [below] {$\bm{-a}\vv{\bm{i}}$};
      \end{tikzpicture}
      \caption{Lien entre le graphe de $f$ et de $h$.}
    \end{figure}
  \end{minipage}
\end{question_kholle}

\begin{question_kholle}{Liens entre le graphe de $f$ et ceux de $g$ et $h$ définies par $g(x)=f(ax)$ et $h(x)=f(a-x)$.}
  \hfill\\
  \begin{minipage}{0.5\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \node at (2.5,3.5) {$\mathcal{D}_g=\left\{\frac{x}{a}\mid x\in\mathcal{D}_f\right\}$};
        \draw [->] (-1,0) -- (5,0);
        \draw [->] (0,-1) -- (0,4);

        \coordinate (A) at (1.2, 2);
        \coordinate (B) at (3.5, 2);

        \draw (1.2,-0.05) -- (1.2,0.05) node[below, yshift=-1mm] {\footnotesize $x$};
        \draw (3.5,-0.05) -- (3.5,0.05) node[below, yshift=-1mm] {\footnotesize $ax$};
        \draw (-0.05,2) -- (0.05,2) node[left, xshift=-3mm] {\footnotesize $f(ax)$};

        \draw [dashed, gray] (0,2) -- (A) -- (1.2, 0);
        \draw [dashed, gray] (0,2) -- (B) -- (3.5, 0);

        \fill [radius=1.5pt] (A) circle node[above=2pt] {\small $(x,g(x))$};
        \fill [radius=1.5pt] (B) circle node[above=2pt] {\small $(ax, f(ax))$};

        \draw [-Stealth, thick] (3.5, 1.9) arc (0:-180:1.15) node [midway, above=2pt] {$\bm{\times \frac{1}{a}}$};
      \end{tikzpicture}
      \caption{Lien entre le graphe de $f$ et de $g$.}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \begin{figure}[H]
      \centering
      \begin{tikzpicture}
        \draw [->] (-1,0) -- (5,0);
        \draw [->] (0,-1) -- (0,4);

        \coordinate (A) at (0.8, 2);
        \coordinate (B) at (3.7, 2);

        \draw [dashed, gray] (0,2) -- (A) -- (0.8, 0);
        \draw [dashed, gray] (0,2) -- (B) -- (3.7, 0);

        \fill [radius=1.5pt] (A) circle node[above=2pt] {\small $(x,h(x))$};
        \fill [radius=1.5pt] (B) circle node[above=2pt] {\small $(x-a, f(x-a))$};

        \draw [thick, dashed] (2.25, -1) -- (2.25, 3.5);

        \draw (0.8,-0.05) -- (0.8,0.05) node[below, yshift=-1mm] {\footnotesize $x$};
        \draw (3.7,-0.05) -- (3.7,0.05) node[below, yshift=-1mm] {\footnotesize $a-x$};
        \draw (2.25,-0.05) -- (2.25,0.05) node[below, yshift=-1mm, fill=white] {\footnotesize $\frac{a}{2}$};
        \draw (-0.05,2) -- (0.05,2) node[left, xshift=-3mm] {\footnotesize $f(a-x)$};

        \node [fill=white] at (2.5,3.5) {$\mathcal{D}_g=\{a-x \mid x\in\mathcal{D}_f\}$};
      \end{tikzpicture}
      \caption{Lien entre le graphe de $f$ et de $h$.}
    \end{figure}
  \end{minipage}
\end{question_kholle}

\pagebreak

\begin{question_kholle}{Présentation exhaustive de la fonction $\arccos$.}

  Premièrement, ladite fonction est la bijection réciproque de la fonction $\bigl.\cos\bigr|_{[0,\pi]}^{[-1,1]}$. D'où :
  \[
    \arccos = \left|
    \begin{array}{rcl}
      [-1,1] & \to     & [0 , \pi]                                                 \\ [1ex]
      x      & \mapsto & \left( \bigl.\cos\bigr|_{[0,\pi]}^{[-1,1]}\right)^{-1}(x)
    \end{array}
    \right.
  \]
  Ainsi, pour $x\in [-1,1]$, $\arccos (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle [0 ,\pi]$, \[\cos(\theta) = x\].

  \noindent Il découle alors naturellement des propriétés héréditairement acquises de $\bigl.\cos\bigr|_{[0,\pi]}^{[-1,1]}$ :

  \begin{enumerate}
    \item $\arccos$ est strictement décroissante sur $[-1,1]$.
    \item $\arccos \in \mathcal{C}^0\left([-1,1],[0 , \pi] \right)$.
    \item $\arccos \in \mathcal{D}^1\left(]-1,1[,]0 ,\pi [ \right)$.
    \item $\displaystyle\arccos'(x) = -\frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
    \item $\arccos$ admet deux demi-tangentes verticales en $-1$ et $1$.
  \end{enumerate}

  \

  Graphe de $\arccos$ :
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=2]
      \draw [->] (-1.5,0) -- (pi+0.5,0);
      \draw [->] (0,-1.5) -- (0,pi+0.5);

      \draw (1,-0.05) -- (1,0.05) node[below, yshift=-3mm] {\footnotesize $1$};
      \draw (-1,-0.05) -- (-1,0.05) node[below, yshift=-3mm] {\footnotesize $-1$};
      \draw (pi/2,-0.05) -- (pi/2,0.05) node[below, yshift=-3mm] {\footnotesize $\frac{\pi}{2}$};
      \draw (pi,-0.05) -- (pi,0.05) node[below, yshift=-3mm] {\footnotesize $\pi$};

      \draw (-0.05,1) -- (0.05,1) node[left, xshift=-3mm] {\footnotesize $1$};
      \draw (-0.05,-1) -- (0.05,-1) node[left, xshift=-3mm] {\footnotesize $-1$};
      \draw (-0.05,pi/2) -- (0.05,pi/2) node[left, xshift=-3mm] {\footnotesize $\frac{\pi}{2}$};
      \draw (-0.05,pi) -- (0.05,pi) node[left, xshift=-3mm] {\footnotesize $\pi$};

      \draw[thick, domain=0:pi, smooth, variable=\x, lime] plot ({\x}, {cos(\x*180/pi)});
      \draw[thick, domain=-1:1, samples=100, variable=\x, purple] plot ({\x}, {pi*acos(\x)/180});
      \draw[thick, domain=-1:pi, smooth, variable=\x, teal] plot ({\x}, {\x});
      \draw[dashed, thick, domain=-1.5:pi, smooth, variable=\x, gray] plot ({\x}, {pi/2-\x}) node[left, xshift=1cm, fill=white] {$y=\frac{\pi}{2}-x$};

      \draw [->] (-1,pi) -- (-1,pi-0.5);
      \draw [->] (1,0) -- (1,0.5);
      \draw [->] (pi,-1) -- (pi-0.5,-1);
      \draw [->] (0,1) -- (0.5,1);
    \end{tikzpicture}
    \caption{$\arccos$ en violet, $\cos$ en vert et la première bissectrice en bleu.}
  \end{figure}
\end{question_kholle}

\pagebreak

\begin{question_kholle}{Présentation exhaustive de la fonction $\arctan$.}
  Premièrement, ladite fonction est la bijection réciproque de la fonction $\bigl. \tan\bigr| _{\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ }$. D'où :
        \[
          \arctan = \left|\begin{array}{rcl}
            \mathbb{R} & \to     & \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[                                         \\ [1ex]
            x          & \mapsto & \left(\bigl.\tan\bigr|_{\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[}\right)^{-1}(x)
          \end{array}\right.
        \]
        Ainsi, pour $x\in \mathbb{R}$, $\arctan (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[$,
  \[
    \tan(\theta) = x
  \]
  \noindent Il découle alors naturellement des propriétés héréditairement acquises de $\bigl.\tan\bigr|_{\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[}$ :

  \begin{enumerate}
    \item $\arctan$ est impaire.
    \item $\arctan \in \mathcal{C}^0\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
    \item $\arctan \in \mathcal{D}^1\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
    \item $\displaystyle\arctan'(x) = \frac{1}{1+x^2}$ pour tout $x\in\mathbb{R}$.
  \end{enumerate}

  Graphe de $\arctan$ :
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.5]
      \draw [->] (-pi-0.5,0) -- (pi+0.5,0);
      \draw [->] (0,-pi-0.5) -- (0,pi+0.5);

      \draw (-pi/2,-0.05) -- (-pi/2,0.05) node[below, xshift=-5mm, yshift=-3mm] {\footnotesize $-\frac{\pi}{2}$};
      \draw (pi/2,-0.05) -- (pi/2,0.05) node[below, xshift=-3mm, yshift=-3mm] {\footnotesize $\frac{\pi}{2}$};

      \draw (-0.05,pi/2) -- (0.05,pi/2) node[left, xshift=-3mm, yshift=-5mm] {\footnotesize $\frac{\pi}{2}$};
      \draw (-0.05,-pi/2) -- (0.05,-pi/2) node[left, xshift=-3mm, yshift=-5mm] {\footnotesize $-\frac{\pi}{2}$};

      \draw[thick, domain=-pi-0.5:pi+0.5, samples=100, variable=\x, purple] plot ({\x}, {pi*atan(\x)/180});
      \clip(-pi-0.5,-pi-0.5) rectangle (pi+0.5,pi+0.5);
      \draw[thick, domain=-pi/2+0.25:pi/2-0.25, smooth, variable=\x, lime] plot ({\x}, {tan(\x*180/pi)});
      \draw[thick, domain=-pi:pi, smooth, variable=\x, teal] plot ({\x}, {\x});

      \draw[dashed, thick, gray] (-pi-0.5, pi/2) -- (pi+0.5,pi/2);
      \draw[dashed, thick, gray] (-pi-0.5, -pi/2) -- (pi+0.5,-pi/2);
      \draw[dashed, thick, gray] (pi/2, -pi-0.5) -- (pi/2, pi+0.5);
      \draw[dashed, thick, gray] (-pi/2, -pi-0.5) -- (-pi/2, pi+0.5);
    \end{tikzpicture}
    \caption{$\arctan$ en violet, $\tan$ en vert et la première bissectrice en bleu.}
  \end{figure}
  On a aussi (visible sur le graphe) :
  \[
    \forall x \in \mathbb{R}_+, \quad \arctan(x) \leq x.
  \]

  Et enfin : % TODO: Graphe
  \[
    \forall x \in \mathbb{R}^*, \quad \arctan(x) + \arctan \left( \frac{1}{x} \right) =
    \left\{ \begin{array}{cl}
      \frac{\pi}{2}  & \text{si } x \ > \ 0  \\
      -\frac{\pi}{2} & \text{si } x \ < \ 0.
    \end{array} \right.
  \]

\end{question_kholle}

\begin{question_kholle}{2 preuves de $\arcsin(x) + \arccos(x) =\frac{\pi}{2}$ sur $[-1,1]$, dont une basée sur une interprétation géométrique du cercle trigonométrique.}
  On remarque sur la \autoref{sem6:q4:1} que la droite d'équation polaire $\displaystyle\theta=\frac{\pi}{4}$ est axe de symétrie de la figure. On a donc
  \[
    \frac{\arcsin x+\arccos x}{2} = \frac{\pi}{4}
  \]
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=4]
      \draw [ultra thick, violet] (1.008,0) arc (0:180:1.008);
      \draw [ultra thick, teal] (0,0.992) arc (90:-90:0.992);
      \draw (0,0) circle (1);
      \draw (-1.2,0) -- (1.2,0);
      \draw (0,-1.2) -- (0,1.2);
      \draw [thick, gray, domain=-1.2:1.2] plot (\x,\x) node[below right] {$y=x$};

      \coordinate (O) at (0,0);
      \coordinate (U) at (1,0);

      \draw [thick] (0,0) -- ({cos(30)},{sin(30)}) coordinate (A);
      \draw [thick] (0,0) -- ({cos(60)},{sin(60)}) coordinate (B);
      \draw pic[-Stealth,"$\arccos x$",draw=black,thick, angle radius=55,angle eccentricity=1.4, pic text options={shift={(1mm,-2mm)}}]{angle = U--O--A};
      % \draw pic[-Stealth,"$\arccos x$",draw=black,thick, angle radius=55,angle eccentricity=1.4, pic text options={fill=white,shift={(-2mm,-2mm)}}]{angle = B--O--(0,1)};
      \draw pic[-Stealth,"$\arcsin x$",draw=black,thick,angle radius=50,angle eccentricity=1.4, pic text options={fill=white,shift={(0,3mm)}}]{angle = U--O--B};
      \draw [thick, dashed, gray] (A) -- ({cos(30)},0) node[below, black] {$x$};
      \draw [thick, dashed, gray] (B) -- (0,{cos(30)}) node[left, black] {$x$};

      %---
      \draw [thick] (0,0) -- ({-cos(30)},{sin(30)}) coordinate (C);
      \draw [thick] (0,0) -- ({cos(60)},{-sin(60)}) coordinate (D);
      \draw pic[-Stealth,"$\arccos x'$",draw=black,thick, angle radius=35,angle eccentricity=1.4, pic text options={shift={(-13mm,-2mm)}}]{angle = U--O--C};
      \draw pic[Stealth-,"$\arcsin x'$",draw=black,thick,angle radius=30,angle eccentricity=1.4, pic text options={shift={(5mm,2mm)}}]{angle = D--O--U};
      \draw [thick, dashed, gray] (C) -- ({-cos(30)},0) node[below, black] {$x'$};
      \draw [thick, dashed, gray] (D) -- (0,{-cos(30)}) node[left, black] {$x'$};


    \end{tikzpicture}
    \caption{Illustration de la relation $\arccos x + \arcsin x = \frac{\pi}{2}$ pour $x\geq 0$ et $x'\leq 0$.\\ En violet le domaine de définition de $\arccos$ et en bleu celui de $\arcsin$.}
    \label{sem6:q4:1}
  \end{figure}

  \noindent\textbf{Preuve formelle :}
  Soit $x\in [-1,1]$. Posons  $\varphi \ = \ \arcsin(x) \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Ainsi :
  \[
    \arcsin(x) + \arccos(x) \ = \ \varphi + \arccos(\sin(\varphi)) \ = \ \varphi + \arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right),
  \]

  \

  or $\varphi \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ donc
  $\frac{\pi}{2}- \varphi \in [0,\pi]$ d'où $\arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right) = \frac{\pi}{2}- \varphi$ si bien que :
  \[
    \arcsin(x) + \arccos(x) \ = \  \varphi +\frac{\pi}{2} - \varphi \ = \ \frac{\pi}{2}.
  \]

\end{question_kholle}

\pagebreak

\begin{question_kholle}{Étude analytique rapide des fonctions \(\cosh\) et \(\sinh \).}
  ~\smallbreak

  \begin{itemize}[label=$\bullet$]
    \item \textit{Domaine de définition et symétries:}
          $\sinh$ et $\cosh$ sont définies sur $\mathbb{R}$.
          De plus,
          \begin{propositions}
            \item $\forall x \in \mathbb{R}, -x\in \mathbb{R}$,
            \item $\forall x \in \mathbb{R},
              \left\{ \begin{array}{c c c c c c c}
                \sinh (-x) & = & \displaystyle\frac{e^{-x} - e^{x}}{2}     & = & -\displaystyle\frac{e^x - e^{-x}}{2} & = & -\sinh(x) \\
                \text{et}  &   &                                           &   &                                      &   &           \\
                \cosh (-x) & = & \displaystyle\frac{e^{-x} + e^{-(-x)}}{2} & = & \displaystyle\frac{e^x + e^{-x}}{2}  & = & \cosh(x).
              \end{array}
              \right.
            $
          \end{propositions}
          Donc $\sinh$ et $\cosh$ sont respectivement impaire et paire.
          \newline
          Nous les étudierons sur $\mathbb{R}_+$ et pour les obtenir les graphes $(\mathcal{C}_{\sinh} \text{ et } \mathcal{C}_{\cosh})$ de ces fonctions sur $\mathbb{R}$ à partir de ceux $(\mathcal{C}_{\sinh}^+ \text{ et } \mathcal{C}_{\cosh}^+)$ obtenus sur $\mathbb{R}_+$, nous le complèterons en traçant les images de ces graphes par la symétrie centrale $s$ de centre $O$ et par la réflexion $r$ d'axe $\left( O, \overrightarrow{\jmath} \right)$ :
          \[
            \mathcal{C}_{\sinh} = \mathcal{C}_{\sinh}^+ \cup s \left( \mathcal{C}_{\sinh}^+ \right) \qquad \text{ et } \qquad \mathcal{C}_{\cosh} = \mathcal{C}_{\cosh}^+ \cup r \left( \mathcal{C}_{\cosh}^+ \right)
          \]
    \item \textit{Variations :} La fonction $\sinh' = \cosh$ est strictement positive sur \R donc $\sinh$ est strictement croissante sur \R. On en déduit alors le signe de $\sinh=\cosh'$ et donc les variations de $\cosh$:
          \begin{center}
            \begin{tikzpicture}
              \tkzTabInit[lgt=4, espcl=3]
              {$x$ / 1, $\cosh' x=\sinh x$ / 1, $\cosh$ / 2, $\sinh'(x)=\cosh x$ / 1, $\sinh$ / 2}
              {$-\infty$, $0$, $+\infty$}

              % Ligne des variations pour cosh(x)
              \tkzTabLine{ , -, z , + }
              \tkzTabVar{ +/$+\infty$, -/ 1, +/$+\infty$ }

              % Ligne des variations pour sinh(x)
              \tkzTabLine{ , +, , + }
              \tkzTabVar{ -/$-\infty$, R/, +/$+\infty$ }
              \tkzTabIma{1}{3}{2}{$0$}
            \end{tikzpicture}
          \end{center}

    \item \textit{Branches infinies en $+\infty$ et position relative de $\mathcal{C}_{\sinh}$ et $\mathcal{C}_{\cosh}$.}

          \[
            \frac{\cosh(x)}{x} = \underset{\xrightarrow[x\to +\infty]{} \ +\infty}{\underbrace{\frac{e^{x}}{x}}} + \underset{\xrightarrow[x\to +\infty]{} \ 0}{\underbrace{\frac{e^{-x}}{x}}} \xrightarrow[x\to +\infty]{} \ +\infty
          \]
          Donc \textbf{le graphe de $\bm{\cosh}$ admet une branche parabolique de direction asymptotique $\bm{\left( O, \overrightarrow{\jmath}\right)}$}.
          De plus,
          \[
            \forall x \in \mathbb{R}, \quad \cosh(x) - \sinh(x) = e^{-x} \xrightarrow[x\to +\infty]{} 0^+
          \]
          Ainsi, les graphes des deux fonctions se rapprochent l'un de l'autre arbitrairement près lorsque $x \to +\infty$, et le graphe de $\cosh$ est au-dessus de celui de $\sinh$.

    \item \textit{Tangente au graphe de $\sinh$ à l'origine et position relative.}\\
          Posons l'application
          \[
            g\left|\begin{array}{rcl}\R_+ & \longrightarrow & \R \\ x & \longmapsto & \sinh(x) - x\end{array}\right.
          \]
          Elle est dérivable sur son ensemble de définition, sa dérivée est positive sur cet intervalle ($\forall x\in\R_+, g'(x)=\cosh x - 1 \geq 0$), et $g(0)=0$ donc \textbf{le graphe de $\bm{\sinh}$ est situé au dessus de sa tangente sur $\bm{\R_+}$}.\\
          Par imparité de la fonction $\sinh$, la position relative courbe / tangente s'inverse si bien que \textbf{l'origine est un point d'inflexion du graphe de $\bm{\sinh}$}.
  \end{itemize}
\end{question_kholle}

\pagebreak

\begin{question_kholle}{Calcul de $\displaystyle\int_0^{2\pi}e^{imt} \mathrm d t$ en fonction de $m \in \Z$. En Déduire qu'une fonction polynomiale nulle sur un cercle centré en l'origine a tous ses coefficients nuls.}
  Soit $m \in \Z$ fixé quelconque. Calculons
  $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t$$
  \begin{itemize}[label=$\star$]
    \item Si $m \neq 0$:
          \begin{align*}
            \frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t & = \frac{1}{2 \pi} \Big[ \frac{e^{mt}}{im} \Big]_0^{2\pi}      \\
                                                             & = \frac{1}{2 \pi} \Big( \frac{1}{im} - \frac{1}{im} \Big) = 0
          \end{align*}
    \item Si $m = 0$:
          $$
            \frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t = \frac{1}{2 \pi} \int_0^{2\pi} \mathrm d t = \frac{2 \pi}{2 \pi} = 1
          $$
  \end{itemize}
  Donc $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t =
    \begin{cases}
      1 \text{ si } m=0 \\
      0 \text{ si } m \neq 0
    \end{cases}
    \bigl(= \delta_{m, 1}\bigr)
  $$
  \\
  Soit $n\in \N$ et $(a_0, ..., a_n) \in \C^{n+1}$ fixés quelconques. Posons, pour tout $z\in\C, P(z) = \sum_{k=0}^n a_k z^k$.\\
  Soient $s\in\Z$ et $R\in\R_+^*$ fixés quelconques.
  \begin{align*}
    \frac{1}{2 \pi} \int_0^{2\pi} P(Re^{it}) e^{-ist} \mathrm d t & = \frac{1}{2 \pi} \int_0^{2\pi} \bigg (\sum_{k=0}^n a_k (Re^{it})^k \bigg) e^{-ist} \mathrm d t                                                                                                                                                                                                                                                     \\
                                                                  & = \frac{1}{2\pi}\sum_{k=0}^{n}\left(\int_{0}^{2\pi}a_kR^ke^{it(k-s)}\right)                                                                                                                                                                                                                               \quad\text{ par linéarité de l'intégrale} \\
                                                                  & = \sum_{k=0}^n\left(a_k R^k\cdot\smash[b]{\underbrace{\frac{1}{2\pi}\int_0^{2\pi} e^{it(k-s)} \dd t}_{=\begin{cases}0&\text{ si $k\neq s$} \\ 1 &\text{sinon}\end{cases}}}\right) \vphantom{\underbrace{\frac{1}{2\pi}\int_0^{2\pi} e^{it(k-s)} \dd t}_{=\begin{cases}0&\text{ si $k\neq s$} \\ 1 &\text{sinon}\end{cases}}}                        \\
                                                                  & = \begin{cases}0 & \text{ si $s\not\in\iset{0,n}$}\\\displaystyle a_sR^s &\text{ sinon}\end{cases}
  \end{align*}
  Supposons à présent qu'il existe un cercle centré en l'origine sur lequel $P$ est identiquement nulle. Notons $R\in\R_+^*$ le rayon d'un tel cercle. Alors,
  \[
    \forall t\in\R,\; P(Re^{it})=0
  \]
  donc
  \[
    \forall s\in\Z,\; \frac{1}{2\pi}\int_0^{2\pi}P(Re^{it})e^{-ist}\dd t = \frac{1}{2\pi}\int_0^{2\pi}0\,\dd t=\frac{1}{2\pi}\bigl[1\bigr]_0^{2\pi}=0
  \]
  or, nous avons vu que
  \[
    \forall s\in\iset{0,n},\; \frac{1}{2\pi}\int_0^{2\pi}P(Re^{it})e^{-ist}\dd t = a_s R^s
  \]
  donc
  \[
    \forall s\in\iset{0,n},\; a_sR^s = 0 \quad\text{d'où}\quad \forall z\in\iset{0,n}, a_s = 0
  \]
  ainsi, $P$ est la fonction polynomiale nulle sur $\C$.
\end{question_kholle}


\pagebreak

\begin{question_kholle}[{
        \[
          \int_{a}^{b} u'(t)v(t) \dd t= u(b)v(b) - u(a)v(a) - \int_{a}^{b} u(t)v'(t) \dd t
        \]
      }]{Technique de l'intégration par parties.}
  Il suffit de reconnaître un terme issu de la dérivée d'un produit de fonctions:
  $$
    (uv)' = u'v + uv' \implies u'v = (uv)' - uv'
  $$

  d'où :

  \begin{align*}
    \int_{a}^{b} u'(t)v(t)\dd t & = \int_{a}^{b} \bigl((uv)'(t) - u(t)v'(t)\bigr) \dd t                                         \\
                                & = \int_{a}^{b} (uv)'(t)\dd t - \int_{a}^{b}u(t)v'(t)\dd t  \text{ (linéarité de l'intégrale)} \\
                                & = \Bigl[ u(t)v(t)\Bigr]_{a}^{b} - \int_{a}^{b} u(t)v'(t)\dd t                                 \\
                                & = u(b)v(b) - u(a)v(a) - \int_{a}^{b} u(t)v'(t)\dd t
  \end{align*}
  La preuve sera suivie d'exemples explicites aux choix de l'examinateur.

\end{question_kholle}

\begin{question_kholle}[{				\[
          \int_{\ph(a)}^{\ph(b)} f(s)\dd s \underset{\substack{\dd s = \ph'(u)\dd u \\ s = \ph(u)}}{=} \int_{a}^{b} f(\ph(u))\ph'(u)\dd u
        \]
      }]{Technique du changement de variable.}
  Il suffit de reconnaître la dérivée d'une composée de fonctions.
  En effet, en notant $F$ une primitive de $f$ sur $I$ (ce qui a bien un sens car $f$ est continue sur $I$),

  \begin{align*}
    \int_{a}^{b} f(\ph(u))\ph'(u)\dd u & =
    \left[ (F \circ \ph)(u)\right]_{a}^{b}                                  \\
                                       & =	F(\ph(b)) - F(\ph(a))            \\
                                       & = \int_{\ph(a)}^{\ph(b)} f(s)\dd s
  \end{align*}
  La preuve sera suivie d'exemples explicites aux choix de l'examinateur.


\end{question_kholle}

\pagebreak

\begin{question_kholle}{Montrer que si $f$ est $T$-périodique sur $\R$, pour tout $a\in\R$, $\displaystyle\int_{a}^{a+T}f(t)\dd t = \int_{0}^{T}f(t)\dd t$.}
  Il existe $k_0\in\Z$ tel que $a\leq k_0 T<a+T$ car
  \begin{equation*}
    a\leq k_0T<a+T \iff \frac{a}{T} \leq K_0 < \frac{a}{T}+1 \iff k_0 = \left\lceil \frac{a}{T}\right\rceil
  \end{equation*}
  Fixons un tel $k_0$. Ainsi,
  \begin{equation}\label{S6:Q10:1}
    \int_{a}^{a+T}{f(t)\dd t} = \int_{a}^{k_0T}{f(t)\dd t} + \int_{k_0T}^{a+T}{f(t)\dd t}
  \end{equation}
  Or,
  \begin{equation}\label{S6:Q10:2}
    \int_{k_0T}^{a+T}{f(t)\dd t} \underset{\substack{u=t-k_0T \\ \dd u=\dd t}}{=} \int_{0}^{a+T-k_0T}{f(u+k_0T)\dd u} = \int_{0}^{a-(k_0-1)T}{f(u)\dd u}
  \end{equation}
  Et
  \begin{equation}\label{S6:Q10:3}
    \int_{a}^{k_0T}{f(t)}{\dd t} \underset{\substack{u=t-(k_0-1)T \\ \dd u=\dd t}}{=} \int_{a-(k_0-1)T}^{T}{f(u+(k_0-1)T)\dd u} = \int_{a-(k_0-1)T}^{T}{f(u)\dd u}
  \end{equation}
  La relation \eqref{S6:Q10:1} donne alors, en utilisant \eqref{S6:Q10:2} et \eqref{S6:Q10:3},
  \begin{equation}\label{S6:Q10:4}
    \int_{a}^{a+T}{f(t)\dd t} = \int_{0}^{a-(k_0-1)T}{f(t)\dd t}+\int_{a-(k_0-1)T}^{T}{f(t)\dd t} = \int_{0}^{T}{f(t)\dd t}
  \end{equation}

  On peut visualiser cette démonstration sur un graphe :

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=0.7]
      \def\offset{0.8}
      \definecolor{mycolor}{HTML}{44a2ce}

      \fill [mycolor, samples=500, domain=0:2*pi-\offset, variable=\x]
      (0, 0)
      -- (0, -0.8)
      -- plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))})
      -- (2*pi-\offset, 0)
      -- cycle;
      \fill [lightgray, samples=500, domain=2*pi-\offset:2*pi, variable=\x]
      (2*pi-\offset, 0)
      -- (2*pi-\offset, -0.8)
      -- plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))})
      -- (2*pi, 0)
      -- cycle;
      \fill [pattern=north west lines, samples=500, domain=0:2*pi, variable=\x]
      (0, 0)
      -- (0, -0.8)
      -- plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))})
      -- (2*pi, 0)
      -- cycle;

      \fill [lightgray, samples=500, domain=4*pi-\offset:4*pi, variable=\x]
      (4*pi-\offset, 0)
      -- (4*pi-\offset, -0.8)
      -- plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))})
      -- (4*pi, 0)
      -- cycle;
      \fill [mycolor, samples=500, domain=4*pi:6*pi-\offset, variable=\x]
      (4*pi, 0)
      -- (4*pi, -0.8)
      -- plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))})
      -- (6*pi-\offset, 0)
      -- cycle;

      \draw [thick, purple, samples=500, domain=-0.5:6*pi+0.5] plot ({\x}, {2*sin(\x*180/pi - 90) - 1.2*cos(2*(\x*180/pi - 90))});

      \draw [->] (-0.5,0) -- (6*pi+0.5,0);
      \draw [->] (0,-2) -- (0,4);

      \draw (0,-0.1) -- (0,0.1) node[above right] {\footnotesize $0$};
      \draw (2*pi,-0.1) -- (2*pi,0.1) node[above] {\footnotesize $T$};
      \draw (4*pi-\offset,-0.1) -- (4*pi-\offset,0.1) node[above] {\footnotesize $a$};
      \draw (4*pi,-0.1) -- (4*pi,0.1) node[above] {\footnotesize $k_0T$};
      \draw (6*pi-\offset,-0.1) -- (6*pi-\offset,0.1) node[above] {\footnotesize $a+T$};
    \end{tikzpicture}
    \caption{Illustration de la relation \eqref{S6:Q10:4}. \\
      En bleu \eqref{S6:Q10:2} et en gris \eqref{S6:Q10:3} }
  \end{figure}
\end{question_kholle}
\pagebreak\section{Semaine 7}

\begin{question_kholle}{Calcul de $\int_0^{2\pi}e^{imt} \mathrm d t$ en fonction de $m \in \Z$. En Déduire qu'une fonction polynomiale nulle sur un cercle centré en l'origine a tous ses coefficients nuls.}
  Soit $m \in \Z$ fq. Calculons:
  $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t$$
  Si $m \neq 0$:
  \begin{align*}
    \frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t & = \frac{1}{2 \pi} \Big[ \frac{e^{mt}}{im} \Big]_0^{2\pi}      \\
                                                     & = \frac{1}{2 \pi} \Big( \frac{1}{im} - \frac{1}{im} \Big) = 0
  \end{align*}
  Si $m = 0$:
  $$
    \frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t = \frac{1}{2 \pi} \int_0^{2\pi} \mathrm d t = \frac{2 \pi}{2 \pi} = 1
  $$
  \\
  Donc $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t =
    \begin{cases}
      1 \text{ si } m=0 \\
      0 \text{ si } m \neq 0
    \end{cases}
  $$
  \\
  Soit $n\in \N$ fq

  Soient $(a_0, ..., a_n) \in \C^{n+1}$ les coefficients de $P(z) = \sum_{k=0}^n a_k z^k$, et $s\in \Z$, et $r \in \R_+^*$ fq. tels que P soit nulle lorsqu'elle est évaluée sur $\mathscr C(0,r)$
  \begin{align*}
    \frac{1}{2 \pi} \int_0^{2\pi} P(re^{it}) e^{-imt} \mathrm d t & = \frac{1}{2 \pi} \int_0^{2\pi} \bigg (\sum_{k=0}^n a_k (re^{it})^k \bigg) e^{-imt} \mathrm d t \\
                                                                  & = \sum_{k=0}^n a_k r^k \underbrace{\int_0^{2\pi} \frac{e^{it(k-s)}}{2 \pi}  \mathrm d t}_{I_k}
  \end{align*}
  On remarque que:
  \begin{itemize}
    \item Si $s \notin [ \! [0, n] \! ], \{k \in [ \! [0, n] \! ] \text{ }| \text{ } k = s\}$ = $\emptyset$, Donc $$\sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k  = \sum_{\substack{k \in [ \! [0, n] \! ] \\ k = s}} a_k r^k  = 0$$
    \item Si $s \in [ \! [0, n] \! ], \{k \in [ \! [0, n] \! ] \text{ }| \text{ } k = s\}$ = ${s}$, Donc $$\sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k = \sum_{\substack{k \in [ \! [0, n] \! ] \\ k = s}} a_k s^k = a_s r^s \label{1}$$
  \end{itemize}
  Or, puisque $P$ s'annule sur le cercle de rayon $r$ et de centre $0$,  $\mathscr C(0,r)$, ces sommes sont aussi nulles. On en déduit, en particularisant pour un $s \in [ \! [0, n] \! ]$ fixé quelconque que:
  $$
    \sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k = a_sr^s = 0 \implies a_s = 0
  $$

  Donc $$
    (\exists r \in \R_+^* : \forall \theta \in \R, P(re^{i\theta})=0) \implies \forall s \in [ \! [0, n] \! ]
  $$
  \\
  Pour la preuve réciproque,  soit $n \in \N$ fq. Soient $(a_0,...,a_n) \in \{ 0 \} ^{n+1}$ les coefficients nuls de la fonction polynomiale $P \in \C[z]$ définie pour tout $z \in \C$.

  En remarquant que $\forall z \in \C , P(z) = 0$, puisque n'importe quel cercle centré en 0 est un sous ensemble de $\C$,  $\exists r \in \R_+^*: \forall z \in \mathscr C(0, r), P(z) = 0$.
\end{question_kholle}

\begin{question_kholle}{Preuve de la Linéarité de la dérivation d'une fonction complexe}
  Définissons les fonctions $f_r$ et $f_i$ comme les parties réelles et imaginaires de $f$.

  Soient $(f, g) \in \mathcal{F}(I, \C)^2$, $(\alpha, \beta) \in \C^2$ fixés quelconques.
  \begin{align*}
    f_r = \Re(f)           & , f_i = \Im(f)      & g_r = \Re(f)     & , g_i = \Im(g)     \\
    \alpha_r = \Re(\alpha) & , \alpha_i = \Im(f) & \beta_r = \Re(f) & , \beta_i = \Im(g)
  \end{align*}

  \begin{align*}
    \Re( \alpha f + \beta g) & = \Re((\alpha_r + i \alpha_i)(f_r + i f_i) + (\beta_r+ i\beta_i)(g_r+ i g_i))                                                                                                                     \\
                             & = \underbrace{\alpha_r f_r + \beta_r g_r - \alpha_i f_i - \beta_i g_i}_{\text{Combinaison linéaire de } \underbrace{(f_r, f_i, g_r, g_i) \in \mathcal D^1(I, \R)^4}_{car (f,g) \in D^1(I, \R)^2}}
  \end{align*}
  Donc, selon le théorème de stabilité par combinaison linéaire des fonctions à valeurs réelles, $\Re(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\Re(\alpha f + \beta g)\big)' = \alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i'$
  \\
  On montre de même que $\Im(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\alpha f + \beta g\big)' = \alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r'$

  Ainsi,
  \begin{align*}
    \big( \alpha f + \beta g \big)' & = (\alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i') + i (\alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r')                                         \\
                                    & = \alpha_r(f_r' + if_i') + \beta_r(g_r' + ig_i') + \alpha_i \underbrace{(-f_i' + if_r')}_{i(f_r' + if_i')} + \beta_i \underbrace{( -g_i' + ig_r')}_{i(g_r' + ig_i')} \\
                                    & =\alpha f' + \beta g'
  \end{align*}
\end{question_kholle}

\begin{question_kholle}{Dérivée composée d'une fonction à valeurs complexes}
  Soient $f \in \mathcal D ^1(J, \C) $ et $h \in \mathcal D^1(I, J)$ (I et J sont deux intervalles réels) fixés quelconques. Notons $f_r$ et $f_i$ respectivement la partie réelle et imaginaire de $f$.

  \begin{align*}
    \left .
    \begin{array}{ll}
      h \in \mathcal D^1(I, J) \\
      f_r \in \mathcal D^1(J, \R) \text{, car } f \in \mathcal D^1(J, \C)
    \end{array}
    \right \}
    \implies f_r \circ h \in \mathcal D^1(I, \R)
  \end{align*}

  On montre de même que $f_i \circ h \in \mathcal D^1(I, \mathbb  R)$ donc $f \circ h \in \mathcal D^1(I, \C)$.

  De plus,

  \begin{align*}
    (f \circ h)' & = (f_r \circ h)' + i (f_i \circ h)'                       \\
                 & = (f_r' \circ h ) \times h' + i((f_i' \circ h) \times h') \\
                 & =(f_r' \circ h + if_i' \circ h) \times h'                 \\
                 & = (f' \circ h) \times h'
  \end{align*}
\end{question_kholle}

\begin{question_kholle}{Caractérisation des fonctions dérivables de dérivée nulle sur un intervalle}
  Soit $f \in \mathcal D ^1 (I, \C)$ où $I$ est un intervalle réel;
  Posons $f_r = \Re (f)$ et $f_i = \Im(f)$.

  \begin{align*}
    \forall t \in I, f'(t) = 0 & \iff \forall t \in I, f_r'(t) + i f_i'(t) = 0                        \\
                               & \iff \begin{cases}
                                        \forall t \in I, f_r'(t) = 0 \\
                                        \forall t \in I, f_i'(t) = 0
                                      \end{cases}                                    \\
                               & \iff \begin{cases}
                                        \exists \lambda_r \in \R : \forall t \in I,  f_r(t) = \lambda_r \\
                                        \exists \lambda_i \in \R : \forall t \in I,  f_i(t) = \lambda_i
                                      \end{cases} \\
                               & \iff \exists \lambda \in \C : \forall t \in I,  f(t) = \lambda
  \end{align*}
\end{question_kholle}

\pagebreak\section{Semaine 8}

\begin{question_kholle}{Preuve de l’expression des solutions réelles des EDL homogènes d’ordre 2 à coefficients constants réels dans le cas $\Delta < 0$ (en admettant la connaissance de l’expression des solutions à valeurs complexes des EDLH2 à coeff. constants).}
  Notons $\Sol_{H, \C}$ et $\Sol_{H, \R}$ les ensembles des solutions complexes et réelles de l'équation différentielle, puisque nous nous plaçons dans le cas $\Delta < 0$ et $\alpha \pm i \beta$ les deux racines complexes conjuguées.
  $$
    \Sol_{H, \C} =
    \left\{
    \begin{array}{l}
      \R \to \C \\
      t \mapsto \lambda e^{(\alpha + i \beta) t}  + \mu e^{(\alpha - i \beta)t}
    \end{array}
    \middle\vert  (\lambda, \mu) \in \C ^2 \right\}
  $$

  Montrons que $\forall f \in \Sol_{H ,\C}, \Re(f) \in  \Sol_{H ,\R}$\\
  Soit $f \in \Sol_{H ,\C}$ fq.
  $$f \in \mathcal D^2(\R, \C) \implies \Re(f) \in \mathcal D^2(\R, \R)$$
  Et, de plus, par morphisme additif de \Re
  $$
    a_2\Re(f)'' + a_1\Re(f)' + a_0\Re(f) = \Re( a_2 f'' + a_1 f' + a_0 f) = 0
  $$
  D'où, avec $f:t \mapsto e^{(\alpha + i \beta)t}$; $\Re(f(t)) = \Re(e^{(\alpha + i \beta)t}) = e^{\alpha t } \cos (\beta t)$. Qui appartient donc à $\Sol_{H, \R}$\\
  En suivant le même raisonnement pour $\Im(f)$, $(t \mapsto e^\alpha \sin(\beta t)) \in \Sol_{H, \R}$


  Ainsi, par combinaison linéaire (qui se base sur le principe de superposition),
  $$
    \left\{
    \begin{array}{l}
      \R \to \R \\
      t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
    \end{array}
    \middle\vert  (\lambda, \mu) \in \R ^2 \right\}
    \subset \Sol_{H ,\R}
  $$

  Réciproquement, soit $ f \in \Sol_{H ,\R}$ fq. Puisque $\R \subset \C$,  $ f \in \Sol_{H ,\C}$.

  $$
    \exists (a, b) \in \C^2 : f \left| \begin{array}{l}
      \R \to \C \\
      t \mapsto a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t}
    \end{array}\right.$$

  Or, puisque toutes les valeurs de $f$ sont réelles, en notant $(a_r, a_i, b_r, b_i)$ les parties réelles et imaginaires respectives de $a$ et $b$.
  \begin{align*}
    \forall t \in \R, f(t) & = \Re(f(t))                                                                                                     \\
                           & = \Re(a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t})                                                  \\
                           & = \Re((a_r + i a_i) e^{(\alpha + i \beta) t}  + (b_r + i b_i) e^{(\alpha - i \beta)t})                          \\
                           & = a_r \cos(\beta t)e^\alpha - a_i\sin(\beta t)e^\alpha + b_r \cos(\beta t)e^\alpha + b_i \sin(\beta t) e^\alpha \\
                           & = (a_r + b_r) \cos(\beta t) e^\alpha + (b_i - a_i) \sin(\beta t) e^\alpha
  \end{align*}
  Ainsi,
  $$f\in \left\{
    \begin{array}{l}
      \R \to \R \\
      t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
    \end{array}
    \middle\vert  (\lambda, \mu) \in \R ^2 \right\}
  $$
  Ce qui conclut la preuve par double inclusion.
\end{question_kholle}

\begin{question_kholle}[
    Considérons le problème de Cauchy suivant :
    $$\left\{ \begin{array}{l}
        a_{2}y''+a_{1}y'+a_{0}y = b \text{ sur } J \\
        y(t_{0}) = \alpha_{0}                      \\
        y'(t_{0}) = \alpha_{1}
      \end{array} \right. \text{ où } (\alpha_{0}, \alpha_{1}) \in \mathbb{K}^{2}, t_{0} \in J, (a_{0}, a_{1}, a_{2}) \in \mathbb{K}^{2} \times \mathbb{K}^{*}, b \in \mathcal{F}(J, \mathbb{K})$$
    Si $b$ est continu sur $J$, alors ce problème de Cauchy admet une unique solution définie sur $J$.
  ]
  {Existence et unicité d'une solution au problème de Cauchy pour les EDL d'ordre 2 à coefficients constants et second membre continu sur $I$ (cas complexe puis cas réel).}

  \textbf{Cas 1. } $\mathbb{K} = \mathbb{C}$ \\
  Nous savons que sous l'hyphothèse de continuité de $b$ sur $J$, les solutions de (EDL2) définies sur $J$ constituent le plan affine $S$ :
  $$S = \left\{ \lambda f_{1} + \mu f_{2} + s | (\lambda, \mu) \in \mathbb{C}^{2} \right\}$$
  où $s$ est une solution particulière de (EDL2), $(f_{1}, f_{2})$ sont deux solutions de (EDLH2) qui engendrent $S_{h}$. On a : \\

  $$\begin{array}{ccl}
      f : J \to \mathbb{C} \text{ est sol. du pb de Cauchy }
       & \iff & \left\{ \begin{array}{l}
                          f \text{ sol de (EDL2) sur } J \\
                          f(t_{0}) = \alpha_{0}          \\
                          f'(t_{0}) = \alpha_{1}
                        \end{array}  \right.                                                                                \\\\
       & \iff & \left\{ \begin{array}{l}
                          f \in S               \\
                          f(t_{0}) = \alpha_{0} \\
                          f'(t_{0}) = \alpha_{1}
                        \end{array}\right.                                                                                         \\\\
       & \iff & \exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
                                                                     f = \lambda f_{1} + \mu f_{2} + s                                  \\
                                                                     \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) + s(t_{0}) = \alpha_{0}    \\
                                                                     \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) + s'(t_{0}) = \alpha_{1} \\
                                                                   \end{array} \right. \\\\
       & \iff & \exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
                                                                     f = \lambda f_{1} + \mu f_{2} + s                                  \\
                                                                     \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) = \alpha_{0} - s(t_{0})    \\
                                                                     \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) = \alpha_{1} - s'(t_{0}) \\
                                                                   \end{array} \right. \\\\
    \end{array} $$
  On en déduit donc que $(\lambda, \mu)$ doit être solution d'un système linéaire $(2,2)$. On a une unique solution si et seulement si les déterminant de ce système est nul. \\
  Explicitons alors le déterminant de ce système, que l'on notera $D$.
  $$D = \left|
    \begin{array}{cc}
      f_{1}(t_{0})  & f_{2}(t_{0})  \\
      f'_{1}(t_{0}) & f'_{2}(t_{0}) \\
    \end{array}
    \right| = f_{1}(t_{0}) \cdot f'_{2}(t_{0}) - f_{2}(t_{0}) \cdot f'_{1}(t_{0}) $$
  Notons $\Delta$ le discriminant de l'équation caractéristique de (EDL2) ($a_{2}r^{2} + a_{1}r^{1} + a_{0} = 0$). On distingue alors deux cas selon la nullité ou non de $\Delta$. Traitons d'abord le cas $\Delta \neq 0$. On peut choisir :
  $$ f_{1}(t_{0}) = e^{r_{1}t_{0}} \text{ et } f_{2}(t_{0}) = e^{r_{2}t_{0}}$$
  $$ f'_{1}(t_{0}) = r_{1}e^{r_{1}t_{0}} \text{ et } f'_{2}(t_{0}) = r_{2}e^{r_{2}t_{0}}$$
  Donc (en sachant que $\Delta \neq 0 \Rightarrow r_{1} \neq r_{2}$):
  $$ D = e^{r_{1}t_{0}} \cdot r_{2}e^{r_{2}t_{0}} - r_{1}e^{r_{1}t_{0}} \cdot e^{r_{2}t_{0}} = (r_{2} - r_{1}) \cdot e^{r_{1}t_{0} + r_{2}t_{0}} \neq 0$$

  Dans le deuxième cas, on a $\Delta = 0$ ; on peut alors prendre :
  $$ f_{1}(t_{0}) = e^{r_{0}t_{0}} \text{ et } f_{2}(t_{0}) = t_{0}e^{r_{0}t_{0}}$$
  Ainsi :
  $$ D = e^{r_{0}t_{0}} \left(r_{0}t_{0}e^{r_{0}t_{0}} + e^{r_{0}t_{0}} \right) - r_{0}e^{r_{0}t_{0}} \times t_{0}e^{r_{0}t_{0}} = e^{2r_{0}t_{0}} \neq 0$$
  On remarque alors que, dans les deux cas, $D \neq 0$, donc le système $(2, 2)$ étudié admet une unique solution, donc il existe un unique couple $(\lambda, \mu)$ le vérifiant d'où l'unicité et existence d'une solution au problème de Cauchy.
  \newline\newline

  \textbf{Cas 2. } $\mathbb{K} = \mathbb{R}$ \\
  $(a_{0}, a_{1}, a_{2}) \in \mathbb{R}^{2} \times \mathbb{R}^{*},(\alpha_{0}, \alpha_{1}) \in \mathbb{R}^{2}, b \in C^{0}(J, \mathbb{R})$
  \newline
  \textbf{Existence :} Puisque $\mathbb{R} \subset \mathbb{C}$, le problème de Cauchy admet, dans $\mathbb{R}$, une solution à valeurs complexes $g$. Posons $f = \Re(g)$ et montrons que $f$ est une solution réelle du problème de Cauchy. \\
  \begin{itemize}
    \item[$\star$] $g \in \mathcal{D}^{2}(J, \mathbb{C}) \text{ donc } f \in \mathcal{D}^{2}(J, \mathbb{R})$
    \item[$\star$] $g$ vérifie $a_{2}g'' + a_{1}g' + a_{0}g = b$ sur $J$ donc en prenant $\Re(\cdot)$ :
          $$\begin{array}{ccl}
              \Re(a_{2}g'' + a_{1}g' + a_{0}g = b) = \Re(b)
               & \iff & a_{2}\Re(g'') + a_{1}\Re(g') + a_{0}\Re(g) = b \\\\
               & \iff & a_{2}f'' + a_{1}f' + a_{0}f = b \text{ sur } J
            \end{array}$$
    \item[$\star$] $f(t_{0}) = \Re(g(t_{0})) = \Re(\alpha_{0}) = \alpha_{0}$
    \item[$\star$] $f'(t_{0}) = \Re(g(t_{0}))' = \Re(g'(t_{0})) = \Re(\alpha_{1}) = \alpha_{1}$
  \end{itemize}
  Donc $f$ est une solution réelle définie sur $J$ au problème de Cauchy.
  \newline

  \textbf{Unicité : }Soient $f_{1}$ et $f_{2}$ deux fonctions à valeurs réelles solutions du problème de Cauchy ci-dessus fixées quelconques : puisque $\mathbb{R} \subset \mathbb{C}$, $f_{1}$ et $f_{2}$ sont des fonctions à valeurs dans $\mathbb{C}$ solutions du même problème de Cauchy; or il y a unicité de la solution au problème de Cauchy dans les fonctions à valeurs complexes, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{C})$, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{R})$.
\end{question_kholle}

\begin{question_kholle}[
    Soient $(a,b)\in \mathbb{C}^2$, $f$ et $g$ les  solutions, définies sur $\mathbb{R}$ à valeurs
    dans $\mathbb{C}$, des problèmes de Cauchy suivants :
    \[
      \left\{ \begin{array}{cl}
        y'' +ay'+by = 0 \\
        y(3) = 1        \\
        y'(3) = 0
      \end{array} \right.
      \quad \text{et} \quad
      \left\{ \begin{array}{cl}
        y'' +ay'+by = 0 \\
        y(3) = 0        \\
        y'(3) = 1
      \end{array} \right.
    \]

    Comment s'exprime la solution définie sur $\mathbb{R}$ de $\left\{ \begin{array}{cl}
        y'' +ay'+by = 0 \\
        y(3) = \alpha   \\
        y'(3) = \beta
      \end{array} \right. $ pour $(\alpha, \beta)\in \mathbb{R}^2$ fixés ?

    Peut-on affirmer que le plan vectoriel des solutions définies sur $\mathbb{R}$ à valeurs dans
    $\mathbb{C}$ de $y'' + ay' + by = 0$ est $\{ \lambda \cdot f + \mu \cdot g  |
      (\lambda, \mu)\in \mathbb{C}^2\}$
  ]
  {Les solutions d'une EDL$_2$ constituent un espace vectoriel.}

  La solution s'exprime simplement comme combinaison linéaire de f et g, plus précisément, la
  combinaison linéaire en $\alpha$ et $\beta$. En effet, soient de tels scalaires, et soient $f$ et
  $g$ de telles solutions, on a :
  \[
    (\alpha \cdot f + \beta \cdot g)'' + a (\alpha \cdot f + \beta \cdot g)' + b (\alpha \cdot f +
    \beta \cdot g) = 0 \text{, par définition des espaces vectoriels.}
  \]
  Et de même, $(\alpha \cdot f + \beta \cdot g)'(3) = \alpha \cdot f'(3) + \beta \cdot g'(3) = \alpha$,
  et $(\alpha \cdot f + \beta \cdot g)''(3) = \alpha \cdot f''(3) + \beta \cdot g''(3) = \beta$.
  \newline
  Ce qui suffit par unicité des solutions ( de la donc) d'un problème de Cauchy dans le cadre du
  théorème du cours.
  \newline
  Pour ce qui est du plan vectoriel des solutions, noté $\Omega$, notons aussi $\Phi$ l'ensemble proposé.
  L'inclusion $\Phi \subset \Omega$ est triviale par propriété de linéarité des espaces vectoriels.
  Finalement, pour $\Omega \subset \Phi$, soit $\omega \in \Omega$, forcément, $\omega$ vérifie
  l'$EDL_2$, mais aussi des conditions de Cauchy bien que celles-ci soient non-spécifiées, ainsi
  posons $\omega'(3) = \delta$ et $\omega''(3) = \theta$, donc en particulier, $ \omega =
    \delta \cdot f + \theta \cdot g$, d'où l'égalité par double inclusion.
\end{question_kholle}

\begin{question_kholle}
  [
    Résolution générale des systèmes linéaires à 2 équations et 2 inconnues en fonction du déterminant du systèmes (\textbf{tous les cas ne sont pas nécessairement à envisager})

    Considérons le système linéaire à deux équations et à deux inconnues $(x,y)$ :
    \begin{equation}
      (S)
      \left\{
      \begin{matrix}
        ax + by = b_1 & (E_1) \\
        cx + dy = b_2 & (E_2)
      \end{matrix}
      \right.
    \end{equation}
    dont $(a,b,c,d) \in \K^4$ sont les coefficients et $(b_1,b_2) \in \K^2$ sont les seconds membres.

    \begin{enumerate}
      \item (S) admet une unique solution si et seulement si
            $\begin{vmatrix}
                a & b \\
                c & d
              \end{vmatrix}
              = ad - bc \neq 0$. De plus, dans ce cas, la solution est
            \begin{equation}
              \left(
              \frac
              {\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}
              {\begin{vmatrix}a&b\\c&d\end{vmatrix}},
              \frac
              {\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}
              {\begin{vmatrix}a&b\\c&d\end{vmatrix}}
              \right)
            \end{equation}
      \item Si $ad - bc = 0$, alors l'ensemble des solutions est soit vide, soit une droite affine de $\K^2$, soit $\K^2$.
    \end{enumerate}
  ]
  {Formules de Cramer pour les systèmes 2 $\times$ 2}
  Procédons par disjonction de cas.

  \begin{itemize}[label=$\bullet$ Supposons]
    \item que $ad - bc \neq 0$.
          \begin{itemize}[label=$\bullet$ Supposons]
            \item que $a \neq 0$.
                  \begin{equation*}
                    \begin{aligned}
                      (S)
                       & \iff \left\{
                      \begin{array}{cccccc}
                        ax & + & by                             & = & b_1                                                            \\
                           &   & \left(d - \frac{bc}{a}\right)y & = & b_2 - \frac{c}{a} b_1 & (L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
                      \end{array}
                      \right.         \\
                       & \iff \left\{
                      \begin{array}{cccccc}
                        ax & + & by                    & = & b_1                                   \\
                           &   & \left(ad - bc\right)y & = & a b_2 - c b_1 & (L_1 \leftarrow aL_1) \\
                      \end{array}
                      \right.         \\
                       & \iff \left\{
                      \begin{array}{ccc}
                        ax & = & \frac{1}{a} \left(b_1 - b\frac{ab_2 - cb_1}{ad - bc}\right) = \frac{1}{a} \frac{adb_1 - bcb_1 + abb_2 - bcb_2}{ad - bc} \\
                        y  & = & \frac{ab_2 - cb_1}{ad - bc}                                                                                             \\
                      \end{array}
                      \right.         \\
                       & \iff \left\{
                      \begin{array}{ccccc}
                        ax & = & \frac{db_1 - bb_2}{ad - bc} & = & \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}} \\
                        y  & = & \frac{ab_2 - cb_1}{ad - bc} & = & \frac{\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}} \\
                      \end{array}
                      \right.
                    \end{aligned}
                  \end{equation*}
                  Donc le système admet une unique solution qui est celle annoncée.

            \item que a = 0. L'hypothèse $ad - bc \neq 0$ implique $bc \neq 0$ donc $b \neq 0$ et $c \neq 0$.
                  \begin{equation*}
                    \begin{aligned}
                      (S)
                       & \iff \left\{
                      \begin{array}{ccccc}
                           &   & by & = & b_1 \\
                        cx & + & dy & = & b_2 \\
                      \end{array}
                      \right.         \\
                       & \iff \left\{
                      \begin{array}{ccc}
                        x & = & \frac{1}{c} \left( b_2 - d\frac{b_1}{b} \right) \\
                        y & = & \frac{b_1}{b}                                   \\
                      \end{array}
                      \right.         \\
                       & \iff \left\{
                      \begin{array}{ccccc}
                        ax & = & \frac{db_1 - bb_2}{- bc} & = & \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}} \\
                        y  & = & \frac{- cb_1}{- bc}      & = & \frac{\begin{vmatrix}0&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}} \\
                      \end{array}
                      \right.
                    \end{aligned}
                  \end{equation*}
          \end{itemize}
          Donc le système admet une unique solution qui est celle annoncée.
  \end{itemize}

  \item $ad - bc = 0$.
  \begin{itemize}[label=$\bullet$ Supposons]
    \item $a \neq 0$. En reprenant la méthode pivot de Gauss,
          \begin{equation*}
            \begin{aligned}
              (S)
               & \iff \left\{
              \begin{array}{cccccc}
                ax & + & by                             & = & b_1                                                            \\
                   &   & \left(d - \frac{bc}{a}\right)y & = & b_2 - \frac{c}{a} b_1 & (L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
              \end{array}
              \right.         \\
               & \iff \left\{
              \begin{array}{cccccc}
                ax & + & by                                    & = & b_1                                   \\
                   &   & \underbrace{\left(ad - bc\right)}_0 y & = & a b_2 - c b_1 & (L_1 \leftarrow aL_1) \\
              \end{array}
              \right.         \\
            \end{aligned}
          \end{equation*}
          Donc le système est de rang 1 avec une condition de compatibilité. \\
          Si $ab_2 - cb_1 \neq 0$, (S) n'admet aucune solution. \\
          Sinon $ab_2 - cb_1 = 0$
          \begin{equation}
            (S) \iff
            ax + by = b_1 \iff
            \begin{pmatrix} x \\ y \end{pmatrix} \in \left\{
            \begin{pmatrix} \frac{b_1}{a} - b\frac{t}{a} \\ t \end{pmatrix}
            |\; t \in \K
            \right\}
          \end{equation}
          Donc (S) admet un droite affine de solutions.

    \item $a = 0$. Puisque $ad - bc = 0$, alors $bc = 0$ donc b ou c est nul.

          \begin{itemize}[label=$\bullet$ Si]
            \item $c = 0$,
                  \begin{equation*}
                    (S) \iff
                    \left\{ \begin{array}{ccc}
                      by & = & b_1 \\
                      dy & = & b_2
                    \end{array} \right.
                  \end{equation*}

                  \begin{itemize}[label=$\bullet$ Si]
                    \item $b = 0$,
                          \begin{equation*}
                            (S) \iff
                            \left\{ \begin{array}{ccc}
                              by & = & b_1 \\
                              0  & = & b_2
                            \end{array} \right.
                          \end{equation*}
                          \begin{itemize}[label=$\bullet$ Si]
                            \item $b_2 = 0$, (S) n'admet aucune solution.
                            \item $b_2 \neq 0$, $(S) \iff dy = b_2$
                                  \subitem$\bullet$ Si $d = 0$, $(S) \iff 0 = b_2$. (S) n'admet aucune solution ($b_2 \neq 0$) ou admet $\K^2$ comme ensemble des solutions ($b_2 = 0$).
                                  \subitem$\bullet$ Si $d \neq 0$, $(S) \iff y = \frac{b_2}{d} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_2}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$. Donc (S) admet une droite affine de solutions.
                          \end{itemize}
                    \item $b \neq 0$
                          \begin{equation*}
                            (S) \iff
                            \left\{ \begin{array}{ccc}
                              y & = & \frac{b_1}{b}        \\
                              0 & = & b_2 - \frac{db_1}{b}
                            \end{array} \right.
                          \end{equation*}
                          \begin{itemize}[label=$\bullet$ Si]
                            \item $b_2 - \frac{db_1}{b} \neq 0$, (S) n'admet aucune solution.
                            \item $b_2 - \frac{db_1}{b} = 0$, $(S) \iff y = \frac{b_1}{b} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_1}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
                          \end{itemize}
                  \end{itemize}
            \item $c \neq 0$ alors $b = 0$
                  \begin{equation*}
                    \begin{aligned}
                      (S)
                       & \iff \left\{ \begin{array}{ccc}
                                        0       & = & b_1 \\
                                        cx + dy & = & b_2
                                      \end{array} \right.
                    \end{aligned}
                  \end{equation*}
                  \begin{itemize}[label=$\bullet$ Si]
                    \item $b_1 \neq 0$, (S) n'admet aucune solution.
                    \item $b_1 = 0$, $(S) \iff x = \frac{b_2}{c} - \frac{d}{c}y \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} \frac{b_2}{c} - \frac{d}{c}t \\ t \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
                  \end{itemize}
          \end{itemize}
  \end{itemize}

\end{question_kholle}

\begin{question_kholle}[]{Déterminer le solutions réelles définies sur $]0, + \infty [$ de $y'' + 3y' +2y = \frac{t-1}{t^{2}}e^{ -t }$}
  

Il s'agit d'une EDL2 avec second membre à coefficients constants avec second membre défini sur $]0, +\infty[$.

Ce second membre est de plus continu sur $\mathbb{R}_{+}^{*}$ donc les solutions définies sur un intervalle maximal sont définies sur $\mathbb{R}^{*}_{+}$ et leur ensemble $\mathcal{S}$ est un plan affine, le plan affine passant par une solution particulière et dirigé par le plan vectoriel $\mathcal{S}_{H}$.
\begin{itemize}[label=$\star$]
  \item L'équation caractéristique de $y'' + 3y' + 2y = 0$ est $r^{2} + 3r + 2 = 0 \iff (r+1)(r+2) = 0$

Donc
$$
\mathcal{S}_{H} = \left\{ \left.\begin{array}{ll} \mathbb{R}^{*}_{+} &\to \mathbb{R} \\ t &\mapsto \lambda e^{-t}+\mu e^{-2t} \end{array}\right| (\lambda, \mu) \in \mathbb{R}^{2}\right\} 
$$

\item Cherchons une solution particulière de la forme $t \mapsto \lambda(t)e^{ -t }$ avec $\lambda \in \mathcal{D}^{2}(\mathbb{R}^{*}_{+}, \mathbb{R})$


\begin{align*}
\begin{matrix}
t \mapsto \lambda(t)e^{ -t } \text{ est solution}\\ \text{ particulière dans }\mathbb{R}_{+}^{*}
\end{matrix}  & \iff \forall t \in \mathbb{R}^{*}_{+},  (\lambda (t)e^{ -t })'' + 3 (\lambda(t)e^{ -t })' + 2(\lambda(t)e^{ -t }) = \frac{t-1}{t^{2}}e^{ -t } \\
&\iff \forall t \in \mathbb{R}_{+}^{*}, (\lambda''(t)- 2\lambda'(t)+\lambda(t))e^{ -t } + 3 (\lambda(t) ' - \lambda(t))e^{ -t  } + 2 \lambda(t) e^{ -t } = \frac{t-1}{t^{2}}e^{ -t } \\
 & \iff\forall t \in \mathbb{R}_{+}^{*}, \lambda'' (t) + \lambda'(t) = \frac{t-1}{t^{2}}
\end{align*}

\begin{itemize}[label=$\lozenge$]
  \item Cherchons donc une solution particulière de
$$
y' + y = \frac{t-1}{t^{2}}
$$

C'est une EDL1 définie et résolue sur $\mathbb{R}_{+}^{*}$, à coefficients et second membre résolus, donc l'ensemble des solutions est une droite affine de solutions définies sur $\mathbb{R}_{+}^{*}$

\begin{itemize}[label=$\triangle$] 
  \item Par la méthode de la variation de la constante, cherchons une solution de la forme $t \mapsto \alpha (t) e^{ -t }$ avec $\alpha \in \mathcal{D}^{1}(\mathbb{R}_{+}^{*}, \mathbb{R})$.


\begin{align*}
t \mapsto \alpha(t)e^{ -t } \text{ est solution particulière } &\iff \forall t \in \mathbb{R}_{+}^{*}, (\alpha(t)e^{ -t })' + \alpha(t)e^{ -t }= \frac{t-1}{t^{2}}\\
&\iff \forall t \in \mathbb{R}_{+}^{*}, \alpha'(t) = \frac{t-1}{t^{2}}e^{ t }
\end{align*}


Cherchons une primitive de $t \mapsto \frac{t-1}{t^{2}}e^{ t }$


\begin{align*}
\int_{1}^{t} \frac{u-1}{u^{2}}e^{ u } \, \mathrm du &= \int_{1}^{t} \frac{e^{ u }}{u} - \frac{e^{ u }}{u^{2}} \, \mathrm du \\
&= \int_{1}^{t} \frac{e^{ u }}{u} \, \mathrm du - \int_{1}^{t} \frac{e^{ u }}{u^{2}} \, \mathrm du \\
&= \int_{1}^{t} \frac{e^{ u }}{u} \, \mathrm du  - \left( \left[ -\frac{1}{u}e^{ u } \right] _{1}^{t} - \int_{1}^{t} -\left( \frac{1}{u} \right) e^{ u } \, \mathrm du  \right)   \\
&= \left[ \frac{e^{ u }}{u} \right] _{1}^{t} = \frac{e^{t}}{t} - e
\end{align*}


Donc $\alpha(t) = \frac{e^{ t }}{t}$ convient.
\end{itemize}

Donc $t \mapsto \alpha(t)e^{ -t } = \frac{e^{ t }}{t}e^{ -t } = \frac{1}{t}$ est une solution particulière de $y'+y = \frac{t-1}{t^{2}}$
\end{itemize}
Donc $\lambda'(t) = \frac{1}{t}$ donc $\lambda(t) = \ln(t)$ convient

Donc $t \mapsto \ln (t) e^{ -t }$ est une solution particulière de (EDL2)
\end{itemize}
Ainsi, le plan affine des solutions sur $\mathbb{R}_{+}^{*}$ est
$$
\mathcal{S} = \left\{ \left.\begin{array}{ll} \mathbb{R}_{+}^{*} &\to \mathbb{R} \\ t &\mapsto (\ln t)e^{ -t }+ \lambda e^{ -t }+\mu e^{ -2t } \end{array}\right| (\lambda , \mu) \in \mathbb{R}^{2}\right\} 
$$

\end{question_kholle}

\pagebreak\section{Semaine 9}

\begin{question_kholle}
  [\noindent Soit \Rel une relation d'équivalence sur $E$. \\
  Soit $x \in E$. \\
  La classe de $x$, notée $\bar{x}$, est l'ensemble des éléments de $E$ en relation avec x.
  $$
  \bar{x} = \left\{ y \in E \;|\; x \Rel y \right\}
  $$
  ]
  {Deux classes d'équivalence sont disjointes ou confondues. Les classes d'équivalence constituent une partition de l'ensemble sur lequel on considère la relation d'équivalence.}
  
  Montrons que deux classes d'équivalence sont disjointes ou confondues.
  
  Soient $(x, y) \in E^2$ fixés quelconques
  \begin{itemize}[label=\textemdash]
    \item Si $\bar{x} \cap \bar{y} = \emptyset$, rien à démontrer.
    \item Sinon $\bar{x} \cap \bar{y} \neq \emptyset$ donc $\exists z \in \bar{x} \cap \bar{y}$. Fixons un tel $z$.
    
    Soit $x' \in \bar{x}$ fq.
    \begin{equation*}
      \left.
      \begin{matrix}
        \left. \begin{matrix}
          x' \in \bar{x} \implies x \Rel x' \underset{\text{symétrie}}{\implies} x' \Rel x \\
          z \in \bar{x} \implies x \Rel z
        \end{matrix}
        \right\} \underset{\text{transitivité}}{\implies} x' \Rel z \\
        z \in \bar{y} \implies y \Rel z \underset{\text{symétrie}}{\implies} z \Rel y
      \end{matrix}
      \right\} \underset{\text{transitivité}}{\implies} x' \Rel y
      \underset{\text{symétrie}}{\implies} y \Rel x'
    \end{equation*}
    
    Donc $x' \in \bar{y}$ donc $\bar{x} \subset \bar{y}$.
    
    En échangeant les rôles de $x$ et $y$, on montre la deuxième inclusion $\bar{y} \subset \bar{x}$.
  \end{itemize}
  \bigbreak
  
  Montrons que les classes d'équivalence de E constituent une partition de E.
  
  Soit $\Sol$ un système de représentant des classes fixé quelconque.
  
  \begin{itemize}[label=\textemdash]
    \item Soit $s\in \Sol$ fixé quelconque $\bar{s} \neq \emptyset$ car $s \Rel s$ par réflexivité.
    \item Soit $(s, s') \in \Sol^2$ fq. D'après la démonstration ci-dessus ci-dessus, $\bar{s} \cap \bar{s'} = \emptyset$ ou $\bar{s} = \bar{s'}$. Si $\bar{s} = \bar{s'}$ alors $s$ et $s'$ représente la même classe ce qui est impossible car un système de représentants des classes contient un unique représentant de chaque classe. Par conséquent, $\bar{s}$ et $\bar{s'}$ sont disjoints.
    \item $\underset{s \in \Sol}{\bigcup} \bar{s} \subset E$ car $\forall s \in \Sol, \bar{s} \in E$ par définition d'une classe d'équivalence. \\
    Réciproquement, soit $x \in E$ fq. \\
    Par réflexivité de \Rel, $x \in \bar{x}$. \\
    Par définition d'un système de classe $\exists ! s_x \in \Sol : s_x \in \bar{x}$ donc $\bar{s_x} = \bar{x}$. Donc $x \in \bar{s_x} \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. Donc $E \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. \\
    Par double inclusion, $E = \underset{s \in \Sol}{\bigcup} \bar{s}$.
  \end{itemize}
  
  Ainsi,
  $$E = \bigsqcup_{s \in \Sol} \bar{s}$$
  
\end{question_kholle}

\begin{question_kholle}[{Soit $n \in \mathbb{N}^{*}$, on définit l'addition dans $\mathbb{Z}/n\mathbb{Z}$ de la manière suivante
  $$
  +_{\mathbb{Z}/n\mathbb{Z}} \left|
  \begin{array}{ccccc} 
    \mathbb{Z}/n\mathbb{Z}  &\times &\mathbb{Z}/n\mathbb{Z} &\to &\mathbb{Z}/n\mathbb{Z} \\ 
    (\bar{x} &, &\bar{y}) &\mapsto &\overline{x+_{\mathbb{Z}}y} 
  \end{array}\right.
  $$
  }]{Définition de l'addition pour $\Z / n \Z$}
  
  \begin{itemize}[label=$\star$]
    \item Cette définition n'est pas cohérente à priori, car la valeur attribuée à $\bar x$ et $\bar y$ dépend de $x$ et de $y$ alors qu'elle ne doit dépendre que de $\bar x$ et $\bar y$. Il faudra bien vérifier que le résultat est le même, peu importe le représentant choisi.
    
    Soient $(x, x', y, y') \in \mathbb{Z}^{4}$ tels que $\bar{x} = \bar{x}'$ et $\bar{y} = \bar{y}'$.
    
    On a $\exists (p, q) \in \mathbb{Z}^{2} : x = x' + np, y=y'+nq$
    
$$
    \overline{x+_{\mathbb{Z}}y} = \overline{x'+np + y' + nq} = \overline{x'+y' + n(p+q)} = \overline{x'+y'}
$$
    
    On a donc bien égalité du résultat, peu importe le représentant de classe choisi, ce qui définit bien l'addition $+_{\mathbb{Z}/n\mathbb{Z}}$.
    
    \item Montrons que $(\mathbb{Z}/n\mathbb{Z}, +_{\mathbb{Z}/n\mathbb{Z}})$ est un groupe abélien.
    \begin{itemize}[label=$\bullet$]
      \item $\mathbb{Z}/n\mathbb{Z}$ est stable pour la loi $+_{\mathbb{Z}/n\mathbb{Z}}$ (par définition).
      
      \item Cette loi est associative : 
      Soient $(a, b, c) \in \mathbb{Z}/n\mathbb{Z}^{3}$, on peut choisir un représentant de classe pour ces trois classes : $(x,y, z) \in \mathbb{Z}^{3}$ tels que $\bar{x} = a, \bar{y} = b, \bar{z} = c$
$$
      (a +_{\mathbb{Z}/n\mathbb{Z}} b)+_{\mathbb{Z}/n\mathbb{Z}} c =  \overline{x+_{\mathbb{Z}}y} +_{\mathbb{Z}/n\mathbb{Z}} c = \underbrace{ \overline{(x+_{\mathbb{Z}}y) +_{\mathbb{Z}} z}= \overline{x+_{\mathbb{Z}}(y+_{\mathbb{Z}}z)} }_{ \text{associativité de }+_{\mathbb{Z}} } =  a +_{\mathbb{Z}/n\mathbb{Z}}\overline{y +_{\mathbb{Z}} z} = a +_{\mathbb{Z}/n\mathbb{Z}} (b +_{\mathbb{Z}/n\mathbb{Z}} c)
$$
      \item Cette loi est commutative :
      Soient $(a, b) \in \mathbb{Z}/n\mathbb{Z}^{2}$, on choisit, $(x, y) \in \mathbb{Z}^{2}$ des représentants de classe tels que $\bar{x} = a, \bar{y} = b$
$$
      a+_{\mathbb{Z}/n\mathbb{Z}}b = \underbrace{ \overline{x +_{\mathbb{Z}} y} = \overline{y+_{\mathbb{Z}}x} }_{ \text{commutativité de } +_{\mathbb{Z}} } = b +_{\mathbb{Z}/n\mathbb{Z}}a
$$
      \item $\mathbb{Z}/n\mathbb{Z}$ possède un élément neutre pour $+_{\mathbb{Z}/n\mathbb{Z}}$ : 
      Soit $a \in \mathbb{Z}/n\mathbb{Z}$, on choisit $x \in \mathbb{Z}$ un représentant de classe tel que $\bar{x} = a$
$$
      a +_{\mathbb{Z}/n\mathbb{Z}} \bar{0} = \overline{x+_{\mathbb{Z}}0} = \bar{x} = a
$$
      Donc $\bar{0}$ est un élément neutre à droite, et par commutativité de $+_{\mathbb{Z}/n\mathbb{Z}}$ prouvée plus haut, $\bar{0}$ est aussi élément neutre à gauche.
    \end{itemize}
    Ainsi, $(\mathbb{Z}/n\mathbb{Z}, +_{\mathbb{Z}/n\mathbb{Z}})$ est un Groupe Abélien.
  \end{itemize}  
\end{question_kholle}


\begin{question_kholle}[]{Dans un ensemble totalement ordonné, toute partie finie non vide possède un plus grand élément et un plus petit élément.}
  Soit $(E, \preccurlyeq)$ un ensemble totalement ordonné, considérons pour tour $n \in \mathbb{N}^{*}$ la propriété.
$$
  \mathcal{H}_{n} : \text{toute partie de }E \text{ de cardinal }n \text{ admet un plus petit et un plus grand élément}
$$
  \begin{itemize}[label=$\star$]
    \item Initialisation $n \leftarrow 1$
    
    Soit $A \in \mathcal{P}(E)$ fixée telle que $\lvert A \rvert = 1$
$A$ est non vide, donc $\exists a \in A : A = \{ a \}$
    
$a$ est le plus petit et le plus grand élément, donc $\mathcal{H}_{1}$ est vraie.
    
    \item Hérédité
    Soit $n \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{H}_{n}$ est vraie.
    Soit $A \in \mathcal{P}(E)$ fixée quelconque tel que $\lvert A \rvert = n+1$
$$
    A \neq \emptyset \implies \exists a \in A : A = (A \setminus \{ a \}) \cup \{ a \}
$$
    Or, $\lvert A \setminus \{ a \} \rvert = n$ donc $\mathcal{H}_{n}$ s'applique et $A \setminus \{ a \}$ possède un plus grand et plus petit élément
$$
    \left\{ \begin{array}{ll}
      m &= \min A \setminus \{ a \}  \\
      M &= \max A \setminus \{ a \}
    \end{array}\right.
$$
    \begin{itemize}[label=$\lozenge$]
      \item Construisons le plus grand élément de $A$
      \begin{itemize}[label=$\bullet$]
        \item Supposons $M \preccurlyeq a$
        D'une part $a \in A$
        D'autre part
        $$
        \forall x \in A, \left. \begin{array}{ll}
          \text{si }x = a, x \preccurlyeq a \text{ (réflexivité)} \\
          \text{sinon } x \in A \setminus \{ a \} \implies x \preccurlyeq M \preccurlyeq a \implies x \preccurlyeq a
        \end{array}\right\} \implies \forall x \in A, x \preccurlyeq a
        $$
        
        Donc $A$ admet un plus grand élément, et c'est $a$.
        
        \item Sinon, si $M \succ a$, mais $M \in A$ et 
        $$
        \forall x \in A, \left. \begin{array}{ll}
          \text{si }x = a, x \preccurlyeq M\\
          \text{sinon } x \in A \setminus \{ a \} \implies x \preccurlyeq \max(A\setminus \{ a \}) =  M
        \end{array}\right\} \implies \forall x \in A, x \preccurlyeq a
        $$
        Donc $A$ admet un plus grand élément, et c'est $M$
      \end{itemize}
      \item On procède de même pour construire le le plus petit élément de $A$ avec $m$.
    \end{itemize}
    Donc $\mathcal{H}_{n+1}$ est vraie.
    Donc toute partie finie non vide d'un ensemble totalement ordonné possède un plus petit et un plus grand élément.
  \end{itemize}
  Étudions l'importance des hypothèses :
  \begin{itemize}[label=$\star$]
    \item Importance de la finitude de la partie :
    
    On sait qu'une partie infinie d'un ensemble totalement ordonné n'admet pas de plus grand élément : $[0, 1[$ dans $(\mathbb{R}, \leqslant)$, $\mathbb{N}$ dans $(\mathbb{R}, \leqslant)$.
    \item Importance du caractère total de l'ordre : on connait des ensembles finis partiellement ordonnés qui n'ont pas de plus grand élément :
    \begin{itemize}
      \item $\{ 3, 12 \}$ dans $(\mathbb{R}, =)$ n'admet pas de plus grand élément
      \item $\{ [1, 2], [3, 4] \}$ dans $(\mathcal{P}(\mathbb{R}), \subset)$ n'admet pas de plus grand élément
      \item $\{ 2, 3 \}$ dans $(\mathbb{N}, |)$ non plus.
    \end{itemize}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}
  [\noindent Soit $(E, \leq)$ un ensemble ordonné, et $A$ une partie non-vide de $E$. \\
  Si $A$ admet un plus grand élément alors $A$ admet une borne supérieure et $\sup{A} = \max{A}$. \\
  Si $A$ admet une borne supérieure appartenant à elle-même alors $A$ admet un plus grand élément et $\max{A} = \sup{A}$.]
  {Si $A$ admet un plus grand élément c'est aussi sa borne supérieure. Si $A$ admet une borne supérieure dans $A$ c'est sont plus grand élément.}
  
  Soient un tel ensemble $E$ et une telle partie $A$ et notons $M$ son plus grand élément. \\
  Posons l'ensemble des majorants de $A$, $M(A) = \{ m\in E \ | \ \forall a \in A, \ a \leq m\}$. \\
  Par définition :
  \[
  \forall m \in M(A), \ M \leq m,
  \]
  car $M\in A$, mais comme $M\in M(A)$, on a directement que $M = \min{M(A)} = \sup{A}$. \\
  
  Pseudo-réciproquement, soit $A$ une partie de $E$ admettant une borne supérieure dans elle même, notons cette borne $S$. \\
  Comme $S \in M(A)$, par définition, $S$ est plus grand que tous les éléments de $A$ mais appartient à $A$, donc de tous les éléments de $A$, $S$ est le plus grand.
\end{question_kholle}

\begin{question_kholle}
  [\begin{equation}
    \forall (a, b) \in \Z^2,
    \exists ! (q, r) \in \Z \times \N :
    \left\{ \begin{matrix}
      a = b q + r \\
      r \in {[\![} 0 ; |b|-1 {]\!]}
    \end{matrix} \right.
  \end{equation}]
  {Théorème de la division Euclidienne dans \Z}
  
  \textit{Unicité} \;
  Soient deux tels entiers $(a,b) \in \Z^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \N\right)^2$ tels que
  \begin{equation*}
    \left\{ \begin{matrix}
      a = b q + r \\
      0 \leqslant r \leqslant |b| - 1
    \end{matrix} \right.
    \qquad
    \left\{ \begin{matrix}
      a = b q' + r' \\
      0 \leqslant r' \leqslant |b| - 1
    \end{matrix} \right.
  \end{equation*}
  Directement,
  \[
  b(q-q') = r'-r,
  \]
  mais comme $-(|b|-1) \leqslant r' - r \leqslant |b| -1$, il vient en divisant par $|b|$ l'inégalité précédente :
  \[
  -1 < q - q' < 1,
  \]
  puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
  \newline
  \\
  \textit{Existence} \; Posons pour $b \geqslant 1$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
  \begin{itemize}
    \item $\Omega \subset \Z$
    \item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots)
    \item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
  \end{itemize}
  Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
  Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $\Omega \subset \Z$, $q \in \Z$ donc $r \in \Z$.
  \\
  Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r\in [\![ 0, |b| -1 ]\!]$.
  
  Si $b< 1$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
\end{question_kholle}

\begin{question_kholle}{Une suite décroissante et minorée de nombres entiers relatifs est stationnaire}
  Soit $u \in \Z^\N$ une suite décroissante et minorée fixée quelconque. \\
  Considérons $A = \{ u_n \;|\; n \in \N \}$ c'est-à-dire l'ensemble des valeurs prises par la suite $u$. \\
  $A$ est : \begin{itemize}[label=\textemdash]
    \item une partie de \Z car $u$ est à valeur dans \Z
    \item non vide car $u_0 \in A$
    \item minoré car $u$ est minorée
  \end{itemize}
  Donc $A$ admet un plus petit élément. Donc $\exists n_0 \in \N: u_{n_0} = min A$. Fixons un tel $n_0$. \\
  Soit $n \in \N$ fq tq $n \geqslant n_0$.
  \begin{equation*}
    \left. \begin{matrix}
      u_n \in A \implies u_n \geqslant \min A = u_{n_0} \\
      u \text{ est décroissante et } n \geqslant n_0 \text{ donc } u_n \leqslant u_{n_0}
    \end{matrix}
    \right\} \implies u_n = u_{n_0}
  \end{equation*}
  Ainsi, $u$ est stationnaire.
\end{question_kholle}

\begin{question_kholle}[{
  Soit $A \in \mathcal{P}(\mathbb{R})$ une partie non vide et majorée.
  Soit $\sigma \in \mathbb{R}$
$$
  \sigma = \sup A \iff \left\{ \begin{array}{ll}
    \forall a \in A, a \leqslant \sigma \\
    \forall \varepsilon \in \mathbb{R}_{+}^{*}, \exists a \in A : \sigma - \varepsilon < a \leqslant \sigma 
  \end{array}\right.
$$
  }]{Caractérisation par les $\varepsilon$ de la borne supérieure}
  
  \begin{itemize}[label = $\star$]
    \item Supposons $\sigma = \sup A$
    \begin{itemize}[label = $\bullet$]
      \item Par définition $\sup A = \min M(A)$ donc $\sigma \in M(A)$ donc $\forall a \in A, a \leqslant \sigma$
      
      \item Soit $\varepsilon >0$ fixé quelconque
      
      \begin{align*}
        \sigma = \min M(A) &\iff \sigma - \varepsilon \not\in M(A) (\text{ sinon } \sigma - \varepsilon \geqslant \min M(A)= \sigma \implies \varepsilon \leqslant 0) \\
        &\iff \exists a \in A: \sigma - \varepsilon < a \leqslant \sigma
      \end{align*} 
      
    \end{itemize}
    
    \item Réciproquement, supposons
$$
    \left\{ \begin{array}{ll}
      \forall a \in A, a \leqslant \sigma \\
      \forall \varepsilon \in \mathbb{R}_{+}^{*}, \exists a \in A : \sigma - \varepsilon < a \leqslant \sigma 
    \end{array}\right.
$$
    \begin{itemize}[label = $\bullet$]
      \item D'après la première propriété, $\sigma \in M(A)$
      \item Montrons que $\sigma$ est le plus petit des minorants par l'absurde en supposant qu'il existe $M \in M(A)$ tel que $M < \sigma$.
      On a $\sigma - M >0$ donc on peut appliquer la deuxième propriété pour $\varepsilon \leftarrow \sigma - M$
$$
      \exists a \in A : \sigma - (\sigma - M )<a 
$$
      Fixons un tel $a$.
      On a donc trouvé un $a \in A$ tel que $M < a$ ce qui contredit le fait que $M$ soit un majorant de $A$.
      Donc il n'existe pas de majorant plus petit que $\sigma$.
      Donc $A$ admet une borne supérieure qui est $\sigma$.
    \end{itemize}
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}[]{Montrer que si $A$ et $B$ sont deux parties non vides majorées de $\R$, alors $\sup(A+B) = \sup A + \sup B$}
  Soient $A$ et $B$ deux parties non vides et majorées de $\mathbb{R}$. On note $A+B$ l'ensemble
$$
  A+B = \{ a+b \mid (a, b) \in A\times B \}
$$
  C'est aussi une partie non vide de $\mathbb{R}$.
  
  Soit $x \in (A+B)$ fixé quelconque. Par définition de $A+B$, $\exists(a, b) \in A\times B : x=a+b$
  
$$
  \left. \begin{array}{ll}
    a \leqslant \sup A\\
    b \leqslant \sup B
  \end{array}\right\} \implies x = a+b \leqslant \sup A + \sup B
$$
  On a donc montré que $\sup A+\sup B$ est un majorant de $A+B$, donc $A+B$ admet un majorant, donc $A+B$ est une partie non vide majorée de $\mathbb{R}$, donc $A+B$ admet une borne supérieure.
  
  Par définition de la borne supérieure, car $\sup(A+B)$ est le plus petit élément de l'ensemble des majorants :
$$\sup(A+B) \leqslant \sup A + \sup B$$
  
  De plus $\sup(A+B)$ est un majorant de $A+B$ donc, pour $(a, b) \in A\times B$ fixés, on a
$$
  a+b \leqslant \sup (A+B) \iff a \leqslant \sup(A+B) -b
$$
  en relâchant le caractère fixé de $a$, on a
$$
  \forall a \in A, a\leqslant \sup(A+B) - b
$$
  donc $\sup(A+B) - b$ est un majorant de $A$, donc plus petit que $\sup A$, d'où
  
$$
  \sup A \leqslant \sup(A+B) - b \iff b \leqslant \sup(A+B) - \sup A
$$
  Donc en relâchant le caractère fixé de $b$ on a
$$
  \forall b \in B, b\leqslant \sup(A+B) - \sup A
$$
  donc $\sup(A+B) - \sup A$ est un majorant de $B$ donc plus petit que $\sup B$
  d'où
  
$$
  \sup B \leqslant \sup(A+B) - \sup A \iff \sup A + \sup B \leqslant \sup (A+B)
$$
  Donc par double inégalité
$$
  \sup A + \sup B = \sup (A+B)
$$
  
\end{question_kholle}
\pagebreak\section{Semaine 10}

\begin{question_kholle}
  [\noindent Soient $(A, B) \in \mathcal{P}(\R)^2$ fq. \\
  \textit{Définition de la densité}
  \begin{align}
    A \text{ est dense dans } B
    \text{ si } \left\{ \begin{array}{ll}
      A \subset B \\
      \mathrm{et} \\
      \forall (u,v) \in \R^2, B \cap {]}u;v{[} \neq \emptyset \implies A \cap  {]}u;v{[} \neq \emptyset
    \end{array} \right.
  \end{align}
  \textit{Caractérisation de la densité par les $\varepsilon$}
  \begin{align}
    A \text{ est dense dans } B
    \iff \left\{ \begin{array}{ll}
      A \subset B \\
      \mathrm{et} \\
      \forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon
    \end{array} \right.
  \end{align}
  ]
  {Caractérisation de la densité d’une partie $A$ de \R dans une partie $B$ de \R la contenant avec des $\varepsilon$.}
  
  \textit{Montrons la caractérisation de la densité}\\
  \emph{Sens Direct} Supposons $A$ dense dans $B$
  \begin{itemize}[label=\textemdash]
    \item Par déf $A \subset B$
    \item Soit $b \in B$ et $\varepsilon \in \R_+^*$ fq
    
    Appliquons le (ii) de la déf de Densité pour $u \leftarrow b - \varepsilon$ et $v \leftarrow b + \varepsilon$
    $$B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset \implies A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$$
    Or, $B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset$ est vraie
    donc $A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$
    
    Ce qui permet de choisir $a \in A \cap ]b - \varepsilon,  b + \varepsilon[$.
    Un tel $a$ vérifie $a \in A$ et $a \in ]b - \varepsilon,  b + \varepsilon[ \iff |b-a| < \varepsilon$
  \end{itemize}
  \bigbreak
  \noindent \emph{Sens réciproque} Supposons $\left\{\begin{array}{ll} A \subset B \\\mathrm{et}\\ \forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon \end{array}\right.$
  
  \begin{itemize}
    \item On a donc $A \subset B$
    \item Soient $(u, v) \in \R^2$ fq tq $B \cap ]u, v[ \neq \emptyset$
    
    Soit $b \in B \cap ]u, v[$ fq.
    Appliquons l'hypothèse pour $b\leftarrow b$ et $\varepsilon \leftarrow \min\{v - b, b - u\}$, qui est autorisé $v-b$ et $b-u$ sont positifs
    
    Donc $\exists a \in A: | b - a| < \varepsilon $
    
    Fixons un tel a, alors:
    $$
    b-\varepsilon < a < b + \varepsilon
    $$
    
    Donc $$
    \left\{\begin{array}{ll}
      a < b + \varepsilon = b + \underbrace{\min\{v - b, b - u\}}_{\leqslant v - b} \leqslant b + v - b = v \\ \mathrm{et}\\
      a > b - \varepsilon = b - \underbrace{\min\{v - b, b - u\}}_{\leqslant b - u} \geqslant b - (b - u) = u
    \end{array}\right.
    $$
    
    Donc $a \in ]u, v[$.
  \end{itemize}
  Donc $A \cap ]u, v[ \neq \emptyset$
  
\end{question_kholle}

\begin{question_kholle}
  [\begin{equation}
    \forall (a, b) \in \R \times \R^*,
    \exists ! (q, r) \in \Z \times \R :
    \left\{ \begin{matrix}
      a = b q + r \\
      r \in [0;|b|[
    \end{matrix} \right.
  \end{equation}]
  {Théorème de la division pseudo-euclidienne dans \R}
  
  \textit{Unicité} \;
  Soient deux tels entiers $(a,b) \in \R^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \R\right)^2$ tels que
  \begin{equation*}
    \left\{ \begin{matrix}
      a = b q + r \\
      r \in [0;|b|[
    \end{matrix} \right.
    \qquad
    \left\{ \begin{matrix}
      a = b q' + r' \\
      r' \in [0;|b|[
    \end{matrix} \right.
  \end{equation*}
  Directement,
  \[
  b(q-q') = r'-r,
  \]
  mais comme $-|b| < r' - r < |b|$, il vient en divisant par $|b|$ l'inégalité précédente :
  \[
  -1 < q - q' < 1,
  \]
  puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
  \newline
  \\
  \textit{Existence} \; Posons pour $b > 0$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
  \begin{itemize}
    \item $\Omega \subset \Z$
    \item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots)
    \item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
  \end{itemize}
  Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
  Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $r \in \R$.
  \\
  Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r \in [ 0, |b| [$.
  
  Si $b < 0$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
\end{question_kholle}

\begin{question_kholle}
  {\Q est dense dans \R et $\R \setminus \Q$ est aussi dense dans \R}
  
  Soit $x \in \R$ fq.
  Posons $\forall n \in \N, a_n = \frac{\lfloor2^n x\rfloor}{2^n}$. \\
  Soit $n \in \N$ fq. \\
  \begin{itemize}
    \item $a_n \in \Q$ car $\lfloor2^n x\rfloor \in \Z$ et $2^n \in \N$.
    \item \begin{equation*}
      a_n = \frac{\lfloor2^n x\rfloor}{2^n}
      \implies \frac{2^n x - 1}{2^n} \leqslant a_n \leqslant \frac{2^n x}{2^n}
      \implies x - \frac{1}{2^n} \leqslant a_n \leqslant x
    \end{equation*}
    Or $\nicefrac{1}{2^n} \arrowlim{n}{+\infty} 0$ donc d'après le théorème d'existence de limite par encadrement, \\ $a_n \arrowlim{n}{+\infty} x$.
  \end{itemize}
  Donc d'après la caractérisation séquentielle de la densité, \Q est dense dans \R.
  \bigbreak
  
  \noindent Soit $x \in \R$ fq. \\
  Alors $x + \sqrt{2} \in \R$.
  D'après la démonstration précédente, $\exists b \in \Q^\N : b_n \arrowlim{n}{+\infty} x + \sqrt{2}$. \\
  Fixons un telle suite $b$.
  Considérons $c = b - \sqrt{2}$. \\
  Soit $n \in \N$ fq.
  \begin{itemize}
    \item $c_n \in \R\setminus\Q$ car $b_n \in \Q$ et $\sqrt{2} \in \R \setminus \Q$.
    \item \begin{equation*}
      \left. \begin{matrix}
        b_n \arrowlim{n}{+\infty} x + \sqrt{2} \\
        c_n = b_n - \sqrt{2}
      \end{matrix} \right\}
      \implies c_n \arrowlim{n}{+\infty} x
    \end{equation*}
  \end{itemize}
  Donc d'après la caractérisation séquentielle de la densité, $\R\setminus \Q$ est dense dans \R.
\end{question_kholle}

\begin{question_kholle}[
  Soit u $\in \K ^ \N, (\ell_1, \ell_2) \in \K ^2$
  Si u converge vers $\ell_1$ et $\ell_2$, alors $\ell_1 = \ell_2$
  ]{Preuve de l'unicité de la limite d'une suite convergente}
  Par l'absurde, supponsons que $u$ converge vers $\ell_1$ et $\ell_2$, et $\ell_1 \neq \ell_2$.
  On prendra $\varepsilon_0 = \varepsilon_1 = \varepsilon_2$ assez petit pour que les tubes soient disjoints.\\
  Posons donc $\varepsilon_0 = \frac{|\ell_1 - \ell_2|}{3}$
  \begin{itemize}
    \item Appliquons la définition de la convergence de u vers $\ell_1$, pour $\varepsilon \leftarrow \varepsilon_0$, ce qui est autorisé car $\varepsilon_0 \in \R_+^*$
    \begin{equation}\label{eq:1}
      \exists N_1 \in \N : \forall n \in \N, n \geqslant N_1 \implies |u_n - \ell_1| \leqslant \varepsilon_0
    \end{equation}
    \begin{equation}\label{eq:2}
      \exists N_2 \in \N : \forall n \in \N, n \geqslant N_2 \implies |u_n - \ell_2| \leqslant \varepsilon_0
    \end{equation}
    Fixons de tels $N_1$ et $N_2$.
    \item Posons $n_0 = N_1 + N_2$
    \begin{itemize}
      \item $n_0 \geqslant N_1$, donc (\ref{eq:1}) s'applique: $|u_{n_0} - \ell_1| \leqslant \varepsilon_0$
      \item $n_0 \geqslant N_2$, donc (\ref{eq:2}) s'applique: $|u_{n_0} - \ell_2| \leqslant \varepsilon_0$
    \end{itemize}
    \item \begin{align*}
      |\ell_1 - \ell_2| & = |\ell_1 - u_{n_0} + u_{n_0} - \ell_2|                                                                                         \\
      & \leqslant \underbrace{|\ell_1 - u_{n_0}|}_{\leqslant \varepsilon_0} + \underbrace{|u_{n_0} - \ell_2|}_{\leqslant \varepsilon_0} \\
      & \leqslant 2 \frac{|\ell_1 - \ell_2|}{3}                                                                                         \\
      \implies 1        & \leqslant \frac 2 3
    \end{align*}
    Contradiction
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{
  Soient $(x, y) \in \R^2$ tels que $x \leqslant y$. $$[x, y] = \{z \in \R \mid x \leqslant z \leqslant y \} = \{tx + (1-t)y \mid t \in [0, 1]\}$$
  }]{Description d'un segment de la droite réelle par les barycentres à coefficients positifs.}
  Le résultat est immédiat pour $x = y$ : $\forall t \in [0, 1], xt+(1-t)x = xt - xt +x =x = y$
  
  Supposons que $x <y$. On procède par double inclusion.
  \begin{itemize}[label=$\star$]
    \item Soit $z \in \{ xt+(1-t)y \mid t \in [0, 1] \}$.
    
    $\exists t \in [0, 1] : z = xt+(1-t)y$
    
    Puisque $t \in [0, 1]$, $t \geqslant 0$ et $1-t \geqslant 0$.
    Donc
    
    \begin{align*}
      x < y \implies x\leqslant y &\underset{ \substack{t \geqslant 0 \\1-t \geqslant 0} }{ \implies } \left\{ \begin{array}{ll}
        tx \leqslant ty \\
        (1-t)x \leqslant (1-t)y 
      \end{array}\right. \\
      &\implies \left\{ \begin{array}{ll}
        tx + (1-t)y \leqslant ty + (1-t) y\\
        tx + (1-t)x  \leqslant tx + (1-t)y
      \end{array}\right. \\
      & \implies \left\{ \begin{array}{ll}
        z \leqslant y \\
        x \leqslant z 
      \end{array}\right. \\
      & \implies z \in[x, y]
    \end{align*}
    
    
    
    \item Réciproquement, soit $z \in[x, y]$. Cherchons le $t\in[0, 1]$ tel que $tx + (1-t)y = z$.
    
$$
    tx + (1-t)y = z \iff t(x-y) = z-y \iff t = \frac{z-y}{x-y} = \frac{y-z}{y-x} \text{ (autorisé car }x<y \implies x-y \neq 0 \text{)}
$$
    
    Vérifions si ce $t$ convient : posons $t = \frac{y-z}{y-x}$. 
    \begin{itemize}[label=$\bullet$]
      \item Vérifions d'abord que $t \in [0,1]$
      
$$
      x\leqslant z \leqslant y
      \implies x-y \leqslant z-y \leqslant 0 \implies y-x\geqslant y-z \geqslant 0 \implies 1 \geqslant \frac{y-z}{y-x} \geqslant 0 \implies 0\leqslant t \leqslant 1
$$
      \item Calculons
$$
      tx+(1-t)y = \frac{y-z}{y-x} x + \left( 1- \frac{y-z}{y-x} \right)y = \frac{y-z}{y-x}x + \frac{z-x}{y-x}y = \frac{yx-zx+zy - xy}{y-x} =  z
$$
      Donc ce $t$ convient.
    \end{itemize}
    Donc $z \in \{ xt+(1-t)y \mid t \in [0, 1] \}$.
  \end{itemize}
  Donc $\{ xt+(1-t)y \mid t \in [0, 1] \} = [x, y]$.
\end{question_kholle}
\begin{question_kholle}{Une suite convergente est bornée}
  
  Soit $u \in \mathbb{K}^{\mathbb{N}}$ convergente.
  Posons $\ell = \lim u$
  Appliquons la définition de la convergence pour $\varepsilon \leftarrow 1$
  $$
  \exists N_{1}\in \mathbb{N}: \forall n \in \mathbb{N}, n \geqslant N_{1} \implies |u_{n}-\ell| \leqslant 1
  $$
  Fixons un tel $N_{1}$
  Posons alors $M = \max\left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$, qui est bien défini, car toute partie finie, non vide d'un ensemble totalement ordonné (ici $(\mathbb{R}, \leqslant)$) admet un pgE.
  
  Soit $n \in \mathbb{N}$ fq.
  \begin{itemize}
    \item Si $n \in [[0, N_{1}]], |u_{n}| \in \left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$ donc $|u_{n}| \leqslant M$
    \item Sinon,
  \end{itemize}
  
  \begin{align*}
    n> N_{1} & \implies |u_{n} - \ell| \leqslant 1              \\
    & \implies |u_{n}| - |\ell| \leqslant 1            \\
    & \implies |u_{n}| \leqslant 1+ |\ell| \leqslant M
  \end{align*}
  
  Ainsi, $\forall n \in \mathbb{N}, |u_{n}| \leqslant M$.
\end{question_kholle}
\begin{question_kholle}[{Soit $A \in \mathcal{P}(\R)$ non vide et majorée. Soit $\sigma \in \R$
  $$
  \sigma = \sup A \iff \left\{ \begin{array}{ll}
    \sigma \in M(A)  \\
    \exists (a_{n})_{n \in \N} \in A^{\N} : \lim_{n \to +\infty} a_n = \sigma
  \end{array}\right.
  $$
  }]{Caractérisation séquentielle de la borne supérieure}
  \begin{itemize}[label=$\star$]
    \item Supposons que $\sigma = \sup A$.
    \begin{itemize}[label=$\bullet$]
      \item Par définition d'une borne sup, $\sigma \in M(A)$.
      \item Soit $n \in \N$. Appliquons la caractérisation de la borne sup par les epsilon pour $\varepsilon \leftarrow \frac{1}{2^{n}}$.
$\exists c \in A : \sigma - \frac{1}{2^{n}} < c \leqslant \sigma$.
      Fixons un tel $c$ et notons le $a_{n}$. En relâchant le caractère fixé de $n$, on a crée la suite $(a_{n})_{n \in \N}$ telle que
$$
      \forall n \in \N, \sigma - \frac{1}{2^{n}} < a_{n}\leqslant \sigma
$$
      Cette suite converge vers $\sigma$ par encadrement.
    \end{itemize}
    \item Réciproquement, supposons que $\sigma \in M(A)$ et qu'il existe une suite $(a_{n})_{n \in \N}$ d'éléments de $A$ qui converge vers $\sigma$. Montrons que $\sigma = \sup A$ d'après la caractérisation par les $\varepsilon$.
    \begin{itemize}[label=$\bullet$]
      \item$\sigma \in M(A)$
      \item Soit $\varepsilon>0$. Appliquons la définition de la convergence de $a$ pour $\varepsilon \leftarrow \frac{\varepsilon}{2}$
      
$$
      \exists N \in \N : \forall n \geqslant N, \lvert a_{n} - \sigma \rvert  \leqslant \frac{\varepsilon}{2} \implies \sigma - \frac{\varepsilon}{2}\leqslant a_{n}
$$
      En particulier $a_{N} \in A$ vérifie
$$
      \sigma - \varepsilon < \sigma - \frac{\varepsilon}{2} \leqslant a_{N} \underbrace{ \leqslant }_{ \sigma \in M(A) } \sigma
$$
      Ce qui permet de conclure.
      Donc $\sigma = \sup A$.
    \end{itemize}
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 11}

\begin{question_kholle}{Convergence d'une suite si ses sous-suites paires et impaires convergent}
  Soit $(a_{n})_{n \in \N}$ telle que $(a_{2n})_{n \in \N}$ et $(a_{2n+1})_{n \in \N}$ convergent vers la même limite $\ell$.
  Montrons que $a$ converge vers $\ell$.
  
  Soit $\varepsilon>0$. On veut construire un $N \in \N$ tel que $\forall n \geqslant N, \lvert a_{n} - \ell \rvert \leqslant \varepsilon$
  
  Appliquons la définition de la limite de $(a_{2n})_{n \in \N}$ et $(a_{2n+1})_{n \in \N}$
  
  \begin{align*}
    \exists N_{1} \in \N : \forall n \geqslant N_{1}, \lvert a_{2n} - \ell\rvert  \leqslant \varepsilon \\
    \exists N_{2} \in \N : \forall n \geqslant N_{2}, \lvert a_{2n+1} - \ell  \rvert \leqslant \varepsilon
  \end{align*}
  \begin{figure}[H]
    
    \centering
    \begin{tikzpicture}
      \draw[thick, dashed] (0.5,2) -- (10,2) node[above left] {$\ell$};
      
      \draw[dotted, red] (0.5,2.5) -- (10,2.5) node[right] {$\ell + \varepsilon$};
      \draw[dotted, red] (0.5,1.5) -- (10,1.5) node[right] {$\ell - \varepsilon$};
      
      \foreach \x in {0.5, 1.5, 2.5, 3.5} {
      \node[circle, fill=blue, inner sep=1.5pt] at (\x, {1 + 0.8*rnd}) {}; % Odd terms
      }
      \foreach \x in {1, 2, 3, 4} {
      \node[circle, fill=green, inner sep=1.5pt] at (\x, {1 + 1.8*rnd}) {}; % Even terms
      }
      
      \node[circle, fill=green, inner sep=1.5pt] at (5, 1) {};
      
      \node[below] at (4.5,0) {$2N_1$};
      \draw[thick, dashed, blue] (4.5, 0) -- (4.5, 2.5) {};
      
      \node[below] at (6,0) {$2N_2 + 1$};
      \draw[thick, dashed, green] (6, 0) -- (6, 2.5) {};
      
      \foreach \x in {4.5, 5.5, 6.5, 7.5, 8.5} {
      \node[circle, fill=blue, inner sep=1.5pt] at (\x, {1.8 + 0.4*rnd}) {};
      }
      
      \foreach \x in { 6, 7, 8} {
      \node[circle, fill=green, inner sep=1.5pt] at (\x, {1.8 + 0.4*rnd}) {};
      }
      
      \draw[->] (0,0) -- (10.5,0) node[right] {$n$};
      \node[below] at (0.5,0) {$0$};
      
      \draw[->] (0.5,0) -- (0.5, 3) node[right] {$a_n$};
      
      \draw[<->, red] (10,2.5) -- (10,1.5);
      \node[right, red] at (10,2) {$\varepsilon$};
      
    \end{tikzpicture}
    \caption{Les termes pairs et impairs sont contenus dans un voisinnage de $\ell$ après certains rangs}
  \end{figure}
  
  Posons $N = \max(2N_{1}, 2N_{2}+1)$ et vérifions que ce rang convient.
  Soit $n \in \N$ tel que $n \geqslant N$. 
  \begin{itemize}[label=$\star$]
    \item Si $n$ est pair, $\exists p \in \N :$ $n = 2p$
$$
    n\geqslant N \geqslant 2N_{1} \implies 2p \geqslant 2N_{1} \implies p \geqslant N_{1}
$$
    Donc d'après la définition de la convergence de $(a_{2n})_{n \in \N}$, on a
$$
    \lvert a_{2p} - \ell \rvert  \leqslant \varepsilon \implies \lvert a_{n} - \ell  \rvert  \leqslant \varepsilon
$$
    
    \item Si $n$ est impair, $\exists p \in \N : n = 2p +1$
$$
    n \geqslant N \geqslant 2N_{2}+1 +\implies 2p+1 \geqslant 2N_{2} + 1 \implies p \geqslant N_{2}
$$
    Donc d'après la définition de la convergence de $(a_{2n+1})_{n \in \N}$, on a 
$$
    \lvert a_{2p+1} - \ell\rvert  \leqslant \varepsilon \implies \lvert a_{n} - \ell \rvert \leqslant \varepsilon
$$
  \end{itemize}
  Donc $\lvert a_{n} - \ell  \rvert\leqslant \varepsilon$
  Donc $a$ tend vers $\ell$.
  
\end{question_kholle}
\textbf{Remarque} Si les deux suites ne convergent pas vers la même limite, comme pour $((-1)^{n})_{n \in \N}$, la suite n'admet pas de limite.

\begin{question_kholle}
  [Soient $(A,B) \in (\mathcal{P}(\R) \setminus \{\varnothing\})^{2}$. Montrons que :
  \\
  $$A \text{ est dense dans } B \iff \left\{ \begin{array}{l}
    A \subset B \\
    \forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b
  \end{array}\right. $$
  ]
  {Caractérisation séquentielle de la densité.}
  
  Sens indirect : supposons $A \subset B$ et $\forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b$ :\\
  \begin{itemize}
    \item[$\star$] $A \subset B$ par hypothèse.
    \item[$\star$] Montrons que $\forall b \in B, \forall \varepsilon \in \R^{*}_{+}, \exists a \in A : |b - a| < \varepsilon$ (on utilise la caractérisation de la densité avec les $\varepsilon$) \\
    Soient $b \in B$ et $\varepsilon \in \R^{*}_{+}$ fixés quelconques : \\
    Par hypothèse appliquée pour $b \leftarrow b$ : $\exists(a_{n}) \in A^{\N} : a_{n} \underset{n \to +\infty}{\longrightarrow}b$ \\
    Appliquons la définition de la convergence de $(a_{n})$ vers $b$ pour $\varepsilon \leftarrow \frac{\varepsilon}{2}$ : \\
    $$\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow |a_{n} - b| \leqslant \frac{\varepsilon}{2}$$
    Fixons un tel N : \\
    En particulier, $a_{N} \in A$ et $|a_{N} - b| \leqslant \frac{\varepsilon}{2} \leqslant \varepsilon$ \\
    Donc $A$ est dense dans $B$.
  \end{itemize}
  
  Sens direct : supposons $A$ dense dans $B$ : \\
  \begin{itemize}
    \item[$\star$] Par définition, $A \subset B$
    \item[$\star$] Soit $b \in B$ fixé quelconque. \\
    Soit $n \in \N$ fixé quelconque :  \\
    Appliquons la caractérisation de la densité par les $\varepsilon$ pour $\varepsilon \leftarrow \frac{1}{2^{n}}$ (autorisé car $\frac{1}{2^{n}} > 0$), et $b \leftarrow b$ :
    $$\exists a \in A : |a - b| \leqslant \frac{1}{2^{n}}$$
    Notons $a_{n}$ un tel élément. Nous venons de construire $(a_{n})_{n \in \N} \in A^{\N}$ vérifiant : \\
    $\forall n \in \N, |a_n - b| \leqslant \frac{1}{2^{n}}$ \\
    Or : $\underset{n \to +\infty}{\lim} \frac{1}{2^{n}} = 0$ \\
    Ainsi, d'après le théorème sans nom, $(a_{n})_{n \in \N}$ converge vers $b$.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}
  [Soit $u \in \R^\N$ une suite monotone :
  {\begin{enumerate}
    \item Si $u$ est croissante
    \begin{enumerate}[label=($\roman*$)]
      \item Soit $u$ est majorée, et dans ce cas, $\lim u = \sup\{ u_k | k \in \N \}$
      \item Soit $u$ n'est pas bornée, et dans ce cas, $u$ diverge vers $+\infty$.
    \end{enumerate}
    \item Si $u$ est décroissante :
    \begin{enumerate}[label=($\roman*$) Soit, leftmargin=4em]
      \item $u$ est minorée, et dans ce cas, $\lim u = \inf\{ u_k | k \in \N \}$
      \item $u$ n'est pas bornée, et dans ce cas, $u$ diverge vers $-\infty$.
    \end{enumerate}
  \end{enumerate} }]
  {Théorème de la convergence monotone}
  
  Soit $u \in \R^\N$ monotone fq.
  
  \begin{enumerate}
    \item Supposons que $u$ est croissante.
    \begin{enumerate}[label=($\roman*$)]
      \item Supposons que $u$ est majorée. \\
      Alors $\exists M \in \R : \forall n \in \N, u_n \leqslant M$. Fixons un tel M. \\
      $\Omega = \{ u_k | k \in \N \}$ est \begin{itemize}
        \item une partie de \R
        \item non vide car $u_0$ y appartient
        \item majorée par M
      \end{itemize}
      donc elle admet un borne supérieure et notons-la $\sigma$. \\
      Soit $\epsilon \in \R_+^*$ fq. \\
      $\sigma - \epsilon < \sigma$ donc $\sigma - \epsilon$ ne majore pas $\Omega$. Donc $\exists N \in \N : u_N > \sigma - \epsilon$. Fixons un tel N. \\
      Soit $n \in \N$ fq tq $n \geqslant N$. \\
      Alors $u_n \underset{\text{par croissant de u}}{\geqslant} u_N \geqslant \sigma - \epsilon$ et $u_n \underset{\text{par défintion de }\sigma}{\leqslant} \sigma$. \\
      Ainsi,
      \begin{equation*}
        \begin{aligned}
          \sigma - \epsilon \leqslant u_n \leqslant \sigma
          & \implies - \epsilon \leqslant u_n - \sigma \leqslant 0 \\
          & \implies | u_n - \sigma | \leqslant \epsilon
        \end{aligned}
      \end{equation*}
      Donc $u_n \arrowlim{n}{+\infty} \sigma$.
      
      \item Supposons que $u$ n'est pas bornée. \\
      Soit $A \in \R$ fq. \\
      $u$ n'est pas bornée donc $\exists N \in \N : u_N > A$. \\
      Or $u$ est croissante donc $\forall n \in \N, n \geqslant N \implies u_n \geqslant A$. \\
      Donc $u_n \arrowlim{n}{+\infty} +\infty$.
    \end{enumerate}
    
    \item Supposons que $u$ est décroissante. \\
    Il suffit dans la preuve ci-dessus de remplacer les inégalités inférieures par des inégalités supérieures et inversement et d'utiliser la notion de borne inférieure plutôt que de borne supérieure.
    \begin{enumerate}[label=($\roman*$)]
      \item Si $u$ est minorée, $u_n \arrowlim{n}{+\infty} \inf\{u_k|k\in\N\}$.
      \item Si $u$ n'est pas bornée, $u_n \arrowlim{n}{+\infty} -\infty$.
    \end{enumerate}
  \end{enumerate}
\end{question_kholle}

\begin{question_kholle}
  [Soit $u\in \R ^{\N}$ qui converge vers $\ell \in \R$. \\
  Alors la moyenne arithmérique des $n\in \N$ premiers termes (appelée moyenne de Césarò) converge vers $\ell$.]
  {Théorème de Césarò}
  
  Soient $u$ une telle suite, $\varepsilon \in \R ^*_+$ et $\ell \in \R$ ladite limite de $u$. Appliquons la définition de la convergence de $u$ pour $\varepsilon \gets \frac{\varepsilon}{2}$ :
  \[
  \exists N \in \N \ : \ \forall n \in \N , \ n\geq N \ \implies \ |u_n - \ell | \leq \frac{\varepsilon}{2}.
  \]
  Fixons un tel $N$. Posons $\omega = \sum_{k=0}^{N-1} |u_k - \ell | \in \R$. Soit $n\in \N$ tel que $n \geq N$. Calculons :
  \[
  \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| = \left| \frac{1}{n} \left( \sum_{k=0}^{n-1}u_k - n\ell \right) \right|  = \left| \frac{1}{n} \sum_{k=0}^{n-1}(u_k - \ell)  \right| \leq \frac{1}{n} \underset{= \ \omega \in \R}{\underbrace{\sum_{k=0}^{N-1}|u_k - \ell|}} + \frac{1}{n} \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\sum_{k=N}^{n}|u_k - \ell|}} \leq \frac{\omega}{n} + \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\varepsilon}{2n}}}.
  \]
  Ces majorations sont issues de l'inégalité triangulaire et de la convergence de $u$. De plus, comme la suite $(v_n) _{n\in \N} = \left( \frac{\omega}{n} \right) _{n\in \N}$ converge vers $0$, on écrit sa définition pour $\varepsilon \gets \frac{\varepsilon}{2}$ :
  \[
  \exists N' \in \N \ : \ \forall n \in \N , \ n\geq N' \ \implies \ |v_n| \leq \frac{\varepsilon}{2}.
  \]
  On fixe un tel $N'$ et on pose $\Lambda = \max{(N, N')}$ qui a bien un sens car $\{N, \ N'\}$ est une partie finie de $\N$.
  De la même manière qu'auparavant, pour $n\in \N$ tel que $n \geq \Lambda$, on a :
  \[
  \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| \leq \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\omega}{n}}} + \frac{\varepsilon}{2} \leq \varepsilon.
  \]
  C'est le théorème souhaité.
\end{question_kholle}

\begin{question_kholle}
  [Soient $(u,v) \in \R^{\N}$ : \\
  {\begin{enumerate}[label=($\roman*$)]
    \item Si $\begin{array}{|l}
      \exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0 \\
      u \text{ converge}
    \end{array}$ \\
    Alors $\lim u \geqslant 0$
    \item Si $\begin{array}{|l}
      \exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n} \\
      u \text{ et } v \text{ convergent}
    \end{array}$ \\
    Alors $\lim u \leqslant \lim v$
  \end{enumerate}}
  ]
  {Théorème de passage à la limite dans une inégalité.}
  ~\smallbreak
  \begin{enumerate}[label=($\roman*$)]
    \item L'hypothèse $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0$ permet d'affirmer que $u$ et $|u|$ coïncident à partir d'un certain rang. \\
    Par ailleurs, la convergence de $u$ et la continuité de $|\cdot|$ sur $\R$ donc en $\lim u$ donnent $|u|$ converge vers $|\lim u|$. \\
    Le caractère asymptotique de la limite permet de conclure que $u$ et $|u|$ ont la même limite. \\
    Donc $\lim u = |\lim u| \geqslant 0$
    \item $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n} \Rightarrow v_{n} - u_{n} \geqslant 0$ \\
    $u$ et $v$ convergent $\Rightarrow v-u$ converge vers $\lim v - \lim u$. \\
    On applique $(i)$ pour $u \leftarrow v - u$, autorisé car $u \text{ et }v$ convergent. \\
    On obtient $\lim v - \lim u \geqslant 0$ d'où $\lim u \leqslant \lim v$.
  \end{enumerate}
\end{question_kholle}

\begin{question_kholle}
  [Soient $u$ et $v$ deux suites réelles adjacentes. Alors $u$ et $v$ convergent et ont la même limite.]
  {Théorème des suites adjacentes}
  
  Soient $u$ et $v$ de telles suites. Quitte à inverser les rôles desdites suites, prenons $u$ croissante et $v$ décroissante. \\
  On a donc :
  \[
  \forall n \in \N, \ (u_n \leq v_n \leq \underset{\in \R}{\underbrace{v_0}}) \wedge (\underset{\in \R}{\underbrace{u_0}}\leq  u_n \leq v_n),
  \]
  car la monotonie des suites induit ces inégalités. D'après le théorème de limite monotone, $u$ étant croissante et majorée elle converge, $v$ étant décroissante et minorée elle converge. \\
  Il s'en suit que par définition des suites adjacentes :
  \[
  0 \ = \lim_{n \to +\infty} (u_n - v_n) \ \underset{u,v \ \text{ convergent}}{\underbrace{=}} \ \lim_{n \to +\infty} u_n - \lim_{n \to +\infty} v_n.
  \]
  Ainsi, $\lim u = \lim v$.
\end{question_kholle}

\begin{question_kholle}
  [Toute suite bornée réelle admet une sous-suite convergente. \\
  L'ensemble des valeurs d'adhérence d'une suite réelle bornée est non vide.]
  {\emph{Facultative} Théorème de Bolzano-Weierstrass}
  
  Soit $u \in \R^\N$ fq bornée. \\
  Alors $\exists M \in \R_+ : \forall n \in \N, |u_n| \leqslant M$.
  
  Construisons une suite de segments dans $[-M;M]$ de plus en plus petits par dichotomie. \\
  Posons $a_0 = -M$, $b_0 = M$ et définissons les suites $c$ et $I$ pour tout $n$ dans \N par $c_n = \frac{a_n + b_n}{2}$ et $I_n = [a_n;b_n]$. \\
  
  \noindent Soit $n\in \N$ fq.
  Supposons $a_n \text{ et } b_n$ construits et $\{ k \in \N \;|\; u_k \in I_n \}$ infini.
  Construisons les termes d'indices $n+1$. \\
  Posons $\left| \begin{array}{lcr}
    I_n^- & = & \{ k \in \N \;|\; u_k \in [a_n;c_n] \} \\
    I_n^+ & = & \{ k \in \N \;|\; u_k \in [c_n;b_n] \} \\
  \end{array} \right.$ \\
  Nous avons $I_n^- \cup I_n^+ = \{ k \in \N \;|\; u_k \in I_n \}$ donc $I_n^-$ ou $I_n^+$ est infini.
  
  \begin{itemize}
    \item Si $I_n^-$ est infini, posons $\left| \begin{array}{lcl}
      a_{n+1} & = & a_n \\
      b_{n+1} & = & c_n
    \end{array} \right.$ \\
    Ainsi $\{ k \in \N \;|\; u_k \in I_{n+1} \} = I_n^-$ est infini.
    \item Si $I_n^+$ est infini, posons $\left| \begin{array}{lcl}
      a_{n+1} & = & c_n \\
      b_{n+1} & = & b_n
    \end{array} \right.$ \\
    Ainsi $\{ k \in \N \;|\; u_k \in I_{n+1} \} = I_n^+$ est infini.
  \end{itemize}
  \bigbreak
  
  \noindent Étudions la suite $\left(I_n\right)_{n\in\N}$.
  \begin{itemize}
    \item Nous avons toujours $a_n \leqslant b_n$ donc $\forall n \in \N, I_n \neq \emptyset$
    \item Par construction, $\forall n \in \N, I_{n+1} \subset I_n$
    \item $ |I_{n+1}| = |a_{n+1}-b_{n+1}| = \frac{1}{2} |a_n-b_n| = \frac{1}{2} |I_n| $
    donc la suite des cardinaux est une suite géométrique de raison $\nicefrac{1}{2}$.
    Donc $|I_n| \arrowlim{n}{+\infty} 0$.
  \end{itemize}
  Donc, d'après le théorème des segments emboîtés, $\exists ! l\ell \in \R : \underset{n\in\N}{\bigcap} I_n = \{\ell\}$. Fixons un tel $\ell$. \\
  
  Construisons maintenant une extractrice $\varphi$ de $u$. \\
  Posons $\varphi(n) = 0$. \\
  Soit $n \in \N$ fq. Supposons $\varphi(n)$ construite.
  \begin{equation*}
    \varphi(n+1) = \min\{ k \in \N | u_k \in I_{n+1} \wedge k > \varphi(n) \}
  \end{equation*}
  $\varphi(n+1)$ est bien définie car $\{ k \in \N | u_k \in I_{n+1} \}$ est une partie de \N non bornée (car infinie). \\
  
  \noindent Ainsi, nous avons construit $\varphi : \N \rightarrow \N$ strictement croissante. Nous pouvons extraire une sous-suite de $u$. Or $\forall n \in \N, u_{\varphi(n)} \in I_n$ donc
  \begin{equation*}
    \forall n \in \N, \quad
    \underbrace{a_n}_{ \arrowlim{n}{+\infty} \ell } \leqslant u_{\varphi(n)} \leqslant \underbrace{b_n}_{ \arrowlim{n}{+\infty} \ell }
  \end{equation*}
  Donc, d'après le théorème d'existence de limite par encadrement, $u_{\varphi(n)} \arrowlim{n}{+\infty} \ell$. \\
  Ainsi $\ell \in L_u$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $u$ une suite bornée. $u$ converge si et seulement si il existe $\ell \in \mathbb{K}$ tel que $L(u)$ est le singleton $\ell$ ]
  {\emph{Facultative} Caractérisation de la convergence par l'unicité d'une valeur d'adhérence pour une suite bornée.}
  Traitons le cas réel, celui sur \C est à adapter sans peine.\\
  Supposons que $u$ converge et posons $\lim u =\ell \in \R  $. Toutes les sous-suites de $u$ convergent vers $\ell$ donc $L(u)=\{\ell \}$. \\
  Supposons maintenant qu'il existe un unique $\ell \in \R$ tel que $L(u) = \{ \ell \}$. Par l'absurde, supposons que $u$ ne converge pas vers $\ell$, c'est-à-dire :
  \[
  \exists \varepsilon \in \R ^* _+ \ : \ \forall N \in \N, \ \exists n \in \N \ : \ n\geq N \text{ et } |u_n - \ell | > \varepsilon.
  \]
  Fixons un tel $\varepsilon$. \\
  %\textbf{Etape 1} : \textit{Construction d'une sous-suite de $u$ dont les termes sont $\varepsilon$-éloignés de $\ell$.} \\
  Posons $\varphi (0) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon \} }$, ce qui a du sens car c'est une partie non-vide de $\N$. Posons ensuite $\varphi (1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(0) < k \} } $, ce qui a du sens pour les mêmes raisons. On construit en itérant ce procédé $\varphi (n)$ tel que :
  \[
  \forall n \in \N, \ \varphi(n+1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(n) < k \} }.
  \]
  De cette manière, nous venons de construire une extractrice telle que :
  \[
  \forall n \in \N, \ |u_{\varphi(n)} - \ell| > \varepsilon.
  \]
  Par hypothèse $u$ est bornée, donc il existe $M\in \R _+$ tel que :
  \[
  \forall n \in \N, \ |u_n| \leq M,
  \]
  donc pour tout $n$ dans $\N$, $|u_{\varphi(n)}| \leq M$, donc $(u_{\varphi(n)})_{n\in \N}$ est bornée. \\
  Par le théorème de Bolzano-Weierstrass, il existe $\psi$ une extractrice et $\ell ' \in \R$, avec $\varphi \circ \psi$ qui est aussi une extractrice par composition d'applications strictement croissantes, donc$(u_{\varphi \circ \psi (n)})_{n\in \N}$ est une sous-suite de $u$ et $\ell ' \in L(u) = \{ \ell \}$.\\
  Par ailleurs, pour tout $n$ dans $\N$ :
  \[
  \underset{\xrightarrow[n\to +\infty]{}|\ell' -\ell|}{\underbrace{|u_{\varphi \circ \psi (n)} - \ell|}} > \varepsilon,
  \]
  donc en passant à la limite dans l'inégalité on a pour tout $n$ dans $\N$, $|\ell ' - \ell | \geq \varepsilon > 0$, ce qui n'est pas possible car $\ell$ est la seule valeur d'adhérence possible et ici la différence n'est pas nulle.
\end{question_kholle}

\pagebreak\section{Semaine 12}

\begin{question_kholle}
  [Soient $a \in \K$ et $v \in \K^\N$ où \K peut être \C ou \R.
    L'ensemble des solutions de l'équation $\forall n \in \N, u_{n+1} = au_{n} + v_{n}$
    est la droite affine :
    \begin{equation}
      \left\{ w + \lambda \left( a^n \right) _{n \in \N} \mid \lambda \in \K \right\}
    \end{equation}
  ]
  {Résolution d'une relation de récurrence linéaire d'ordre 1 à coefficients constants et avec second membre}

  Posons $w$ la suite définie par $$
    \left\{ \begin{array}{ll}
      w_{0} = 1 \\
      \forall n \in \N, w_{n+1} = aw_{n} + v_{n}
    \end{array}\right.
  $$
  $w$ est "évidemment solution de particulière de l'équation"

  Maintenant que nous disposons d'une solution particulière, et ayant observé que l'équation est linéaire, mettons en œuvre l'artillerie classique pour exprimer l'ensemble des solutions par l'habituelle technique.


  \begin{align*}
    \forall n \in \N, u_{n+1} = au_{n} + v_{n} & \iff \forall n \in \N, u_{n+1} - au_{n} = v_{n}                                                         \\
                                                       & \iff \forall n \in \N, u_{n+1} - au_{n} = w_{n+1} - aw_{n}                                              \\
                                                       & \iff \forall n \in \N, (u-w)_{n+1} = a(u-w)_{n}                                                         \\
                                                       & \iff u-w \in \text{Vect}\left\{ \left( a^n \right) _{n \in \N} \right\}                                 \\
                                                       & \iff \exists \lambda \in \K : u-w = \lambda \left( a^{n} \right) _{n \in \N}                    \\
                                                       & \iff \exists \lambda \in \K : \forall n \in \N, u_{n} = w_{n} + \lambda a^n                     \\
                                                       & \iff  u \in \left\{ \left( w_{n} + \lambda a^n \right) _{n \in \N} \mid \lambda \in \K \right\}
  \end{align*}

\end{question_kholle}

\begin{question_kholle}
  [Soient $(a,b) \in \C^2$.
    L'ensemble des solutions $S_H$ de l'équation d'inconnue $u \in \C^\N$
    \begin{equation}
      \forall n \in \N, u_{n+2} = a u_{n+1} + b u_n
    \end{equation}
    est le plan vectoriel $\text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$ où $r_1$ et $r_2$ sont les racines de l'équation caractéristique ($r^2 = ar + b$) quand $\Delta \neq 0$.]
  {Résolution d'une relation de récurrence linéaire homogène d'ordre 2 à coefficients constants dans \C lorsque l'équation caractéristique possède un discriminant non nul}

  Soient $(a,b) \in \C^2$ fq.

  \underline{Lemme} Soit $r \in \C$. $\left(r^n\right)_{n\in\N}$ est solution de l'équation de récurrence si et seulement si $r^2 = ar + b$.
  \begin{equation*}
    \begin{aligned}
      (r^n)_{n\in\N} \text{ est solution }
      \iff \forall n \in \N, r^{n+2}                                                    & = a r^{n+1} + b r^n \\
      \iff \forall n \in \N, r^{n} \left( r^2 - a r - b \right)                         & = 0                 \\
      \underbrace{\iff}_{\text{ En particularisant pour } n \leftarrow 0} r^2 - a r - b & = 0                 \\
      \iff r^2                                                                          & = a r + b
    \end{aligned}
  \end{equation*}

  Considérons le cas où l'équation $r^2 = ar + b$ admet deux racines distinctes ($\Delta \neq 0$) $r_1$ et $r_2$.
  D'après le lemme, $\left(r_1^n\right)_{n\in\N}$ et $\left(r_2^n\right)_{n\in\N}$ sont solutions. Par linéarité de l'équation, toute combinaison linéaire est solution de l'équation homogène.
  Donc $\text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \} \subset S_H$.

  Réciproquement, soit $u \in \S_H$ fq.
  Étudions le système à deux inconnues $(\lambda, \mu) \in \C^2$ :
  \begin{equation*}
    \left\{ \begin{matrix}
      \lambda r_1^0 + \mu r_2^0 = u_0 \\
      \lambda r_1^1 + \mu r_2^1 = u_1
    \end{matrix} \right.
    \iff \left\{ \begin{matrix}
      \lambda + \mu = u_0 \\
      \lambda r_1 + \mu r_2 = u_1
    \end{matrix} \right.
  \end{equation*}
  $\begin{vmatrix}
      1   & 1   \\
      r_1 & r_2
    \end{vmatrix}
    = r_2 - r_1 \neq 0$
  Donc d'après les formules de Cramer, ce système admet une unique solution.

  Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ par :
  \begin{equation*}
    u_n = \lambda r_1^n + \mu r_2^n \text{ et } u_{n+1} = \lambda r_1^{n+1} + \mu r_2^{n+1}
  \end{equation*}
  \begin{itemize}
    \item $\mathcal{P}(0)$ est vrai par construction de $\lambda$ et $\mu$.
    \item Soit $n \in \N$ fq tq $\mathcal{P}(n)$ vrai.
          D'après $\mathcal{P}(n)$, $u_{n+1} = \lambda r_1^{n+1} + \mu r_2^{n+1}$.
          \begin{equation*}
            \begin{aligned}
              u_{n+2} & = a u_{n+1} + b u_n                                                                                                                      \\
                      & = a \left( \lambda r_1^{n+1} + \mu r_2^{n+1} \right) + b \left( \lambda r_1^n + \mu r_2^n  \right) \quad \text{ d'après } \mathcal{P}(n) \\
                      & = \lambda r_1^n \left(ar_1 + b\right) + \mu r_2^n \left(ar_2 + b\right)                                                                  \\
                      & = \lambda r_1^{n+2} + \mu r_2^{n+2} \quad \text{ car } r_1 \text{ et } r_2 \text{ sont racine de } r^2 = ar + b
            \end{aligned}
          \end{equation*}
  \end{itemize}
  Ainsi $S_H \subset \text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$.

  Par double inclusion, $S_H = \text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $u$ une suite bornée. $u$ converge si et seulement si il existe $\ell \in \K$ tel que $L(u)$ est le singleton $\ell$ ]
  {Caractérisation de la convergence par l'unicité d'une valeur d'adhérence pour une suite bornée.}

  Traitons le cas réel, celui sur \C est à adapter sans peine.\\
  Supposons que $u$ converge et posons $\lim u =\ell \in \R  $. Toutes les sous-suites de $u$ convergent vers $\ell$ donc $L(u)=\{\ell \}$. \\
  Supposons maintenant qu'il existe un unique $\ell \in \R$ tel que $L(u) = \{ \ell \}$. Par l'absurde, supposons que $u$ ne converge pas vers $\ell$, c'est-à-dire :
  \[
    \exists \varepsilon \in \R ^* _+ \ : \ \forall N \in \N, \ \exists n \in \N \ : \ n\geq N \text{ et } |u_n - \ell | > \varepsilon.
  \]
  Fixons un tel $\varepsilon$. \\
  %\textbf{Etape 1} : \textit{Construction d'une sous-suite de $u$ dont les termes sont $\varepsilon$-éloignés de $\ell$.} \\
  Posons $\varphi (0) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon \} }$, ce qui a du sens car c'est une partie non-vide de $\N$. Posons ensuite $\varphi (1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(0) < k \} } $, ce qui a du sens pour les mêmes raisons. On construit en itérant ce procédé $\varphi (n)$ tel que :
  \[
    \forall n \in \N, \ \varphi(n+1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(n) < k \} }.
  \]
  De cette manière, nous venons de construire une extractrice telle que :
  \[
    \forall n \in \N, \ |u_{\varphi(n)} - \ell| > \varepsilon.
  \]
  Par hypothèse $u$ est bornée, donc il existe $M\in \R _+$ tel que :
  \[
    \forall n \in \N, \ |u_n| \leq M,
  \]
  donc pour tout $n$ dans $\N$, $|u_{\varphi(n)}| \leq M$, donc $(u_{\varphi(n)})_{n\in \N}$ est bornée. \\
  Par le théorème de Bolzano-Weierstrass, il existe $\psi$ une extractrice et $\ell ' \in \R$, avec $\varphi \circ \psi$ qui est aussi une extractrice par composition d'applications strictement croissantes, donc$(u_{\varphi \circ \psi (n)})_{n\in \N}$ est une sous-suite de $u$ et $\ell ' \in L(u) = \{ \ell \}$.\\
  Par ailleurs, pour tout $n$ dans $\N$ :
  \[
    \underset{\xrightarrow[n\to +\infty]{}|\ell' -\ell|}{\underbrace{|u_{\varphi \circ \psi (n)} - \ell|}} > \varepsilon,
  \]
  donc en passant à la limite dans l'inégalité on a pour tout $n$ dans $\N$, $|\ell ' - \ell | \geq \varepsilon > 0$, ce qui n'est pas possible car $\ell$ est la seule valeur d'adhérence possible et ici la différence n'est pas nulle.
\end{question_kholle}

\begin{question_kholle}
  [Soient $f : \mathcal{D} \rightarrow \R$ et $I \subset \mathcal{D}_f$ une intervalle $f$-stable. \\
    Soit $\left(u_n\right)_{n\in\N} \in \R^\N$ la suite récurrente associée à la fonction $f$ c'est-à-dire $\forall n \in \N, u_{n+1} = f(u_n)$.
    \begin{itemize}
      \item Si $f$ est croissante sur $I$.
            \subitem Si $u_1 \geqslant u_0$ alors $u$ est croissante.
            \subitem Si $u_1 \leqslant u_0$ alors $u$ est décroissante.
      \item Si $f$ est décroissante sur $I$.
            \subitem Les sous-suites $\left(u_{2n}\right))_{n\in\N}$ et $\left(u_{2n+1}\right))_{n\in\N}$ sont monotone et ont une monotonie opposée (utiliser les premiers termes pour trouver leur monotonie respectives).
    \end{itemize}]
  {Monotonie de $u$ et des sous-suites des termes pairs et impairs de la suite $u_{n+1} = f(u_n)$ selon la monotonie de $f$}

  Soient de tels $f, I$ et $u$.
  \begin{itemize}
    \item Supposons que $f$ est croissante sur $I$. \\
          Supposons $u_1 \geqslant u_0$. Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ par
          \begin{equation*}
            \mathcal{P}(n) : " u_{n+1} \geqslant u_n "
          \end{equation*}
          \subitem Par hypothèse, $u_1 \geqslant u_0$ donc $\mathcal{P}(0)$ est vrai.
          \subitem Soit $n \in \N$ fq tq $\mathcal{P}(n)$ vrai. \\
          \begin{equation*}
            u_{n+1} \geqslant u_n
            \underbrace{\implies}_{f \text{ est croissante sur } I} f(u_{n+1}) \geqslant f(u_n)
            \implies u_{n+2} \geqslant u_{n+1}
          \end{equation*}
          Donc $\mathcal{P}(n+1)$ est vrai. \\
          Si $u_1 \leqslant u_0$, il suffit de changer $\geqslant$ par $\leqslant$ dans la récurrence ci-dessus.
    \item Supposons que $f$ est décroissante sur $I$. \\
          Donc $\forall n \in \N, u_{2(n+1)} = f \circ f(u_{2n}) \text{ et } u_{2(n+1)+1} = f \circ f(u_{2n+1})$. Or $f \circ f$ est croissante, donc $\left(u_{2n}\right)_{n\in\N}$ et $\left(u_{2n+1}\right)_{n\in\N}$ sont monotones. \\
          Supposons que $\left(u_{2n}\right)_{n\in\N}$ est croissante.
          Soit $n \in \N$ fq. Alors
          \begin{equation*}
            u_{2n} \leqslant u_{2(n+1)}
            \underbrace{\implies}_{f \text{ est décroissante sur } I} f(u_{2n}) \geqslant f(u_{2(n+1)})
            \implies u_{2n+1} \geqslant u_{2(n+1)+1}
          \end{equation*}
          Donc $\left(u_{2n+1}\right)_{n\in\N}$ est décroissante. \\
          De même, si $\left(u_{2n}\right)_{n\in\N}$ est décroissante alors $\left(u_{2n+1}\right)_{n\in\N}$ est croissante.
  \end{itemize}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      
      \draw[->] (-6, 0) -- (7, 0) node[below] {$x$};
      \draw[->] (0, -0.5) -- (0, 4.5) node[right] {$f(x) = \sqrt{2 - x}$}; 
        

      \draw[black, dashed] (0, 0) -- (4, 4);

      \draw[black] (-1.5 * 2, 0) node[below] {$u_0$} -- (-1.5 * 2, 1.870 * 2);
      \draw[black, dotted] (-1.5 * 2, 1.870 * 2) -- (1.870 * 2, 1.870 * 2);
      \draw[black, dotted] (1.870 * 2, 1.870 * 2) -- (1.870 * 2, 0);
      
      \draw[black] (1.870 * 2, 0) node[below] {$u_1$} -- (1.870 * 2, 0.3605 * 2);
      \draw[black, dotted] (1.870 * 2, 0.3605 * 2) -- (0.3605 * 2, 0.3605 * 2);
      \draw[black, dotted] (0.3605 * 2, 0.3605 * 2) -- (0.3605 * 2, 0);
      
      \draw[black] (0.3605 * 2, 0) node[below] {$u_2$} -- (0.3605 * 2, 1.2804 * 2);
      \draw[black, dotted] (0.3605 * 2, 1.2804 * 2) -- (1.2804 * 2, 1.2804 * 2);
      \draw[black, dotted] (1.2804 * 2, 1.2804 * 2) -- (1.2804 * 2, 0);
      
      \draw[black] (1.2804 * 2, 0) node[below] {$u_3$} -- (1.2804 * 2, 0.8483 * 2);
      \draw[black, dotted] (1.2804 * 2, 0.8483 * 2) -- (0.8483 * 2, 0.8483 * 2);
      \draw[black, dotted] (0.8483 * 2, 0.8483 * 2) -- (0.8483 * 2, 0);
      
      \draw[black] (0.8483 * 2, 0) node[below] {$u_4$} -- (0.8483 * 2, 1.0732 * 2);
      \draw[black, dotted] (0.8483 * 2, 1.0732 * 2) -- (1.0732 * 2, 1.0732 * 2);
      \draw[black, dotted] (1.0732 * 2, 1.0732 * 2) -- (1.0732 * 2, 0);
      
      \draw[black] (1.0732 * 2, 0) node[below] {$u_5$} -- (1.0732 * 2, 0.9627 * 2);
    %   \draw[black, dotted] (1.0732 * 2, 0.9627 * 2) -- (0.9627 * 2, 0.9627 * 2);
    %   \draw[black, dotted] (0.9627 * 2, 0.9627 * 2) -- (0.9627 * 2, 0);
      
      
      \draw[thick, smooth, domain=-4:4, samples=100] plot(\x, {2 * sqrt (2 - \x/2)});

    \end{tikzpicture}
    \caption{Construction des termes de la suite de vérifiant la relation de récurrence $u_{n+1} = \sqrt{2 - u_n}$ pour le permier terme $u_0 = - \frac 3 2$. $f: x \mapsto \sqrt{2-x}$ est une fonction décroissante, et l'on remarque effectivement que la sous suite de termes pairs $(u_0, u_2, u_4, \dots)$ croît, alors que la sous suite de termes impairs $(u_1, u_3, u_5, \dots)$ décroît.}
  \end{figure}

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      
      \draw[->] (-0.5, 0) -- (7, 0) node[below] {$x$};
      \draw[->] (0, -0.5) -- (0, 4.5) node[right] {$g(x) = x^2$}; 
        

      \draw[black, dashed] (0, 0) -- (4.3, 4.3);

      \draw[black] (0.9 * 4, 0) node[below] {$v_0$} -- (0.9 * 4, 0.81 * 4);
      \draw[black, dotted] (0.9 * 4, 0.81 * 4) -- (0.81 * 4, 0.81 * 4);
      \draw[black, dotted] (0.81 * 4, 0.81 * 4) -- (0.81 * 4, 0);
      
      \draw[black] (0.81 * 4, 0) node[below] {$v_1$} -- (0.81 * 4, 0.6561 * 4);
      \draw[black, dotted] (0.81 * 4, 0.6561 * 4) -- (0.6561 * 4, 0.6561 * 4);
      \draw[black, dotted] (0.6561 * 4, 0.6561 * 4) -- (0.6561 * 4, 0);
      
      \draw[black] (0.6561 * 4, 0) node[below] {$v_2$} -- (0.6561 * 4, 0.4305 * 4);
      \draw[black, dotted] (0.6561 * 4, 0.4305 * 4) -- (0.4305 * 4, 0.4305 * 4);
      \draw[black, dotted] (0.4305 * 4, 0.4305 * 4) -- (0.4305 * 4, 0);
      
      \draw[black] (0.4305 * 4, 0) node[below] {$v_3$} -- (0.4305 * 4, 0.1853 * 4);
      \draw[black, dotted] (0.4305 * 4, 0.1853 * 4) -- (0.1853 * 4, 0.1853 * 4);
      \draw[black, dotted] (0.1853 * 4, 0.1853 * 4) -- (0.1853 * 4, 0);
      
      \draw[black] (0.1853 * 4, 0) node[below] {$v_4$} -- (0.1853 * 4, 0.0343 * 4);
      \draw[black, dotted] (0.1853 * 4, 0.0343 * 4) -- (0.0343 * 4, 0.0343 * 4);
      \draw[black, dotted] (0.0343 * 4, 0.0343 * 4) -- (0.0343 * 4, 0);
      
      \draw[black] (0.0343 * 4, 0) node[below] {$v_5$} -- (0.0343 * 4, 0);
      
      
      \draw[thick, smooth, domain=0:4.3, samples=100] plot(\x, {(\x/2)^2});

    \end{tikzpicture}
    \caption{Construction des termes de la suite de vérifiant la relation de récurrence $v_{n+1} = v_n^2$ pour le permier terme $v_0 = \frac {9} {10}$. $g: x \mapsto x^2$ est croissante sur $[0, +\infty[$, et l'on remarque effectivement que la suite $v$ est monotone.}
  \end{figure}
\end{question_kholle}

\begin{question_kholle}
  [Montrons que : $ \overset{\circ}{\Q} = \emptyset $]
  {L'intérieur de l'ensemble des rationnels est vide.}
  Par l'absurde, supposons que $\Q$ possède au moins un point intérieur. \\ Fixons $r_0 \in \overset{\circ}{\Q}$. Par définition d'un point intérieur, il existe $\varepsilon \in \R _+ ^* $ : $]r_0 - \varepsilon , \ r_0 + \varepsilon[ \subset \Q$. Or, par densité des irrationnels dans $\R$, il existe $\alpha \in \R \backslash \Q$ : $r_0 - \varepsilon < \alpha < r_0 + \varepsilon$. On en déduit que $\alpha \in ]r_0 - \varepsilon , \ r_0 + \varepsilon[$, or $]r_0 - \varepsilon , \ r_0 + \varepsilon[ \subset \Q$ donc $\alpha \in \Q$ ce qui contredit le choix de $\alpha \in \R \backslash \Q$. Ainsi, $\overset{\circ}{\Q} = \emptyset$
\end{question_kholle}

\begin{question_kholle}
  [Soient $f,g \ : \ \mathcal{D} \to \R$, $\ell \in \R$ et $a \in \overline{\mathcal{D}}$ tels que $|f(x) - \ell| \leq g(x)$ au voisinage de $a$ et $g$ tend vers $0$ en $a$. Alors $f$ tend vers $\ell$ en $a$.]
  {Théorème sans nom version continue au voisinage de $a$}

  On traite le cas $a\in \R$. Par définition de $|f(x) - \ell| \leq g(x)$ au voisinage de $a$,
  \[
    \exists \eta \in \R _+ ^* \ : \ \forall x \in \mathcal{D}, \ |x-a| \leq \eta \ \implies \ |f(x) - \ell| \leq g(x).
  \]
  Fixons un tel $\eta$. \\
  Soit $\omega \in \R_+ ^*$. Appliquons la définition de $\lim_{x \to a} g(x) = 0$ pour $\varepsilon \gets \omega$ :
  \[
    \exists \eta' \in \R_+ ^* \ : \ \forall x \in \mathcal{D}, \ |x-a| \leq \eta' \ \implies \ |g(x)| \leq \omega.
  \]
  Fixons un tel $\eta'$. \\
  Posons $\Omega = \min{ \{\eta,  \eta' \} }$. \\
  Soit $x\in \mathcal{D}$ tel que $|x-a| \leq \Omega$.
  \[
    |f(x) - \ell | \leq g(x) \leq \omega,
  \]
  car la définition de $\Omega$ permet de remplir les conditions des deux propriétés.
\end{question_kholle}
\pagebreak\section{Semaine 13}

\begin{question_kholle}
  [Soient $g$ une fonction définie sur $\mathcal{D}_g \subset \R$ et $f$ une fonction définie sur $\mathcal{D}_f \subset \R$ telle que $f(\mathcal{D}_f) \subset \mathcal{D}_g$.
    Si $\left. \begin{array}{cc}
        g \text{ admet une limite } \ell \in \overline{\R} \text{ en } b \in \overline{\mathcal{D}_g} \\
        f \text{ admet } b \text{ comme limite en } a \in \overline{\mathcal{D}_f}
      \end{array} \right\}$
    alors $g \circ f$ admet $\ell$ comme limite en $a$.]
  {Théorème de composition des limites}

  Traitons le cas où $\ell \in \R$, $a \in \R$ et $b \in \R$. \\
  Soit $\varepsilon \in \R_+^*$ fq. \\
  Appliquons la définition de $g(y) \arrowlim{y}{b} \ell$ pour cet $\varepsilon$ :
  \begin{equation*}
    \exists \eta_g \in \R_+^* : \forall y \in \mathcal{D}_g, | y - b | \leqslant \eta_g \implies | g(y) - \ell | \leqslant \varepsilon
  \end{equation*}
  Appliquons la définition de $f(x) \arrowlim{x}{a} b$ pour cet $\eta_g$ :
  \begin{equation*}
    \exists \eta_f \in \R_+^* : \forall x \in \mathcal{D}_f, | x - a | \leqslant \eta_f \implies | f(x) - b | \leqslant \eta_g
  \end{equation*}
  Posons $\eta = \eta_f$.

  Soit $x \in \mathcal{D}_{g \circ f}$ fq tq $ | x - a | \leqslant \eta $. Or $f(\mathcal{D}_f) \subset \mathcal{D}_g$ donc $\mathcal{D}_{g \circ f} = \mathcal{D}_f$. \\
  Ainsi, $x \in \mathcal{D}_f$ et $ | x - a | \leqslant \eta_f $ d'où $ | f(x) - b | \leqslant \eta_g $ d'où $ | g(f(x)) - \ell | \leqslant \varepsilon $. Donc
  \begin{equation*}
    g \circ f \arrowlim{x}{a} \ell
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}[{
  Soit $f : \mathcal{D}_{f}\to \R$, $a \in \overline{\mathcal{D}_{f}}$ et $\ell \in \overline{\R}$
$$
f \text{ admet } \ell \text{ pour limite en } a \iff \left\{ \begin{array}{ll}
 \text{pour toute suite } u \in \mathcal{D}_{f}^{\N}, \\
\text{si } u \text{ tend vers } a, \text{ alors } f(u) \text{ tend vers } \ell
\end{array}\right.
$$
}]{Caractérisation séquentielle de la limite}
  
\begin{itemize}[label=$\star$]
  \item Supposons que $f$ admet $\ell$ pour limite en $a$. Traitons le cas $a \in \R$ et $\ell \in \R$.
  Soit $u \in \mathcal{D}_{f}^{\N}$ fixée quelconque telle que $u$ tend vers $a$.
  Soit $\varepsilon>0$ fixé quelconque. Appliquons la définition de la limite de $f$ en $a$ pour $\varepsilon$

  $$
  \exists \eta >0 : \forall x \in \mathcal{D}_{f}, \lvert x - a \rvert \leqslant \eta \implies \lvert f(x) - \ell \rvert  \leqslant \varepsilon
  $$
  Fixons un tel $\eta$ et appliquons la définition de la convergence de $u$ pour $\varepsilon \leftarrow \eta$
  $$
  \exists N \in \N : \forall n \geqslant N, \lvert u_{n} - a \rvert  \leqslant \eta
  $$
  Fixons un tel $N$.
  Soit $n \in \N$ tel que $n \geqslant N$.
  On a, bien entendu
  $$
  \lvert u_{n} - a \rvert  \leqslant \eta \implies \lvert f(u_{n}) - \ell \rvert \leqslant \varepsilon
  $$
  Ce qui montre la convergence de $(f(u_{n}))_{n \in \N}$ vers $\ell$.

  \item Réciproquement, raisonnons par contraposée et montrons l'implication suivante
  $$
  \mathrm{non}(f \text{ admet }\ell \text{ pour limite en }a) \implies \underbrace{ \mathrm{non} \left\{ \begin{array}{ll}
    \text{pour toute suite } u \in \mathcal{D}_{f}^{\N}, \\
  \text{si } u \text{ tend vers } a, \text{ alors } f(u) \text{ tend vers } \ell
  \end{array}\right. }_{ \exists u \in \mathcal{D}_{f}^{\N} : u \text{ tend vers }a \text{ et } f(u) \text{ ne tend pas vers }\ell }
  $$

  Supposons donc 
  
  \begin{align}
  \mathrm{non}(f \text{ admet }\ell \text{ pour limite en }a)  &\iff \mathrm{non}(\forall \varepsilon>0, \exists \eta >0 : \forall x \in \mathcal{D_{f}}, \lvert x -a\rvert \leqslant \eta \implies \lvert f(x) - \ell \rvert\leqslant \varepsilon  ) \\
  &\iff \exists \varepsilon>0 : \forall \eta >0, \exists x \in \mathcal{D}_{f} : \lvert x-a \rvert \leqslant \eta \text{ et } \lvert f(x) - \ell \rvert  > \varepsilon \label{S13:Q2:1}
  \end{align}
  

  Fixons donc un tel $\varepsilon$ et construisons une suite $u \in \mathcal{D}_{f}^{\N}$ telle que $u$ tend vers $a$ et $f(u)$ ne tend pas vers $\ell$.

  Soit $n \in \N$ fixé quelconque. Appliquons l'hypothèse \eqref{S13:Q2:1} pour $\eta \leftarrow \frac{1}{2^{n}}$.
  $$
  \exists x_{n} \in \mathcal{D}_{f} : \lvert x_{n} - a \rvert  \leqslant \frac{1}{2^{n}} \text{ et } \lvert f(x_{n}) - \ell \rvert  > \varepsilon
  $$
  En relâchant le caractère fixé de $n$, on a constuit une suite $(x_{n})_{n \in \N} \in \mathcal{D}_{f}^{\N}$ telle que
  $$
  \forall n \in \N, \lvert x_{n} - a \rvert \leqslant \underbrace{ \frac{1}{2^{n}} }_{ \xrightarrow[n \to \infty]{} 0}
  $$
  Donc $(x_{n})_{n \in \N}$ tend vers $a$.

  La suite vérifie aussi
  $$
  \forall n \in \N, \lvert f(x_{n}) - \ell \rvert > \varepsilon
  $$
  ce qui montre que $(f(x_{n}))_{n \in \N}$ ne converge pas vers $\ell$ car 
  
  \begin{align*}
  \text{non}((f(x_{n}))_{n \in \N} \text{ converge vers } \ell) &\iff \mathrm{non}(\forall \varepsilon_{1}>0, \exists N \in \N : \forall n \in \N, n \geqslant N \implies \lvert f(x_{n}) - \ell \rvert \leqslant \varepsilon_{1}) \\
  &\iff \exists \varepsilon_{1}>0: \forall N \in \N, \exists n  \in \N : n\geqslant N \text{ et } \lvert f(x_{n}) - \ell \rvert  > \varepsilon_{1}
  \end{align*}
  
  Ce qui est immédiat en posant $\varepsilon_{1} = \varepsilon$ et pour n'importe quel $N$ en posant $n = N$.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[]{Deux stratégies pour prouver qu'une fonction n'admet pas de limite en un point}
  \begin{itemize}[label=$\bullet$]
    \item Soit en exhibant une suite $(y_n)_{n \in \N} \in \mathcal D_f^{\N}$ qui tend vers $a$ et telle que $(f(y_n))_{n \in \N}$ n'admet pas de limite en $a$.
    
    Par exemple $f(x) = \cos \frac 1 x$ n'a pas de limite en zéro : observons que la suite $y = \left( \frac{1}{n\pi} \right)_{n \in \N^{*}}$ converge vers $0$ tandis que la suite $f(y)=((-1)^{n})_{n \in \N^{*}}$ diverge.

    \item Soit en exhibant deux suites $y, z$ qui tendent vers $a$ et telles que $(f(y_{n}))_{n \in \N}$ et $(f(z_{n}))_{n \in \N}$ admettent deux limites différentes.

    Par exemple, pour montrer que $f(x) = \sin x$ n'admet pas de limite en $+\infty$, il suffit d'observer que les suites $y = (n \pi)_{n \in \N}$ et $z = (2n\pi + \frac{\pi}{2})_{n \in \N}$ tendent vers $+\infty$ et ont respectivement pour suites images $\tilde{0}$ et $\tilde{1}$ qui convergent respectivement vers $0$ et $1$.
  \end{itemize}
  
\end{question_kholle}

\begin{question_kholle}[{
  Soient $f$ et $g$ définies sur $\mathcal{D}$ et $a \in \overline{\mathcal{D}}$. Si
  \begin{itemize}[label=$\bullet$]
    \item $f \leqslant g$ sur un voisinnage de $a$
    \item $f$ et $g$ admettent une limite finie en $a$
  \end{itemize}
alors $\displaystyle \lim_{ x \to a }f(x) \leqslant \lim_{ x \to a }g(x)$
}]{Passage à la limite dans une inégalité}
\begin{itemize}[label=$\star$]
  \item \textbf{En utilisant la caractérisation séquentielle de la limite}
  
  Traitons le cas $a = +\infty$.
  
  Posons $\ell_{f} \in\R$ et $\ell_{g} \in \R$ les limites finies respectives de $f$ et $g$.
  
  Par définition de $a \in \overline{\mathcal{D}}$, $\exists (a_{n})_{n \in \N} \in \mathcal{D}^{\N} : \lim_{ n \to \infty }a_{n} = a = +\infty$
  
  Par définition de voisinnage de $a$ en $+\infty$, $\exists A \in \R : \forall x \in \mathcal{D}, x\geqslant A \implies f(x) \leqslant g(x)$
  
  Fixons un tel $A$ et appliquons la définition de la divergence de $(a_{n})_{n \in \N}$ vers $+\infty$ pour $A$.
  $$
  \exists N \in \N : \forall n \geqslant N, a_{n} \geqslant A
  $$
  Fixons un tel $N$. On a alors
  $$
  \forall n \geqslant N, a_{n} \geqslant A \implies f(a_{n}) \leqslant g(a_{n})
  $$
  Donc par passage à la limite dans l'inégalité pour les suites
  $$
  \lim_{ n \to \infty } f(a_{n}) \leqslant \lim_{ n \to \infty } g(a_{n})
  $$
  donc par caractérisation séquentielle de la limite
  $$
  \lim_{ x \to a=+\infty}  f(x) \leqslant \lim_{ x \to a  = +\infty} g(x) 
  $$

  \item \textbf{En utilisant le caractère local de la limite}
  Tout d'abord $f \leqslant g$ sur un voisinnage de $a$, donc $g - f \geqslant 0$ sur un voisinnage de $a$ donc $g-f = \lvert g-f \rvert$ sur un voisinnage de $a$.
  Or,
  $$
  \left. \begin{array}{ll}
  \lim_{ x \to a }g(x) = \ell_{g} \\
  \lim_{ x \to a } f(x) = \ell_{f}
  \end{array}\right\}
  \implies \lim_{ x \to a } \lvert g(x) - f(x) \rvert  = \lvert \ell_{g} - \ell _{f} \rvert 
  $$
  Donc, avec le caractère local de la limite, puisque $g-f$ et $|g-f|$ coïncident sur un voisinnage de $a$,
  $$
  \lim_{ x \to a } g(x) - f(x) = \lim_{ x \to a } \lvert g(x) - f(x) \rvert  = \lvert \ell_{g} - \ell_{f} \rvert 
  $$
  Or,  on a aussi $\lim_{ x \to a }g(x) - f(x) = \ell_{g} - \ell_{f}$.
  Donc
  $$
  \ell_{g} - \ell_{f} = \lvert \ell_{g} - \ell_{f} \rvert \geqslant 0 \implies \ell_{g} \geqslant \ell_{f}
  $$
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[{
  Soit $f$ une fonction croissante définie sur $]a, b[$ avec $(a, b) \in \overline{\R}^{2}, a<b$.
  \begin{itemize}[label=$\bullet$]
    \item Si $f$ est majorée, alors $f$ admet une limite finie en $b$ qui vaut $\lim_{ x \to b }f(x) = \sup f(]a, b[)$.
    \item Si $f$ n'est pas majorée, alors $f$ tend vers $+\infty$ en $b$.
  \end{itemize}
}]{Limite de fonctions monotones sur un segment.}
  \begin{itemize}[label=$\star$]
    \item Supposons que $f$ est majorée sur $]a, b[$. L'ensemble $f(]a, b[)$ est une partie de $\R$, non vide et majorée, donc admet une borne supérieure $S \in \R$.
    Montrons que $\lim_{ x \to b }f(x) = S$.
    
    Soit $\varepsilon>0$ fixé quelconque. On veut construire un $\eta>0$ tel que $\forall x \in ]b-\eta, b[$, $\lvert f(x) - S \rvert \leqslant \varepsilon$.
    D'après la caractérisation de la borne supérieure par les epsilon appliquée pour $\varepsilon$, $$\exists y_{\varepsilon} \in f(]a, b[) : S - \varepsilon < y_{\varepsilon} \leqslant \varepsilon$$
    Or, $y_{\varepsilon} \in f(]a, b[) \implies \exists x_{\varepsilon}\in ]a, b[ : y_{\varepsilon} = f(x_{\varepsilon})$
    Posons $\eta = b - x_{\varepsilon} > 0$ et vérifions qu'il convient.
    Soit $x \in ]b - \eta, b[$ fixé quelconque.
    on a $$
    b-\eta < x \implies b- (b - x_{\varepsilon})< x \implies x_{\varepsilon} < x \implies \underbrace{ f(x_{\varepsilon}) }_{ y_{\varepsilon} } \leqslant f(x)
    $$
    De plus, $f(x) \leqslant S$ par définition de la borne supérieure, donc
    $$
    S - \varepsilon < y_{\varepsilon} \leqslant f(x) \leqslant S
    $$
    Donc $\lvert f(x) - S \rvert \leqslant \varepsilon$ ce qui prouve la convergence.

    \item Supposons que $f$ n'est pas majorée sur $]a, b[$.
    On veut montrer que $f$ tend vers $+\infty$, autrement dit que
    $$
    \forall A \in \R, \exists \eta >0 : \forall x \in ]b- \eta , b[, f(x)\geqslant A
    $$
    Soit $A \in \R$ fixé quelconque.
    $f$ n'est pas majorée, donc $\exists x_{0} \in ]a, b[ : f(x_{0}) \geqslant A$.
    Posons $\eta = b - x_{0} > 0$
    Soit $x \in ]b-\eta, b[$ fixé quelconque.
    $$
    b-\eta< x \implies b-(b-x_{0})< x \implies x_{0}<x \implies f(x_{0}) \leqslant f(x)
    $$
    Donc $f(x) \geqslant f(x_{0})\geqslant A$
    Donc $\forall x \in ]b-\eta, b[, f(x) \geqslant A$.
    Donc $f$ tend vers $+\infty$ en $b$.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}
  [{ Soit une fonction continue $f : [a;b] \rightarrow \R$ avec $(a,b) \in \R^2$ et $a < b$. \\
  Si $f(a)f(b) \leqslant 0$ alors $\exists c \in [a;b] : f(c) = 0$. \\
  On rencontre aussi : \textit{Si $\mathit{f(a)f(b) < 0}$ alors $\mathit{\exists c \in ]a;b[ : f(c) = 0}$.} }]
  {Théorème des valeurs intermédiaires}

  La démonstration repose sur la technique de la dichotomie.

  \begin{figure}[!h]
    %					\centering
    \tikzmath{ \labTVI = 12; } % la longueur du segment [a;b] dans la démonstration du TVI
    \begin{tikzpicture}
      \draw (0,0) node[anchor=north] {a} -- (\labTVI,0) node[anchor=north] {b};
      \foreach \x in {0,...,\labTVI} {
          \draw (\x,0.1) -- (\x,-0.1);
        };
      \draw[red] (0,0) to[out angle=80, in angle=240, curve through={(\labTVI/5,2) (\labTVI/3,-1) (\labTVI*3/5,0.5)}] (\labTVI,0);

      \draw[green] (\labTVI/2,0.1) -- (\labTVI/2,-0.1);
      \draw (\labTVI/2,0) node[green, anchor=north] {$b_1$};
      \draw[green] (\labTVI/4,0.1) -- (\labTVI/4,-0.1);
      \draw (\labTVI/4,0) node[green, anchor=north] {$a_2$};
      \draw[green] (\labTVI*3/8,0.1) -- (\labTVI*3/8,-0.1);
      \draw (\labTVI*3/8,0) node[green, anchor=north] {$b_3$};
      \draw[green] (\labTVI*5/16,0.1) -- (\labTVI*5/16,-0.1);
      \draw (\labTVI*5/16,0) node[green, anchor=north] {$b_4$};
    \end{tikzpicture}
  \end{figure}

  \noindent Soient $a,b,f$ de tels objets. Procédons à la construction des suites $(a_n)_{n\in\N}, (b_n)_{n\in\N}, (c_n)_{n\in\N}$.

  Posons $a_0 = a$, $b_0 = b$ et $c_0 = \frac{a+b}{2}$ (le milieu du segment $[a;b]$). Nous avons, par hypothèse $f(a_0)f(b_0) \leqslant 0$.

  Soit $n \in \N$ fq. Supposons les trois suites construites au rang $n$ telles que $f(a_n)f(b_n) \leqslant 0$ et $c_n = \frac{a_n+b_n}{2}$ (milieu de $[a_n;b_n]$).
  \begin{itemize}
    \item Si $f(a_n)f(b_n) \leqslant 0$, posons $\left| \begin{array}{lcl}
              a_{n+1} & = & a_n                       \\
              b_{n+1} & = & c_n                       \\
              c_{n+1} & = & \frac{a_{n+1}+b_{n+1}}{2}
            \end{array} \right.$
    \item Sinon $f(a_n)f(b_n) > 0$. Or $f(a_n)f(b_n) \leqslant 0$, donc $f(a_n)^2 f(b_n) f(c_n) \leqslant 0$. Donc $f(b_n)f(c_n) \leqslant 0$. Posons $\left| \begin{array}{lcl}
              a_{n+1} & = & c_n                       \\
              b_{n+1} & = & b_n                       \\
              c_{n+1} & = & \frac{a_{n+1}+b_{n+1}}{2}
            \end{array} \right.$
  \end{itemize}
  Ainsi, nous avons bien construits $a_{n+1}, b_{n+1}, c_{n+1}$ telles que $f(a_{n+1})f(b_{n+1}) \leqslant 0$ et ${ c_{n+1} = \frac{a_{n+1}+b_{n+1}}{2} }$ (milieu de $[a_{n+1};b_{n+1}]$).

  Par récurrence immédiate, $(a_n)_{n\in\N}$ est croissante, $(b_n)_{n\in\N}$ est décroissante et ${ \forall n \in \N, b_n - a_n = \frac{b-a}{2^n} }$ d'où $b_n - a_n \arrowlim{n}{+\infty} 0$.
  Donc les suites $a$ et $b$ sont adjacentes.
  D'après le théorème des suites adjacentes, elles convergent vers la même limite. Notons la $c$.

  D'après le bonus de ce même théorème, $\forall n \in \N, a_n \leqslant c \leqslant b_n$ donc pour $n = 0$, $a \leqslant c \leqslant b$. Ainsi,
  \begin{equation*}
    c \in [a;b]
  \end{equation*}

  Par ailleurs, $\forall n \in \N, f(a_n)f(b_n) \leqslant 0$. Par continuité de $f$ sur $[a;b]$ donc en $c$, $f(a_n) \arrowlim{n}{+\infty} f(c)$ et $f(b_n) \arrowlim{n}{+\infty} f(c)$. Par passage à limite dans l'inégalité,
  \begin{equation*}
    f(c) \times f(c) \leqslant 0
  \end{equation*}
  Or $f(c)^2 \geqslant 0$, d'où $f(c)^2 = 0$. Ainsi,
  \begin{equation*}
    f(c) = 0
  \end{equation*}
  Donc $c$ est un point fixe.

\end{question_kholle}

\begin{question_kholle}
  [{L'image d'un segment par une fonction continue sur ce segment est un segment : soient $(a, b) \in \R^2$ tels que $a < b$ et $f: [a, b] \to \R$. Si $f \in \mathcal{C}^0([a, b], \R)$ alors $\exists (x_{1}, x_{2}) \in \R^2 : f([a, b]) = [f(x_{1}), f(x_{2})]$}]
  {Théorème de Weierstraß}
  \begin{itemize}

    \item \emph{Étape 1} Montrons que $f([a, b])$ est majoré.

          Par l'absurde, supposons que $f([a, b])$ n'est pas majoré

          Alors \begin{equation}\label{eq:1}
            \forall A \in \R, \exists x \in [a, b] : f(x) > A
          \end{equation}

          Soit $n \in \N$ fq.
          Appliquons (\ref{eq:1}) pour $A \leftarrow n$:
          $\exists x \in [a, b] : f(x) > n$, et fixons un tel $x$ que l'on note $x_{n}$
          Nous venons de créer la suite $(x_{n})_{n \in \N} \in [a, b]^{\N}$ qui vérifie:


          $$
            \left.
            \begin{array}{ll}
              \forall n \in \N, f(x_{n}) \geqslant n \\
              \lim_{ n \to \infty } n =  +\infty
            \end{array}
            \right\} \underbrace{ \implies }_{ \text{théorème de divergence par minoration} } f(x_{n}) \xrightarrow[n \to +\infty]{} + \infty
          $$


          $(x_{n})_{n \in \N}$ est bornée (à valeurs dans $[a, b]$) donc, selon le théorème de Bolzanno-Weierstraß:
          $$
            \exists \ell \in \R : \exists \varphi : \N \to \N : \text{strict. croissante tel que } (x_{\varphi(n)})_{n \in \N} \text{ tend vers } \ell
          $$
          Donc, en passant à la limite : $\forall n \in \N, a \leqslant x_{\varphi(n)} \leqslant b \implies a \leqslant \ell \leqslant b \implies \ell \in [a, b]$

          Par continuité de $f$ sur $[a, b]$, donc en $\ell$, $(f(x_{\varphi(n)}))_{n \in \N}$ converge vers $f(\ell)$.

          Or $$
            \left\{ \begin{array}{ll}
              (f(x_{\varphi(n)}))_{n \in \N} \text{ est une sous suite de } (f(x_{n}))_{n \in \N} \\
              f(x_{n}) \xrightarrow[n \to + \infty]{} + \infty
            \end{array}\right.$$

          donc $(f(x_{\varphi(n)}))_{n \in \N}$, tend vers $+ \infty$, ce qui est absurde, donc $f$ est majorée.

          On fait de même pour la minoration.

    \item  \emph{Étape 2:} Montrons que $f([a, b])$ admet un pge et un ppe.

          Montrons donc que $f([a, b])$ admet une borne sup, qui, puisque c'est une valeur atteinte, deviendra un max.

          $$
            f([a, b]) \text{ est } \left\{ \begin{array}{ll}
              \text{ une partie de } \R   \\
              \text{ non vide car contient } f(a) \\
              \text{majorée d'après l'étape 1}
            \end{array}\right.
          $$

          $f([a, b])$ admet donc une borne supérieure $\sigma$.

          Appliquons la caractérisation séquentielle de la borne supérieure:
          $$
            \exists (y_{n})_{n \in \N}, \in f([a, b])^{\N} : (y_{n}) \text{ converge vers } \sigma
          $$
          $$
            \forall n \in \N, y_{n} \in f([a, b]) \implies \exists x_{n} \in [a, b] : f(x_{n} ) = y_{n}
          $$
          Fixons un tel $x_{n}$ pour tout $y_{n}$.
          On a donc construit $(x_{n})_{n \in \N} \in [a, b]^{\N} : f(x_{n}) \xrightarrow[n \to +\infty]{} \sigma$

          De plus, $(x_{n})$ est bornée (à valeurs dans $[a, b]$) donc, selon le théorème de Bolzanno-Weierstraß:
          $$
            \exists \ell \in \R : \exists \varphi : \N \to \N : \text{strict. croissante tel que } (x_{\varphi(n)})_{n \in \N} \text{ tend vers } \ell
          $$
          Donc, en passant à la limite : $\forall n \in \N, a \leqslant x_{\varphi(n)} \leqslant b \implies a \leqslant \ell \leqslant b \implies \ell \in [a, b]$


          Par continuité de $f$ sur $[a, b]$, donc en $\ell$, $(f(x_{\varphi(n)}))_{n \in \N}$ converge vers $f(\ell)$.

          Or,
          $$
            \left\{ \begin{array}{ll}
              (f(x_{\varphi(n)}))_{n \in \N} \text{ est une sous suite de } (f(x_{n}))_{n \in \N} \\
              f(x_{n}) \xrightarrow[n \to + \infty]{} \sigma
            \end{array}\right.$$

          Par unicité de la limite, $\sigma = f(\ell)$.

          On montre de même qu'il existe $\ell' \in [a, b]: f(\ell') = \inf f([a, b])$

          Ainsi, $f(\ell) = \max f([a, b])$ et $f(\ell') = \min f([a, b])$



    \item \emph{Étape 3:}
          Montrons que $f([a, b]) = [f(\ell'), f(\ell)]$.

          Par la construction précédente, $\forall y \in f([a, b]), y \in [f(\ell'), f(\ell)]$.

          Ainsi, $f([a, b]) \subset [f(\ell'), f(\ell)]$.

          Réciproquement, l'image par la fonction continue $f$ du segment $[a, b]$ qui est un intervalle est un intervalle:

          $$
            \left.
            \begin{array}{ll}
              f([a,b]) \text{ est un intevalle} \\
              f(\ell) \in f([a, b])             \\
              f(\ell') \in f([a, b])
            \end{array}
            \right\}
            \implies
            [f(\ell'), f(\ell)] \subset f([a,b])
          $$

          D'où $[f(\ell'), f(\ell)] = f([a,b])$

  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 14}

\begin{question_kholle}
  [Soit $f \left| \begin{matrix}
        \R_+^* \rightarrow \R \\
        x \mapsto \frac{\ln x}{x}
      \end{matrix} \right. $. \\
    Exprimer $f^{(n)}$ pour tout $n \in \N$.]
  {Expression de dérivées successives}
  Soit $x\in \mathcal{D}_f$. \\
  Considérons le prédicat $P(\cdot)$ définit pour $n\in \N$ par :
  $$ P(n) \ : \ \text{\textquotedblleft} \ f^{(n)}(x) = \frac{(-1)^n n!}{x^{n+1}}\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right] \ " $$
  Initialisation : \\
  Pour $n = 0$, $$f^{(0)}(x) = f(x) = \frac{\ln (x)}{x} = \frac{(-1)^0 0!}{x^{0+1}}\left[ \ln (x) - \sum_{k=1}^0 \frac{1}{k} \right],$$ donc $P(0)$ est vrai. \\
  Hérédité :
  \\
  Soit $n\in \N$ tel que $P(n)$. On a,
  $$f^{(n+1)}(x) = (f^{(n)}(x))' = \left( \frac{(-1)^n n!}{x^{n+1}}\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right] \right)'$$
  par véracité de $P(n)$. Ainsi,
  $$\begin{array}{rcl}
      f^{(n+1)}(x) & = & \frac{(-1)^nn!x^n - (-1)^n(n+1)!x^n\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right]}{x^{2(n+1)}} \\ [1ex]
                   & = & \frac{(-1)^{n+1}(n+1)!\ln (x) - (-1)^{n+1}(n+1)!\sum_{k=1}^{n+1} \frac{1}{k} }{x^{n+2}}           \\ [1ex]
                   & = & \frac{(-1)^{n+1} (n+1)!}{x^{n+2}}\left[ \ln (x) - \sum_{k=1}^{n+1} \frac{1}{k} \right]
    \end{array}$$
  c'est l'expression recherchée, donc $P(n+1)$ est vrai. \\
  Par théorème de récurrence sur $\N$, $P(n)$ est vraie pour tout $n\in \N$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $f : I \rightarrow f(I) \subset \R$ continue, strictement monotone sur $I$ et dérivable en $a \in I$.
    Si $f'(a) \neq 0$ alors $f$ est bijective, $f^{-1}$ est dérivable en $f(a)$ et $f^{-1}(f(a)) = \frac{1}{f'(a)}$.]
  {Dérivé d'une bijection réciproque}
  Soient de tels objets. \\
  Rappelons le lemme inattendu. Soit $g : J \rightarrow \R$ monotone (où $J$ est un intervalle). Nous avons l'équivalence suivante :
  \begin{equation*}
    f(J) \text{ est un intervalle } \iff f \text{ est continue sur } J
  \end{equation*}
  Par définition, $f$ est surjective. Comme elle est strictement monotone, $f$ est injective. Ainsi $f$ est bijective. \\
  D'après le lemme inattendu, $f(I)$ est un intervalle. Nous avons $f^{-1} : f(I) \rightarrow I$ avec $f(I)$ et $I$ des intervalles donc $f^{-1}$ est continue sur $f(I)$. \\
  Calculons la limite du taux d'accroissement de $f^{-1}$ en $f(a)$ :
  \begin{equation*}
    \forall x \in f(I), \tau_{f^{-1},f(a)} = \frac{f^{-1}(x) - f^{-1}(f(a))}{x - f(a)}
  \end{equation*}
  Posons $u = f^{-1}(x)$. D'où :
  \begin{equation*}
    \tau_{f^{-1},f(a)} = \frac{u - a}{f(u) - f(a)}
  \end{equation*}
  De plus, par continuité de $f^{-1}$, $u \arrowlim{x}{f(a)} f^{-1}(f(a)) = a$.
  Par dérivabilité en a et par continuité de $x \mapsto x^{-1}$ en $f(a) \neq 0$, $\frac{u - a}{f(u) - f(a)} \arrowlim{u}{a} \frac{1}{f('(a)}$. \\
  Ainsi, $f^{-1}$ est dérivable en $f(a)$ et $f^{-1}(f(a)) = \frac{1}{f'(a)}$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $f : I \rightarrow \R$. Si $f$ admet un extremum local en $a \in \overset{\circ}{I}$ et si $f$ est dérivable en $a$ alors $f'(a) = 0$
    {\begin{figure}[!h]
          \centering
          \tikzmath{ integer \m; real \fm; \m = 3; \fm = 0.8; }
          \begin{tikzpicture}
            \draw[-stealth] (0,0) -- (6,0) node[anchor=west] {$x$};
            \draw[-stealth] (0,0) -- (0,3) node[anchor=south] {$y$};

            \draw[red] plot [smooth] coordinates {(0,2.5) (\m,\fm) (6,1.7)};
            \draw[blue, dashed] (\m,\fm) -- (\m,0) node[anchor=north] {minimum local};

            \draw[stealth-stealth, teal] (\m-1,\fm) -- (\m+1,\fm);
            \draw (\m,\fm) node[anchor=north east, teal] {$m$} node[anchor=south, teal] {$f'(m)=0$};
          \end{tikzpicture}
        \end{figure}}]
  {Dérivée d'un extremum local intérieur au domaine de définition}
  Soient de tels objets. \\
  $a \in \overset{\circ}{I} \implies \exists \eta_1 \in \R_+^* : [a-\eta_1;a+\eta_1] \subset I$ Fixons un tel $\eta_1$. \\
  Calculons le taux d'accroissement en $a$.
  \begin{equation*}
    \forall x \in [a-\eta_1;a+\eta_1], \tau_{f,a}(x) = \frac{f(x) - f(a)}{x - a}`
  \end{equation*}
  Or $f$ est dérivable en $a$ donc $\tau_{f,a}(x)$ admet une limite lorsque $x \rightarrow a$.
  Traitons le cas où $a$ est maximum local. Par définition :
  \begin{equation*}
    \exists \eta_2 \in \R_+^* : \forall x \in [a-\eta_2;a+\eta_2], f(x) \leqslant f(a)
  \end{equation*}
  Fixons un tel $\eta_2$. Soit $x \in [a-\eta_2;a+\eta_2] \setminus \{a\}$ fq. \\
  Alors $f(x) - f(a) \leqslant 0$. \\
  Si $x > a$, $x - a > 0$. Alors $\frac{f(x) - f(a)}{x - a} \leqslant 0$. Donc $\textlim{x}{a} \tau_{f,a}(x) \leqslant 0$. \\
  Sinon $x < a$, $x - a < 0$. Alors $\frac{f(x) - f(a)}{x - a} \geqslant 0$. Donc $\textlim{x}{a} \tau_{f,a}(x) \geqslant 0$. \\
  Ainsi $0 \leqslant \textlim{x}{a} \tau_{f,a}(x) \leqslant 0$. Donc $f'a) = 0$.
\end{question_kholle}

\begin{question_kholle}
  [Soient $(a,b)\in \R ^2$ tels que $a<b$. Soit $I$ le segment $a,b$.
  \\
  Soit $f \ : \ I \ \to \ \R $ continue sur ledit segment et dérivable sur l'ouvert associé.\\
  {\begin{enumerate}[label=($\roman*$)]
    \item Théroème de Rolle : \\
          Si $f(a) = f(b)$, alors $\exists \ c \in \overset{\circ}{I}$ tel que $f'(c) =0$
          \begin{figure}[!h]
            \centering
            \tikzmath{ integer \a \b \f \c \fc; \a = 1; \b = 9; \f = 2; \c = \a + 3; \fc = \f + 2; }
            \begin{tikzpicture}
              \draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
              \draw[-stealth] (0,0) -- (0,5) node[anchor=south] {$y$};

              \coordinate (A) at (\a,\f);
              \coordinate (B) at (\b,\f);
              \coordinate (C) at (\c,\fc);

              %\draw[red] (A) to[out angle=80, in angle=240, curve through={(C) (6,1.5)}] (B);
              \draw [red] plot [smooth] coordinates {(A) (C) (6,1.5) (B)};
              \draw[blue, thick] (A) -- (B);
              \draw[blue, dashed] (A) -- (\a,0) node[anchor=north] {$a$};
              \draw[blue, dashed] (B) -- (\b,0) node[anchor=north] {$b$};
              \draw[blue, dashed] (A) -- (0,\f) node[anchor=east] {$f(a)=f(b)$};

              \draw[stealth-stealth, teal] (\c-2,\fc) -- (\c+2,\fc);
              \draw (C) node[anchor=north, teal] {$c$} node[anchor=south, teal] {$f'(c)=0$};
            \end{tikzpicture}
            \caption{Théorème de Rolle}
          \end{figure}

    \item Formule des accroissements finis : \\
          $\exists \ c \in \overset{\circ}{I} \ : \ f'(c) = \frac{f(b)-f(a)}{b-a}.$
          \begin{figure}[!h]
            \centering
            \tikzmath{integer \a \fa \b \fb \c \fc; \a = 1; \fa = 1; let \b = 9; \fb = 5; \c = \a + 5; \fc = \fa + 1; }
            \begin{tikzpicture}
              \draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
              \draw[-stealth] (0,0) -- (0,6) node[anchor=south] {$y$};

              \coordinate (A) at (\a,\fa);
              \coordinate (B) at (\b,\fb);
              \coordinate (C) at (\c,\fc);

              %\draw[red] (A) to[out angle=80, in angle=240, curve through={(C) (6,1.5)}] (B);
              \draw [red] plot [smooth] coordinates {(A) (3,4) (C) (7,4.5) (B)};
              \draw[blue, thick] (A) -- (B);
              \draw[blue, dashed] (A) -- (\a,0) node[anchor=north] {$a$};
              \draw[blue, dashed] (A) -- (0,\fa) node[anchor=east] {$f(a)$};
              \draw[blue, dashed] (B) -- (\b,0) node[anchor=north] {$b$};
              \draw[blue, dashed] (B) -- (0,\fb) node[anchor=east] {$f(b)$};

              \draw[stealth-stealth, teal] (\c-2,\fc-1) -- (\c+2,\fc+1);
              \draw (C) node[anchor=south, teal] {$c$} node[anchor=north west, teal] {$f'(c)=\frac{f(b)-f(a)}{b-a}$};
            \end{tikzpicture}
            \caption{Formule des accroissements finis}
          \end{figure}
  \end{enumerate}}
  ]
  {Théorème de Rolle et formule des accroissements finis}
  Soient de tels objets. \\
  Prouvons $(i)$, donc supposons $f(a) = f(b)$. \\
  $f$ est continue sur $I$ donc par le théorème de Weierstraß, elle est bornée et atteint ses bornes sur ce segment :
  \\
  $$\exists \ (x_m, x_M)\in I^2 \ : \ (f(x_m) = \min f(I)) \wedge (f(x_M) = \max f(I))$$
  donc, si $(x_m, x_M)\in \{a,b\}^2$, alors,
  $$\forall x \in I, \ f(a)=f(x_m) \leq f(x) \leq f(x_M)=f(a)$$
  donc $\forall x \in I, f(x) = f(a)$ c'est-à-dire que $f$ est constante et donc tous les points intermédiaires à $I$ sont des $c$ valides.\\
  Sinon, $(x_m \notin \{a,b\}) \vee (x_M \notin \{a,b\}) $, quitte à prendre l'autre valeur, supposons que $x_M \notin \{a,b\}$, ainsi, $x_M \in \overset{\circ}{I}$ et $f(x_M)$ est un maximum global donc, $f$ étant dérivable sur $\overset{\circ}{I}$ elle est dérivable en $x_M$ donc $f'(x_M)=0$, on pose $c = x_M$, ce qui conclut. \\ \\
  Prouvons $(ii)$.\\
  Posons $d \ :\ I \ \to \ \R, \ x \ \mapsto \ f(x) - \left( \frac{f(b)-f(a)}{b-a}(x-a) +f(a) \right) $. $d$ est continue sur $I$ et dérivable sur $\overset{\circ}{I}$ comme combinaison linéaire de telles fonctions. On a $d(a) = 0$ et $d(b) = 0$ donc $d(a) = 0 = d(b)$. On peut alors appliquer le Théorème de Rolle pour $f \gets d, a \gets a $ et $ b \gets b$ : il existe $c\in \overset{\circ}{I}$ tel que $d'(c) = 0$, c'est le résultat.
\end{question_kholle}

\begin{question_kholle}
  [Soit $f\in \mathcal{C}^0(I, \R) \cap \mathcal{D}^1(\overset{\circ}{I}, \R)$ et $x_0 \in I$, posons {$X_- = ]-\infty;x_0]$} la demi-droite fermée en $x_0$ et vers $-\infty$, de même {$X_+ = [x_0;+\infty[$} la demi-droite fermée en $x_0$ et vers $+ \infty$. \\ \\
  {\begin{enumerate}[label=$(\roman*)$]
    \item \begin{itemize}[label=$\star$, leftmargin=0.4cm]
            \item Si $\exists \ m \in \R \ : \ \forall x \in \overset{\circ}{I}, \ m\leq f'(x)$, alors, $$\forall x \in I \cap X_+, \ f(x_0) + m(x-x_0) \leq f(x)$$ et $$\forall x \in I \cap X_-, \ f(x) \leq f(x_0) + m(x-x_0)$$
            \item Si $\exists \ M \in \R \ : \ \forall x \in \overset{\circ}{I}, \ f'(x) \leq M $, alors, $$\forall x \in I \cap X_+, \ f(x) \leq f(x_0) + M(x-x_0)$$ et $$\forall x \in I \cap X_-, \ f(x_0) + M(x-x_0) \leq f(x) $$
            \item Si $\exists \ (m,M) \in \R^2 \ : \ \forall x \in \overset{\circ}{I}, \ m\leq f'(x) \leq M$, alors, $$\forall x \in I \cap X_+, \ f(x_0) + m(x-x_0) \leq f(x) \leq f(x_0) +M(x-x_0)$$ et $$\forall x \in I \cap X_-, \ f(x_0) + M(x-x_0) \leq f(x) \leq f(x_0) +m(x-x_0)$$
          \end{itemize}
    \item Si $\exists M \in \R \ : \ \forall x \in \overset{\circ}{I}, \ |f'(x)|\leq M$, alors, $$\forall (x,y) \in I^2, \ |f(y) -f(x)| \leq M|y-x|$$
  \end{enumerate}}

  {\begin{figure}[!h]
    \centering
    \tikzmath{ integer \x0 \fx0 \m \M; \x0 = 5;\fx0 = 2.5; \m = 0.2; \M = 0.7; }
    \begin{tikzpicture}
      \draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
      \draw[-stealth] (0,0) -- (0,5) node[anchor=south] {$y$};

      \coordinate (A) at (1,\fx0-\m*\x0+\m*1);
      \coordinate (B) at (9,\fx0-\m*\x0+\m*9);
      \coordinate (C) at (1,\fx0-\M*\x0+\M*1);
      \coordinate (D) at (9,\fx0-\M*\x0+\M*9);
      \draw[ultra thick, black] (A) -- (B) node[anchor=north west] {$y = f(x_0) + m(x -x_0)$};
      \draw[ultra thick, black] (C) -- (D) node[anchor=south east] {$y = f(x_0) + M(x -x_0)$};
      \filldraw[magenta!30] (A) -- (B) -- (D) -- (C) -- cycle;

      \draw (\x0,\fx0) node[cross out, minimum size=8*\pgflinewidth, inner sep=0pt, outer sep=0pt, draw=blue, rotate=45, thick] {};
      \draw[blue, dashed] (\x0,\fx0) -- (0,\fx0) node[anchor=east] {$f(x_0)$};
      \draw[blue, dashed] (\x0,\fx0) -- (\x0,0) node[anchor=north] {$x_0$};
    \end{tikzpicture}
    \caption{Interprétation géométrique des accroissements finis}
  \end{figure}}]
  {Inégalité des accroissements finis}
  $(i)$ Soit $x\in I$ et posons $S$ le segment d'extrémités $x$ et $x_0$. \\
  $\star$ Si $x\neq x_0$, $f$ est continue sur $S$ et dérivable sur $\overset{\circ}{S}$, la formule des accroissements finis donne alors l'existence d'un $c$ appartenant à $\overset{\circ}{S}$ tel que $$f(x) - f(x_0) = (x-x_0)f'(c)$$ Si $x> x_0, \ x-x_0 > 0$, or $m \leq f'(c) \leq M$ donc $$m(x-x_0) \leq (x-x_0)f'(c) \leq M(x-x_0)$$ si bien que $$m(x-x_0) \leq f(x) - f(x_0) \leq M(x-x_0) $$ d'où $$f(x_0) + m(x-x_0) \leq f(x) \leq f(x_0) + M(x-x_0). $$ Si $x<x_0$, il suffit de retourner l'inégalité lors de la première multiplication et $(i)$ est prouvé.\\ \\
  $(ii)$ Soit $y \in I.$\\
  L'hypothèse $\forall x\in \overset{\circ}{I}, \ |f'(x)|\leq M$ équivaut à $\forall x \in \overset{\circ}{I}, \ -M \leq f'(x) \leq M$, donc on peut appliquer $(i)$ pour $x_0 \gets y, \ M \gets M$ et $m\gets -M$ : $$\forall x \in I\cap [y, +\infty [,  \ f(y) -M(x-y) \leq f(x) \leq f(y) + M(x-y)$$
          Or $x-y >0$ donc $|f(x) -f(y) | \leq M|x-y|$.
          Et $$\forall x \in I\cap ]-\infty, y ],  \ f(y) +M(x-y) \leq f(x) \leq f(y) - M(x-y)$$
  Or $x-y < 0$ donc $|f(x) -f(y) | \leq M|x-y|$. \\
  Par conséquent, $\forall (x,y)\in I^2, \ |f(y) -f(x)| \leq M|y-x|.$
\end{question_kholle}

\begin{question_kholle}
  [Soit $f\in \mathcal{C}^1(I,\R)$, $I$ le segment $a,b$. Alors $f$ est $||f'||_{\infty,I}$-lipschitzienne sur I.]
  {Caractère lipschitzien d'une fonction $\mathcal{C}^1$ sur un segment}
  Soient de tels objets. \\
  $\star$ $f\in \mathcal{C}^1(I, \R)$ donc $f\in \mathcal{C}^0(I, \R)$. \\
  $\star$ $f\in \mathcal{C}^1(I,\R)$ donc $f\in \mathcal{D}^1(\overset{\circ}{I}, \R)$.\\
  $\star$ $f\in \mathcal{C}^1(I,\R)$ donc $f'$ est continue sur $I$ donc le réel $||f'||_{\infty,I}$ est bien défini et $$\forall  x \in \overset{\circ}{I}, \ |f'(x)| \leq ||f'||_{\infty,I}.$$ Ces propriétés permettent d'appliquer le corollaire du TAF qui conclut que $f$ est $||f'||_{\infty,I}$-lipschitzienne.
\end{question_kholle}

\begin{question_kholle}
  [Soit $f\in \mathcal{F}(I, \R)$ et $a\in I$.\\
    \newline
    \textit{Lemme} : \\
    Si
    $\left\{ \begin{array}{cl}
        f \text{ est dérivable sur } I\backslash \{a\} \\
        f \text{ est continue en }a                    \\
        f'_{|I\backslash \{a\}} \text{ admet une limite $\ell \in \overline{\R}$ en }a
      \end{array}
      \right.$, alors $\lim_{x \to a} \frac{f(x) -f(a)}{x-a} = \ell$\\
    \newline
    \textit{Théorème} : \\
    Si
    $\left\{ \begin{array}{cl}
        f \text{ est dérivable sur } I\backslash \{a\} \\
        f \text{ est continue en }a                    \\
        f'_{|I\backslash \{a\}} \text{ admet une limite \textbf{finie} $\ell \in \R$ en }a
      \end{array}
      \right.$, alors $\left\{ \begin{array}{cl}
        f \text{ est dérivable en } a \\
        f'(a) = \ell \ (\textbf{donc } f' \textbf{ est continue en } a)
      \end{array}
      \right.$ ]
  {Théorème du prolongement de la propriété de la dérivabilité}
  Prouvons le lemme pour $\ell\in \R$, c'est le cas qui nous intéresse. \\
  Soient de tels objets. Soit $\varepsilon\in \R_+^*$. Appliquons la définition de $\lim_{\substack{x\to a \\ x\neq a}}  f'_{|I\backslash \{a\}}(x) =\ell $ pour $\varepsilon \gets \varepsilon$ :
  $$\exists \ \eta \in \R_+^* \ : \ \forall x\in I\backslash \{a\}, \ |x-a| \leq \eta \ \implies \ | f'_{|I\backslash \{a\}}(x) - \ell |\leq \varepsilon.$$ Fixons un tel $\eta$.\\
  Soit $x\in I\backslash \{a\}$ tel que $|x-a|\leq \eta$.\\ La fonction $f$ est continue sur $I$ donc $f$ est continue sur le segment d'extrémités $a$ et $x$ qui est par ailleurs inclus dans $I$ par convexité d'un intervalle.\\ La fonction $f$ est dérivable sur $I$ donc $f$ est dérivable sur l'intervalle ouvert $a$, $x$ qui est aussi inclus dans $\overset{\circ}{I}$ par convexité.\\ L'égalité des accroissements finis s'applique à $f$ sur l'intervalle $a$ et $x$ : $$\exists \ c_x \in ]a,x[ \cup ]x,a[ \ : \ \frac{f(x) -f(a)}{x-a} = f'(c_x)$$
  Or $|c_x-a| \leq |x-a| \leq \eta $ donc ladite définition de la limite s'applique pour $x\gets c_x$ : $|f'(c_x) - \ell|\leq \varepsilon $ si bien que $$|\frac{f(x)-f(a)}{x-a}- \ell |\leq \varepsilon.$$ D'où le lemme. \\
  \newline
  Prouvons alors le théorème.\\
  Sous ces hypothèses, le lemme s'applique donc $\lim_{x\to a} \frac{f(x) -f(a)}{x-a} = \ell$, or $\ell \in \R$, donc le taux d'accroissement de $f$ en $a$ admet une limite finie en $a$ ce qui prouve la dérivabilité de $f$ en $a$ et $f'(a) = \ell$. Ce qui suffit.
\end{question_kholle}

\begin{question_kholle}
  [Posons $\zeta \left| \begin{matrix}
        \R \longrightarrow \R \\
        x \mapsto \left\{ \begin{array}{cc}
                            0                 & \text{si } x \leqslant 0 \\
                            \e^{-\frac{1}{x}} & \text{si } x > 0
                          \end{array} \right.
      \end{matrix} \right.$.
    Montrons que $\zeta \in \Cont{\infty}{\R}{\R}$.
      {\begin{figure}[!h]
          \centering
          \begin{tikzpicture}
            \begin{axis}[
                axis lines = center,
                xlabel = $x$,
                ylabel = {$f(x)$},
                width=15cm,
                height=5cm,
                ymax=1.5
              ]
              \addplot[
                domain=0.01:20,
                samples=200,
                color=red,
              ]
              {exp(-1/x)};
              \addplot[
                domain=-10:0,
                samples=100,
                color=red,
              ]
              {0};
              \addlegendentry{$\zeta$}

              \addplot[
                domain=0:20,
                samples=100,
                color=blue,
                dashed
              ]
              {1};
            \end{axis}
          \end{tikzpicture}
        \end{figure}}]
  {La fonction $\zeta$ (pas celle-là une autre) est de classe \Cont{\infty}{}{} sur \R}
  ~ \\

  \begin{itemize}[label=$\star$]
    \item $\zeta_{]-\infty;0[}$ est constante donc $\zeta \in \Cont{\infty}{]-\infty;0[}{}$.
    \item $x \mapsto - \frac{1}{x} \in \mathcal{C}^\infty(]0;+\infty[,]-\infty;0[)$ et $\exp \in \Cont{\infty}{]-\infty;0[}{}$ donc, par stabilité de \Cont{\infty}{}{} par composition, $\zeta \in \Cont{\infty}{]0;+\infty[}{}$.
  \end{itemize}

  Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ :
  \begin{equation}
    \mathcal{P} : \text{\textquotedblleft} \ \exists P_n \in \R[x] : \forall x \in \R^*, \ \zeta^{(n)} = \left\{ \begin{array}{lc}
      0                                       & \text{si } x < 0 \\
      \frac{P_n(x)}{x^{2n}} \e^{-\frac{1}{x}} & \text{si } x > 0
    \end{array} \right. \ \text{\textquotedblright}
  \end{equation}
  \begin{itemize}[label=$\star$]
    \item $\mathcal{P}(0)$ est vrai par définition de $\zeta$ en posant $P_0(x) = 1$
    \item Soit $n \in \N^*$ \fq \ tel que $\mathcal{P}$ est vrai.
          D'une part, $\forall x \in ]-\infty;0[, \zeta^{(n)}(x) = 0$ donc
          \begin{equation*}
            \forall x \in ]-\infty;0[, \zeta^{(n+1)}(x) = 0
          \end{equation*}
          D'autre part, $\forall x \in ]0;+\infty[, \zeta^{(n)}(x) = \frac{P_n(x)}{x^{2n}} \e^{-\frac{1}{x}}$ ce qui est un produit de trois expressions dérivables. D'où :
          \begin{equation*}
            \begin{aligned}
              \forall x \in ]-\infty;0[,
              \zeta^{(n+1)}(x)
               & = \left( P_n'(x) \frac{1}{x^{2n}} + P_n(x) \frac{-2n}{x^{2n+1}} + \frac{P_n(x)}{x^{2n}} \frac{1}{x^2} \right) \e^{-\frac{1}{x}} \\
               & = \frac{x^2 P_n'(x) - 2nxP_n(x) + P_n(x)}{x^{2(n+1)}} \e^{-\frac{1}{x}}
            \end{aligned}
          \end{equation*}
          Si bien qu'en posant $P_{n+1}(x) = x^2 P_n'(x) - 2 n x P_n(x) + P_n(x) \in \R[x]$, on obtient :
          \begin{equation*}
            \forall x \in ]0;+\infty[, \zeta^{(n+1)}(x) = \frac{P_{n+1}(x)}{x^{2(n+1)}} \e^{-\frac{1}{x}}
          \end{equation*}
          Par conséquent, $\mathcal{P}(x)$ est vrai.

          Appliquons maintenant le théorème de prolongement du caractère \Cont{\infty}{}{}.
          \begin{itemize}[label=$\star$]
            \item Nous avons montré que $\zeta \in  \Cont{\infty}{\R \!\setminus\! \{0\}}{}$.
            \item Calculons les limites à gauche et à droite de 0. Soit $k \in \N$ \fq.
                  \begin{itemize}[label=$\star\star$]
                    \item $\zeta^{(k)}$ est nulle sur $]-\infty;0[$, $\zeta^{(k)} \arrowlim{x}{0^-} 0$.
                    \item De plus, $\exists P_n \in \R[x] : \ \forall x \in ]0;+\infty[, \zeta^{(k)}(x) = \frac{P_k(x)}{x^{2k}} \e^{-\frac{1}{x}}$. Posons $u = \frac{1}{x}$, ainsi $\zeta^{(k)}(x) = u^{2k} P_k(\frac{1}{u}) \e^{-\frac{1}{x}}$ et $u \arrowlim{x}{0^+} +\infty$. \\
                          Le théorème des croissances comparées donne $u^{2k} P_k(\frac{1}{u}) \e^{-u} \arrowlim{u}{+\infty} 0$ donc $\zeta^{(k)}(x) \arrowlim{x}{0^+} 0$.
                  \end{itemize}
          \end{itemize}
          Donc $\zeta \in \Cont{\infty}{\R}{\R}$.
  \end{itemize}
\end{question_kholle}

\pagebreak\section{Semaine 15}

\begin{question_kholle}
  [Soit $f : I \rightarrow \R$ convexe sur $I$. \\
    Soit $n \in \N^*$. Soient $x \in I^n$, $\lambda \in {[0;1]}^n$ telle que $\displaystyle \sum_{k=1}^{n} \lambda_k = 1$. \\
    \begin{equation}
      \sum_{k=1}^{n} \lambda_k x_k \in I \wedge
      f\left( \sum_{k=1}^{n} \lambda_k x_k \right)
      \leqslant \sum_{k=1}^{n} \lambda_k f\left( x_k \right)
    \end{equation}]
  {Inégalité de Jensen}
  Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N^*$ par :
  \begin{equation*}
    \mathcal{P}(n) : \text{\textquotedblleft}
    \forall x \in I^n,
    \forall \lambda \in [0;1]^n,
    \sum_{k=1}^{n} \lambda_k = 1 \implies
    \sum_{k=1}^{n} \lambda_k x_k \in I \wedge
    f\left( \sum_{k=1}^{n} \lambda_k x_k \right)
    \leqslant \sum_{k=1}^{n} \lambda_k f\left( x_k \right)
    \text{\textquotedblright}
  \end{equation*}

  \begin{itemize}[label=*, leftmargin=0.5cm]
    \item Soient $x \in I^1$ et $\lambda \in [0;1]^1$ tel que $\sum_{k=1}^{1} \lambda_k = 1$. \\
          Alors $\lambda_1 = 1$. Trivialement, $\sum_{k=1}^{1} \lambda_k x_k = \lambda_1 x_1 = x_1 \in I$. \\
          De plus, $f\left( \sum_{k=1}^{1} \lambda_k x_k \right)
            = f\left( \lambda_1 x_1 \right)
            = f\left( x_1 \right)
            = \lambda_1 f\left( x_1 \right)
            = \sum_{k=1}^{1} \lambda_k f\left( x_k \right)$. \\
          Donc $\mathcal{P}(1)$ vrai.

    \item  Soit $n \in \N^*$  tel que $\mathcal{P}(n)$ vrai. \\
          Soient $x \in I^{n+1}$ et $\lambda \in [0;1]^{n+1}$ tel que $\sum_{k=1}^{n+1} \lambda_k = 1$. \\
          $\{ x_k \;|_; k \in [\![1;n+1]\!] \}$ est une partie non vide ($n \geqslant 1$) d'un ensemble totalement ordonnée $\left(\R,\leqslant\right)$.
          Posons $a = \min\{ x_k \;|_; k \in [\![1;n+1]\!] \}$ et $b = \max\{ x_k \;|_; k \in [\![1;n+1]\!] \}$. D'où
          \begin{equation*}
            a
            \underbrace{=}_{\displaystyle \sum_{k=1}^{n+1} \lambda_k = 1} \sum_{k=1}^{n+1} \lambda_k a
            \underbrace{\leqslant}_{a \leqslant x_k} \sum_{k=1}^{n+1} \lambda_k x_k
            \underbrace{\leqslant}_{x_k \leqslant b} \sum_{k=1}^{n+1} \lambda_k b
            \underbrace{=}_{\displaystyle \sum_{k=1}^{n+1} \lambda_k = 1} b
          \end{equation*}
          Or $\{ x_k \;|_; k \in [\![1;n]\!] \} \subset I$ (car $x \in I^n$) donc $a \in I \wedge b \in I$. Donc
          \begin{equation*}
            \sum_{k=1}^{n+1} \lambda_k x_k
            \in [a;b]
            \underbrace{\subset}_{\begin{array}{c} \text{par convexité} \\ \text{de l'intervalle } I \end{array}} I
          \end{equation*}

          $\sum_{k=1}^{n+1} \lambda_k = 1$ donc $\exists i_0 \in [\![1;n+1]\!] : \lambda_{i_0} \neq 1$ (sinon $\sum_{k=1}^{n+1} \lambda_k = n+1 \neq 1$ car $n \neq 0$). \\
          Fixons un tel $i_0$.
          \begin{equation*}
            \begin{aligned}
              f\left( \sum_{k=1}^{n+1} \lambda_k x_k \right)
                                                                                                         & = f\left( \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k x_k + \lambda_{i_0} x_{i_0} \right)                                                             \\
                                                                                                         & = f\left( \lambda_{i_0} x_{i_0} + \left( 1 - \lambda_{i_0} \right) \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} x_k \right)  \\
              \underbrace{\leqslant}_{\begin{array}{c} \text{Par convexité} \\ \text{de } f \end{array}} & \lambda_{i_0} f(x_{i_0}) + \left( 1 - \lambda_{i_0} \right) f\left( \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} x_k \right)
            \end{aligned}
          \end{equation*}
          Or $\displaystyle \forall i \in [\![1;n+1]\!] \lambda_i \leqslant \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k = 1 - \lambda_{i_0}$ Donc $\displaystyle \frac{\lambda_i}{1 - \lambda_{i_0}} \in [0;1]$ et $\displaystyle \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} = 1$. Nous pouvons appliquer $\mathcal{P}(n)$ pour $\lambda_i \rightarrow \frac{\lambda_i}{1 - \lambda_{i_0}}$ :
          \begin{equation*}
            \begin{aligned}
              f\left( \sum_{k=1}^{n+1} \lambda_k x_k \right)
               & \leqslant \lambda_{i_0} f(x_{i_0}) + \left( 1 - \lambda_{i_0} \right) \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} f\left( x_k \right) \\
               & \leqslant \lambda_{i_0} f(x_{i_0}) + \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k f\left( x_k \right)                                                            \\
               & \leqslant \sum_{k = 1}^{n+1} \lambda_k f\left( x_k \right)                                                                                                                                  \\
            \end{aligned}
          \end{equation*}
          Donc $\mathcal{P}(n+1)$ vrai.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}
  [Soit $n \in \N^*$. Soit $x \in \R_+^{*n}$.
    \begin{equation}
      \left( \prod_{k=1}^{n} x_k \right)^{\nicefrac{1}{n}}
      \leqslant \frac{1}{n} \sum_{k=1}^{n} x_k
    \end{equation}]
  {Inégalité arithmético-géométrique}
  Soit de tels objets. Posons $\forall k \in [\![1;n]\!], \lambda_k = \nicefrac{1}{n}$. \\
  Sachant que l'exponentielle est convexe, appliquons l'inégalité de Jensen pour $x_k \leftarrow ln(x_k)$ (autorisé car $x_k \in \R_+^*$) :
  \begin{equation*}
    \exp \left( \sum_{k=1}^{n} \frac{1}{n} \ln \left( x_k \right) \right)
    \leqslant \sum_{k=1}^{n} \frac{1}{n} \exp \left( \ln \left( x_k \right) \right)
  \end{equation*}
  L'exponentielle est la bijection réciproque du logarithme népérien et est un morphisme additif. Nous obtenons ainsi l'inégalité recherchée.
\end{question_kholle}
\pagebreak\section{Semaine 16}

\begin{question_kholle}
  {Unicité de la partie régulière d'un développement limité}

  Soit $f$ une fonction admettant un $DL_n(x_0)$ avec $n \in \N$ et $x_0 \in \mathcal{D}_f$. \\
  Supposons que $f$ admette deux développements limités. C'est-à-dire qu'il existe $a \in \C^{n+1}$ et $b \in \C^{n+1}$ \tqs :
  \begin{equation*}
    \begin{aligned}
      f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} a_k (x - x_0)^k + o\left((x-x_0)^n\right) \\
      f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} b_k (x - x_0)^k + o\left((x-x_0)^n\right)
    \end{aligned}
  \end{equation*}

  Posons $u = x - x_0$ et  $\tilde{f}(u) = f(x_0+u)$ de sorte que les hypothèses sur $f$ se traduise par l'existence d'un $DL_n(0)$ pour $\tilde{f}$ :
  \begin{equation*}
    f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} a_k u^k + o\left(u^n\right)
    \text{ et }
    f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} b_k u^k + o\left(u^n\right)
  \end{equation*}
  Appliquons la définition d'un $DL_n(0)$. Il existe deux fonctions $\varepsilon_1$ et $\varepsilon_2$ définies sur $\mathcal{D}_{\tilde{f}}$ \tqs
  \begin{equation*}
    \begin{aligned}
      \forall u \in \mathcal{D}_{\tilde{f}}, \ \tilde{f}(u) = \sum_{k=0}^{n} a_k u^k + u^n \varepsilon_1 \\
      \forall u \in \mathcal{D}_{\tilde{f}}, \ \tilde{f}(u) = \sum_{k=0}^{n} b_k u^k + u^n \varepsilon_2 \\
      \textlim{u}{0} \varepsilon_1(u) = 0 \text{ et } \textlim{u}{0} \varepsilon_2(u) = 0
    \end{aligned}
  \end{equation*}
  Donc
  \begin{equation*}
    \forall u \in \mathcal{D}_{\tilde{f}}, \
    \sum_{k=0}^{n} (a_k - b_k) u^k = u^n \left( \varepsilon_2(u) - \varepsilon_1(u) \right)
  \end{equation*}

  Par l'absurde, supposons que $\exists k_0 \in \lient 0; n \rient : a_{k_0} \neq b_{k_0}$. Posons $k_1$ le plus petit entier dont les coefficients $a$ et $b$ sont différents :
  \begin{equation*}
    k_1 = \min \left\{ k \in \lient 0;n \rient \;|\; a_k \neq b_k \right\}
  \end{equation*}
  Nous obtenons alors
  \begin{equation*}
    \forall u \in \mathcal{D}_{\tilde{f}}, \
    \sum_{k=0}^{k_1-1} \underbrace{(a_k - b_k)}_{=0} u^k + (a_{k_1} - b_{k_1}) u^{k_1} + \sum_{k=k_1+1}^{n} (a_k - b_k) u^k = u^n \left( \varepsilon_2(u) - \varepsilon_1(u) \right)
  \end{equation*}
  Multiplions par $u^{-k_1}$ puis calculons la limite en $u \rightarrow 0$.
  D'un coté, pour $k > k_1$, nous avons $k - k_1 \leqslant 1$ donc $(a_k - b_k) u^{k-k_1} \arrowlim{u}{0} 0$.
  De l'autre coté, $u^{n-k_1}$ tend vers $0$ ou $1$ selon si $k_1 < n$ ou $k_1 = n$. Et, par hypothèse, $\varepsilon_2(u) - \varepsilon_1(u) \arrowlim{u}{0} 0$.
  Par unicité de la limite, $a_{k_1} - b_{k_1} = 0$. Ce qui contredit la définition de $k_1$.

  Par conséquent $\forall k \in \lient 0;n \rient, \ a_k = b_k$. Ainsi, la partie régulière d'un $DL$ est unique.
\end{question_kholle}

\setnbquestion{6}

\begin{question_kholle}
  {Deux fonctions équivalentes au voisinage de $a$ ont le même signe sur un voisinage de $a$}

  Soient $f : \mathcal{D} \rightarrow \R$ et $g : \mathcal{D} \rightarrow \R$ telles que $f(x) \underset{x \rightarrow a}{\sim} g(x)$ avec $a \in \mathcal{D}$. \\
  Appliquons la définition de l'équivalence pour $\varepsilon \leftarrow \frac{1}{2}$, il existe un voisinage $V$ de $a$ tel que :
  \begin{equation*}
    \forall x \in V \cap \mathcal{D},
    | f(x) - g(x) | \leqslant \frac{1}{2} | g(x) |
  \end{equation*}

  Fixons un tel voisinage $V$.
  Nous obtenons :
  \begin{equation*}
    \forall x \in V \cap \mathcal{D},
    \underbrace{g(x) - \frac{1}{2} | g(x) |}_{\text{du signe de }g(x)}
    \leqslant f(x) \leqslant
    \underbrace{g(x) + \frac{1}{2} | g(x) |}_{\text{du signe de }g(x)}
  \end{equation*}

  Ainsi $f(x)$ et $g(x)$ ont le même signe sur $V \cap \mathcal{D}$.
\end{question_kholle}

\begin{question_kholle}
  [ Soient $f \in \Cont{\infty}{\mathcal{D}}{}$ et $a \in \overset{\circ}{\mathcal{D}}$. Supposons que $E_0 = \left\{ p \in \N^* \setminus \{1\} \;|\; f^{(p)}(a) \neq 0 \right\}$ est non vide. \\
    Posons $p_0 = \min E_0$. \\
    $f$ admet un extremum local en $a$ si et seulement si $f'(a) = 0$ et $p_0$ est pair. \\
    $f$ admet un point d'inflexion en $a$ si et seulement si $p_0$ est impair. ]
  {Condition nécessaire et suffisante pour qu'une fonction \Cont{\infty}{}{} admette un extremum local ou un point d'inflexion}

  Soient de tels objets. Traitons le cas de l'extremum local.

  \noindent $f \in \Cont{\infty}{}{}$ donc, la formule Taylor-Young donne un $DL_{p_0}(a)$ de $f$ :
  \begin{equation*}
    f(x) \underset{x \rightarrow a}{=}
    \sum_{k=0}^{p_0} \frac{f^{(k)}(a)}{k!} (x-a)^k + o \left( (x-a)^{p_0} \right)
  \end{equation*}

  En développant :
  \begin{equation*}
    f(x) \underset{x \rightarrow a}{=}
    f(a) + \underbrace{f'(a)(x-a)}_{= 0} + \underbrace{\ldots + \frac{f^{(p_0-1)}(a)}{(p_0-1)!} (x-a)^{p_0-1}}_{= 0 \text{ par défintion de }p_0} + \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0} + o \left( (x-a)^{p_0} \right)
  \end{equation*}

  Ainsi (car $f^{(p_0)}(a) \neq 0$)
  \begin{equation}
    f(x) - f(a) \underset{x \rightarrow a}{\sim} \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}
  \end{equation}
  Au voisinage de $a$, $f(x) - f(a)$ et $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ ont le même signe.
  \\

  Supposons que $f$ admette un extremum local en $a$.
  Or $a \in \overset{\circ}{\mathcal{D}}$ et $f$ est dérivable en 0, donc $f'(a) = 0$.
  Comme $f$ admette un extremum local en $a$, $f(x) - f(a)$ est de signe constant au voisinage de $a$.
  Donc $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ est de signe constant au voisinage de $a$.
  Par conséquent, $p_0$ est pair.
  \\

  Réciproquement, supposons que $f'(a) = 0$ et que $p_0$ est pair. $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ est de signe constant au voisinage de $a$. Donc $f(x) - f(a)$ est de signe constant au voisinage de $a$. Ainsi, $a$ est un extremum local de $f$.
  \\

  Traitons le cas du point d'inflexion. La formule de Taylor-Young donne :
  \begin{equation}
    f(x) - \underbrace{\left( f(a) + (x-a)f'(a) \right)}_{\text{tangente en } (a,f(a))}
    \underset{x \rightarrow a}{\sim} \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}
  \end{equation}
  Le signe de l'écart courbe/tangente en $a$ est donc celui de $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$. Ce qui conclut de la même manière que l'extremum local.
\end{question_kholle}

\pagebreak\section{Semaine 17}

\setnbquestion{4}
\begin{question_kholle}
  [Soient $a,b\in \N ^*$ et $c\in \Z$. Il existe des entiers $x,y\in \Z$ tels que $ax + yb = c$ si et seulement si $c$ est multiple du pgcd de $a$ et $b$.]
  {Théorème de Bézout}
  Soient $a,b\in \N^*$. On suppose l'algorithme d'Euclide réalisé pour $a,b$, ainsi à la fin de ce dernier on a un entier naturel $r_n$ tel que $r_n = a\wedge b$. Comme l'algorithme est terminé, on peut remonter chaque ligne de proche en proche, on aurait, à titre d'exemple, pour une première itération, $r_n =r_{n-2} - q_{n} \cdot r_{n-1}$. En réalisant toutes les étapes nécessaires, on obtient une relation entre $r_n$ et $a,b$, cette relation s'écrit : $$\exists \ (x_0, y_0)\in (\Z ^*)^2 \ : \ a\wedge b = r_n = ax_0 + y_0b.$$
  Si $c$ est un multiple de $a\wedge b $, alors il existe $k\in \Z^*$ tel que $c = k(a\wedge b)$, donc en multipliant le résultat montré au dessus par $k$, on a le sens indirect. Si pour $c\in \Z$, il existe des entiers $x,y\in \Z$ tels que $ax + yb = c$, alors le pgcd de $a$ et $b$ divise le membre de gauche et donc par égalite le membre de droite aussi donc $c$ est multiple de $a\wedge b$, ce qui suffit.
\end{question_kholle}

\setnbquestion{6}
\begin{question_kholle}
  [Soient $a,b,c$ trois entiers naturels non nuls. Si $c$ est premier avec $a$ et divise le produit $ab$, alors il divise $b$.]
  {Théorème de Gauss}
  Soient $a,b,c$ des entiers naturels vérifiant les hypothèses. \\
  Comme $c$ est premier avec $a$ on écrit une relation de Bézout pour $1$, leur pgcd et on multiplie le tout par $b$ :
  $$\exists \ (u,v) \in (\N ^*)^2 \ : \ au + vc = 1 \ \implies \ abu + vbc = b, $$
  or $c$ divise $ab$ et lui-même donc aussi le membre de gauche donc par égalité, le membre droite, c'est le théorème.
\end{question_kholle}

\setnbquestion{8}
\begin{question_kholle}
  [Soient $a,b,c\in \Z$. Résoudre l'équation $$ax + yb = c,$$ d'inconnues $x$ et $y$ dans $\Z$.]
  {Résoudre une équation du type $ax + yb = c$}
  Soient $a,b,c\in \Z$ et une telle équation, notée $(i)$, en lesdites inconnues. \\
  Si $a\wedge b \not| \ c$, alors le théorème de Bézout, affirme que l'équation n'a pas de solution. \\
  Supposons le contraire. Posons $d = a\wedge b$. Le lemme technique affirme l'existence de $a'$ et $b'$ dans $\Z$, tels que $a'd= a$, $b'd =b$ et $a' \wedge b' = 1$. Donc, comme $d$ divise $c$, il existe $c'$ tel que $c = c'd$. On réécrit l'équation, notée $(ii)$ : $$ a'x +yb'  = c'.$$ On sait d'après le théorème de Bézout qu'il existe des solutions, en particulier grâce à l'algorithme d'Euclide on construit $(x_0,y_0)$, une solution de la nouvelle équation, puis on l'injecte et on raisonne par équivalence, on note $\omega$ l'ensemble des solutions de $(ii)$ et $\Omega$ celui de $(i)$ :
  \begin{center}
    $
      \begin{array}{ccc}
        (x,y) \in \Omega & \iff & (x,y)\in \omega                                           \\
                         & \iff & a'x +yb' = c'                                             \\
                         & \iff & a'x + yb' = a'x_0 + y_0b'                                 \\
                         & \iff & a'(x-x_0) = b'(y_0 -y)                                    \\[1ex]
                         & \iff & \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
                                                                   a'(x-x_0) & = & b'(y_0 -y) \\
                                                                   y_0 - y   & = & a'k
                                                                 \end{array} \right. \\[2ex]
                         & \iff & \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
                                                                   a'(x-x_0) & = & b'(y_0 -y) \\
                                                                   y         & = & y_0 - a'k
                                                                 \end{array} \right. \\[2ex]
                         & \iff & \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
                                                                   x & = & x_0 + b'k \\
                                                                   y & = & y_0 - a'k
                                                                 \end{array} \right.        \\[2ex]
                         & \iff & (x,y) \in \{ (x_0 + b'k,\  y_0 - a'k) \ | \ k\in \Z \}
      \end{array}
    $ \\
  \end{center}
  La première ligne découle de la divisibilité des coefficients par $d$, la deuxième est la définition d'appartenance à $\omega$, la troisième est une réécriture du fait que $(x_0,y_0)$ soit solution de $(ii)$, la quatrième est une factorisation banale, la cinquième une utilisation du théorème de Gauss pour le sens direct et le sens indirect ne pose pas de problème, la sixième est une réécriture de la deuxième relation, la septième découle de l'expression de $y$ pour le sens direct et le sens indirect s'obtient en multipliant avec parcimonie l'équation, la huitième est une réécriture de la septième qui ne pose pas de problème. C'est $\Omega$, par équivalence.

\end{question_kholle}
\pagebreak\section{Semaine 18}

\begin{question_kholle}
  {L'ensemble des nombres premiers est infini}
  
  Notons l'ensemble des nombres premiers $\PRIME = \left\{ n \in \N \;|\; \left| \mathcal{D}(n) \cup \N \right| = 2 \right\}$
  Par l'absurde, supposons que \PRIME est fini. \\
  Posons $\displaystyle m = 1 + \prod_{p \in \PRIME} p \in \N$. \\
  Comme $2 \in \PRIME$, $m \geqslant 2$. Donc $m$ admet un diviseur premier, $\exists q \in \PRIME : q \;|\; m$. Donc $q \wedge m = q$. \\
  Par ailleurs, $\displaystyle m = 1 + q \left( \prod_{\tiny \begin{matrix} p \in \PRIME \\ p \neq q \end{matrix}} p \right)$. Donc $\displaystyle m - q \left( \prod_{\tiny \begin{matrix} p \in \PRIME \\ p \neq q \end{matrix}} p \right) = 1$. D'après le théorème de Bézout, $q \wedge m = 1$. \\
  Donc $q = 1$ ce qui est une contradiction avec $q \in \PRIME$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $n \in \N^*, p \in \PRIME, k_0 \in \N$.
    \begin{equation}
      \nu_p(n) = k_0 \iff
      \exists m \in \Z : \left\{ \begin{matrix}
        n = p^{k_0} m \\
        m \wedge p = 1
      \end{matrix} \right.
    \end{equation}
  ]
  {Caractérisation de la valuation \textit{p}-adique}
  
  $\implies$ Supposons que $\nu_p(n) = k_0$. \\
  Par définition de la valuation \textit{p}-adique, $p^{\nu_p(n)} \;|\; n$ donc $p^{k_0} \;|\; n$.
  Notons $m \in \Z$ le quotient de la division euclidienne de $n$ par $p^{k_0}$. Nous avons $n = p^{k_0} m$. \\
  Comme $m \wedge p \in \mathcal{D}(p) \cap \N$, $m \wedge p \in \left\{1,p\right\}$.
  Par l'absurde, supposons que $m \wedge p = p$.
  \begin{equation*}
    \begin{aligned}
      p \;|\; m
       & \implies \exists m' \in \Z : m = p m'                                                     \\
       & \implies \exists m' \in \Z : n = p p^{k_0} m' = p^{k_0+1} m'                              \\
       & \implies k_0 + 1 \in \left\{ k \in \N \;|\; p^k | n \right\}                              \\
       & \implies k_0 + 1 \leqslant \max \left\{ k \in \N \;|\; p^k | n \right\} = \nu_p(n) = k_0
    \end{aligned}
  \end{equation*}
  Ce qui est une contradiction donc $m \wedge p = 1$.
  
  $\impliedby$ Supposons $\exists m \in \Z : \left\{ \begin{matrix}
      n = p^{k_0} m \\
      m \wedge p = 1
    \end{matrix} \right.$ \\
  Par définition de la valuation \textit{p}-adique, $p^{\nu_p(n)} \;|\; n$ donc $p^{\nu_p(n)} \;|\; p^{k_0} m$. Or $m \wedge p = 1$ donc $m \wedge p^{\nu_p(n)} = 1$.
  D'après le théorème de Gauss, $p_{\nu_p(n)} \;|\; p^{k_0}$. Donc $\exists \alpha \in \Z : \alpha p_{\nu_p(n)} = p^{k_0}$
  \begin{equation*}
    \begin{aligned}
      \alpha p_{\nu_p(n)} = p^{k_0}
       & \implies p^{k_0} - \alpha p_{\nu_p(n)} = 0                                                            \\
       & \implies p^{k_0} \left( 1 - \alpha p^{\nu_p(n) - k_0} \right) = 0 \text{ car } k_0 \leqslant \nu_p(n) \\
       & \implies \alpha p^{\nu_p(n) - k_0} = 1 \text{ car \Z est intègre}                                     \\
       & \implies p^{\nu_p(n) - k_0} \in \mathcal{D}(1) \cap \N                                                \\
       & \implies p^{\nu_p(n) - k_0} = 1                                                                       \\
       & \implies \nu_p(n) - k_0 = 0                                                                           \\
       & \implies \nu_p(n) = k_0                                                                               \\
    \end{aligned}
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}
  [\begin{equation}
      \forall (a, b) \in \Z^2, \
      a|b \ \iff \ \forall p \in \PRIME, \ \nu _p (a) \leq \nu _p (b)
    \end{equation}]
  {Caractérisation de $a | b$ par les valuations $p$-adiques et preuve de leur propriété de morphisme.}
  Premièrement, montrons que la valuation $p$-adique est un morphisme de $(\Z ^* , \times)$ dans $(\N ,+)$. \\
  Soient de tels entiers relatifs $a,b$. \\
  \[
    \exists \ m,n \in (\Z ^*)^2 \ : \ \left(\left(a = p^{\nu _p (a)}m\right) \ \wedge \ (m\wedge p = 1)\right) \ \wedge \ \left(\left(b = p^{\nu _p (b)}n\right) \ \wedge \ (n\wedge p = 1)\right),
  \]
  donc $ab = p^{\nu _p (a) + \nu _p (b)}mn$ et $mn \wedge p = 1$, par la réciproque de la caractérisation des valuations $p$-adiques :
  \[
    \nu _p (ab) = \nu _p (a) + \nu _p (b).
  \]
  Prouvons le sens réciproque de la susdite caractérisation.  Supposons le membre de droite. \\
  D'après le théorème de décomposition en facteurs premiers,
  \[
    |b| = \prod_{p\in \PRIME} p^{\nu _p (b)} = \prod_{p\in \PRIME} p^{\nu _p (a)}(p^{\nu _p (b) - \nu _p (a)}) = \prod_{p\in \PRIME}  p^{\nu _p (a)} \prod_{p\in \PRIME} p^{\nu _p (b) - \nu _p (a)}= |a|\prod_{p\in \PRIME} p^{\nu _p (b) - \nu _p (a)},
  \]
  la première manipulation se justifie par hypothèse et la seconde peut se justifier par le calcul.\\
  Ainsi, $|a| | |b|$ donc $a|b$. \\
  Prouvons le sens direct. Supposons le membre de gauche.  \\
  Soit $p \in \PRIME$. Il existe $k\in \Z$ tel que $ak = b $ car $a |b$. Ainsi,
  \[
    \nu _p (b) = \nu _p (ak) = \nu _p (a) + \nu _p (k) \geq \nu _p (a).
  \]
  Ce qui suffit.
\end{question_kholle}

\begin{question_kholle}
  [Le pgcd comme produit des $p$ à la puissance du minimum des $\nu _p$ et le ppcm comme le produit des  $p$ à la puissance du maximum des $\nu _p$.
    \begin{equation}
      \begin{aligned}
        a \wedge b & = \prod_{p\in \PRIME} p^{\min (\nu _p (a),\nu_p (b))} \\
        a \vee b   & = \prod_{p\in \PRIME} p^{\max (\nu _p (a),\nu_p (b))}
      \end{aligned}
    \end{equation}]
  {Expression du pgcd et du ppcm à partir des décomposition en facteurs premiers de $a$ et $b$.}
  Prouvons la formule du pgcd et déduisons-en la formule du ppcm. \\
  Soient $(a,b)\in (\Z ^*)^2$. Soit $p \in \PRIME $. Il faut et il suffit de montrer que $\nu_p (a \wedge b) = \min (\nu_p(a), \nu_p(b))$ pour obtenir le résultat. On a $a\wedge b | a$ et $a \wedge b | b$ donc d'après la caractérisation de la divisibilité par les valuations $p$-adiques, $\nu_p (a \wedge b) \leq \nu _p (a)$ et $\nu _p (a\wedge b) \leq \nu _p (b)$ donc $\nu _p (a\wedge b) \leq \min (\nu _p(a), \nu_p (b)).$ \\
  Posons $m = \min (\nu _p(a), \nu_p (b))$. On a
  \[
    |a| = \prod_{q \in \PRIME} q ^{\nu _q (a)} = p^m \left( (p^{\nu_p (a)- m})\prod_{q \in \PRIME \backslash \{p\}} q ^{\nu _q (a)} \right),
  \]
  car par définition, $m \leq \nu _p (a)$, donc $p^m | a$, on montrerait de même que $p^m |b$, donc par définition, $p^m | a\wedge b$, donc une nouvelle fois en appliquant la caractérisation de la divisibilité par les valuations $p$-adiques, $m \leq \nu_p ( a\wedge b)$. Finalement, $\nu_p (a\wedge b) = m$. \\
  On en déduit la formule du ppcm :
  \[
    |a||b| = (a \wedge b) (a \vee b) \ \implies \ a\vee b = \prod_{p\in \PRIME} p^{\nu _p(a) + \nu _p(b) - \min (\nu _p (a),\nu_p (b))} =  \prod_{p\in \PRIME} p^{\max (\nu _p (a),\nu_p (b))}
  \]
  
\end{question_kholle}

\begin{question_kholle}
  [Petit Th. de Fermat :
  {\begin{enumerate}[label=($\roman*$)]
    \item $\forall a \in \Z , \ a^p \equiv a \mod p$ \\
          $\ \forall x \in \Z / p\Z, \ x^p = x $
    \item $\forall a \in \Z, \ p\not | a, \ \implies \ a^{p-1} \equiv 1 \mod p$ \\
          $\ \forall x \in \Z / p\Z, \ x^{p-1} = 1 $
  \end{enumerate}}]
  {Pour $p$ premier, $(a+b)^p \equiv a^p + b^p \mod p$, en déduire le petit Th. de Fermat (2 versions), expression du résultat dans $\Z / p\Z$.}
  Soient $a,b$ de tels entiers relatifs et soit $p$ un nombre premier. Calculons,
  \[
    (a + b )^p  = \sum_{k = 0}^p \binom{p}{k}a^{p-k}b^k  = a^p + b^p + \sum_{k = 1}^{p-1} \binom{p}{k}a^{p-k}b^k \equiv a^p + b^p \mod p,
  \]
  car $\forall k \in [\![1,p-1 ]\!], \ p | \binom{p}{k}$ (élémentaire), d'où le résultat. \\
  Dans $\Z /p\Z$, ce résultat s'énonce comme suit :
  \[
    \forall (x,y) \in \Z /p\Z ^2, \ (x+y)^p = x^p + y^p.
  \]
  En guise d'application, démontrons le petit Th. de Fermat énoncé plus haut. \\
  Démonstration du $(i)$. Considérons le prédicat $\PRIME ( \cdot)$ défini sur $\N$  par :
  \[
    \PRIME (a) : "a^p \equiv a \mod p".
  \]
  Initialisation : Pour $a = 0$, rien à faire, donc $\PRIME (0)$ est vrai. \\
  Hérédité : Soit $a\in \N$ \tq $\ \PRIME (a)$. Calculons,
  \[
    (a+1)^p  \equiv a^p + 1 \mod p \overset{\PRIME (a)}{\equiv} a + 1 \mod p,
  \]
  donc $\PRIME (a+1)$ vrai. \\
  Par Th. de récurrence sur $\N$, $\PRIME(a)$ est vrai pour tout $a\in \N$. \\
  Il faut maintenant étendre le résultat à $\Z$. Soit $p\in \PRIME \backslash \{2\}$, ainsi $p$ est impair. Soit $a\in \Z \backslash \N$. Calculons,
  \[
    a^p\equiv (-|a|)^p \mod p \equiv - |a|^p \mod p \overset{\underset{\text{pour }a \gets |a|}{\text{Th. de Fermat}}}{\equiv} - |a| \mod p \equiv a \mod p  .
  \]
  Si $p =2$, $\ a^2 \equiv |a|^2 \mod 2 \equiv |a| \mod 2 \equiv -|a| \mod 2 \equiv a \mod 2$. \\
  Le $(ii)$, soit $a\in \Z$ tel que $p \not | a$.
  \[
    (p\not | a)\wedge (p\in \PRIME) \implies p\wedge a = 1,
  \]
  d'après le $(i)$, $\ p | a^p -a \ \implies \ p| a(a^{p-1} -1) \ \overset{\text{Th. de Gauss}}{\implies}  \ p| a^{p-1} -1 \ \implies \ a^{p-1} \equiv 1 \mod p$. \\
  Les écritures dans $\Z /p \Z$ ne posent pas de problème.s, ce qui conclut.
\end{question_kholle}

\begin{question_kholle}
  []
  {$\Z /n\Z$ est un corps si et seulement si $n$ est premier.}
  Montrons le sens réciproque, supposons $n\in \PRIME$. \\
  Soit $x\in \Z/n\Z$ tel que $x \neq \overline{0}$. \\
  $\exists \ a \in [\![0,p-1]\!]\ : \ c = \overline{a}$, $\ I =[\![0, p-1]\!]$ étant un système de représentant des classes. \\
  Comme $a\in I, \ n\not | a$, or $n \in \PRIME$, donc $n \wedge a = 1$. Par Bezout, il existe $u,v \in \Z^2$ tels que $au +nv =1$, donc $u$ est l'inverse de $a$ modulo $n$ donc $a\in \Z/n\Z ^\times$, dès lors, tout élément non nul de $\Z/n\Z$ est inversible, or c'est un anneau commutatif, donc c'est un corps. \\ \\
  Montrons le sens direct en raisonnant par contraposition, supposons $n\not \in \PRIME$. \\
  Comme $n$ n'est pas premier et est plus grand que $2$, il admet un diviseur, $d$, dans $I\backslash \{0,1\} = J$. Notons $d'$ le quotient de la division euclidienne de $n$ par $d$, on a alors $a = dd'$ et $d'\in J$. Donc $\overline{d}\overline{d'}=\overline{0}$ et comme $d,d' \in J$, on a $d,d' \neq 0$, donc $\overline{d}$ est un diviseur de zéro de $\Z/n\Z$, donc $\overline{d}$ est un élément non nul de $\Z/n\Z$ non inversible, donc $\Z/n\Z$ n'est pas un corps. En contraposant ce que nous venons de démontrer on a le résulat. Ce qui conclut.
\end{question_kholle}

\begin{question_kholle}
  {Les éléments inversibles d'un anneau $A$ forment un groupe multiplicatif noté $\left( A^\times, \times \right)$}
  
  Soit $(A, +, \times)$ un anneau. \\
  Un élément inversible (ou unité) est un élément de $A$ symétrisable pour la loi $\times$. Posons l'ensemble des éléments inversibles $A^\times = \left\{ a \in A \;|\; \exists b \in A : a \times b = b \times a = 1_A \right\}$.
  
  \begin{itemize}[label=$\star$, leftmargin=.5cm]
    \item Montrons que la LCI $\times$ se restreint bien à $A^\times$ en un LCI $\times_{A^\times}$. \\
          Soient $(a_1, a_2) \in {A^\times}^2$.
          Par défintion de $A^\times$, $\exists (b_1, b_2) \in A^2 : a_1 \times b_1 = b_1 \times a_1 = 1_A \text{ et } a_2 \times b_2 = b_2 \times a_2 = 1_A$.
          \begin{equation*}
            \left( a_1 \times a_2 \right) \times \left( b_2 \times b_1 \right)
            \underbrace{=}_{\text{loi associative}} a_1 \times \underbrace{a_2 \times b_2}_{= ~ 1_A} \times b_1
            = a_1 \times b_1
            = 1_A
          \end{equation*}
          \begin{equation*}
            \left( b_2 \times b_1 \right) \times \left( a_1 \times a_2 \right)
            \underbrace{=}_{\text{loi associative}} b_2 \times \underbrace{b_1 \times a_1}_{= ~ 1_A} \times a_2
            = b_2 \times a_2
            = 1_A
          \end{equation*}
          Donc $\left( a_1 \times a_2 \right) \in A^\times$.
          
    \item La loi $\times$ est associative donc la loi $\times_{A^\times}$ l'est aussi.
          
    \item $1_A$ vérifie $1_A \times 1_A = 1_A$ donc $1_A \in A^\times$. \\
          De plus, $\forall a \in A^\times, 1_A \times_{A^\times} a = a \times_{A^\times} 1_A = a$ donc $\times_{A^\times}$ admet $1_A$ comme élément neutre.
          
    \item Soit $a \in A^\times$. Par définition de $A^\times$, $\exists b \in A : a \times b = b \times a = 1_A$. \\
          D'où $b \in A^\times$. En pensant les égalités ci-dessus dans $A^\times$,
          \begin{equation*}
            a \times_{A^\times} b = b \times_{A^\times} a = 1_A
          \end{equation*}
          Donc $a$ est inversible dans $A^\times$.
  \end{itemize}
  
  Ainsi, $\left( A^\times, \times_{A^\times} \right)$ est un groupe.
\end{question_kholle}

\begin{question_kholle}
  {L'image directe par un morphisme d'anneau d'un sous-anneau de l'anneau de départ est un sous anneau de l'anneau d'arrivée. De même pour l'image réciproque.}
  
  Soient $\left(A,+,\times\right)$ et $\left(B,+,\times\right)$ deux anneaux et $f : A \rightarrow B$ un morphisme d'anneau.
  
  \noindent Soit $A'$ un sous-anneau de $A$. Montrons que $f(A')$ est un sous-anneau de $B$.
  \begin{itemize}[label=$\star$, leftmargin=.5cm]
    \item Par définition de $f$, $f(A') \subset B$ et $(B,+,\times)$ est un anneau.
    \item Soient $(u,v) \in f(A')^2$. Alors $\exists (a,b) \in A'^2 : f(a) = u \text{ et } f(b) = v$. $f$ est un morphisme d'anneau donc un morphisme de groupe de $(A,+)$ dans $(B,+)$ donc
          \begin{equation*}
            u - v = f(a) - f(b) = f(a - b)
          \end{equation*}
          Comme $A'$ est un sous-anneau, $a - b \in A'$. Donc $u - v \in f(A')$. \\
          De même, $f$ est un morphisme d'anneau donc un morphisme de monoïde de $(A,\times)$ dans $(B,\times)$ donc
          \begin{equation*}
            u \times v = f(a) \times f(b) = f(a \times b)
          \end{equation*}
          Comme $A'$ est un sous-anneau, $a \times b \in A'$. Donc $u \times v \in f(A')$.
    \item $f$ est un morphisme d'anneau donc $1_B = f(1_A)$. Or $A'$ est un sous-anneau donc $1_A \in A'$. D'où $1_B \in f(A')$.
  \end{itemize}
  
  \noindent Soit $B'$ un sous-anneau de $B$. Montrons que $f^{-1}(B')$ est un sous-anneau de $A$.
  \begin{itemize}[label=$\star$, leftmargin=.5cm]
    \item Par définition de $f$, $f^{-1}(B') \subset A$ et $(A,+,\times)$ est un anneau.
    \item Soient $(a,b) \in f^{-1}(B')^2$. $f$ est un morphisme d'anneau donc un morphisme de groupe de $(A,+)$ dans $(B,+)$ donc
          \begin{equation*}
            f(a - b) = \underbrace{f(a)}_{\in B'} - \underbrace{f(b)}_{\in B'} \in B'
          \end{equation*}
          Donc $a - b \in f^{-1}(B')$. \\
          De même, $f$ est un morphisme d'anneau donc un morphisme de monoïde de $(A,\times)$ dans $(B,\times)$ donc
          \begin{equation*}
            f(a b) = \underbrace{f(a)}_{\in B'} \underbrace{f(b)}_{\in B'} \in B'
          \end{equation*}
          Donc $a b \in f^{-1}(B')$.
    \item $f$ est un morphisme d'anneau donc $1_B = f(1_A)$. Or $B'$ est un sous-anneau donc $1_B \in B'$. D'où $1_A \in f^{-1}(B')$.
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 19}

\begin{question_kholle}
  [Pour une matrice $A \in \mathcal{M}_{(n,p)}(\K)$, la matrice transposée est définie :
  {\begin{equation*}
    \forall (k, l) \in [\![1,p]\!] \!\times\! [\![1,n]\!], \ \left[A^T\right]_{kl} = A_{lk}
  \end{equation*}}
  Formellement, la transposition est une application de $\mathcal{M}_{(n,p)}(\K)$ dans $\mathcal{M}_{(p,n)}(\K)$.]
  {$\left(A \times B\right)^T = B^T \times A^T$}

  Soit $(A, B) \in \mathcal{M}_{(n,p)}(\K) \times \mathcal{M}_{(p,q)}(\K)$. \\
  $\left(A \times B\right)^T \in \mathcal{M}_{(q,n)}(\K)$. Soit $(i, j) \in [\![1,q]\!] \!\times\! [\![1,n]\!]$.
  \begin{equation*}
    \begin{aligned}
      \left[ \left(A \times B\right)^T \right]_{i,j}
       & = \left[A \times B\right]_{j,i}                                          \\
       & = \sum_{k=1}^{p} A_{j,k} \times_\K B_{k,i}                               \\
       & = \sum_{k=1}^{p} B_{k,i} \times_\K A_{j,k}                               \\
       & = \sum_{k=1}^{p} \left[B^T\right]_{i,k} \times_\K \left[A^T\right]_{k,j} \\
       & = \left[ \left(B^T\right) \times \left(A^T\right) \right]_{i,j}
    \end{aligned}
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}
  [Le symbole de Kronecker est défini de la manière suivante :
  \begin{equation*}
    \forall (x, y) \in \R^2, \delta_{xy} = \left\{ \begin{matrix}
      0 \text{ si } x \neq y \\
      1 \text{ si } x = y
    \end{matrix} \right.
  \end{equation*}
  La matrice $E^{i,j} \in \mathcal{M}_{(n, p)}(\K)$ avec {$(i, j) \in [\![ 1, n ]\!] \!\times\! [\![ 1, p ]\!]$} ne possède que des coefficients nuls sauf le coefficient de la $i^{e}$ ligne et $j^{e}$ colonne qui vaut 1. Formellement :
  {\begin{equation*}
    \forall (r, s) \in [\![ 1, n ]\!] \times [\![ 1, p ]\!], \
    \left[E^{i,j}\right]_{rs} = \delta_{ir} \delta_{js}
  \end{equation*}}]
  {Calculer $E^{i,j} \times E^{k,l}$ en fonction de $i$, $j$, $k$, $l$ et des symboles de Kronecker}

  Calculons $E^{i,j}(n,p) \times E^{k,l}(p,q)$.

  Soient $(r, s) \in [ \! [ 1, n] \!] \times [ \! [ 1, q ] \!]$ fq

  \begin{align*}
    \left[ E^{i,j} \times E^{k,l} \right] _{rs}
     & = \sum_{t = 1}^{n}E^{i,j}_{r,t} E^{k,l}_{t,s}                     \\
     & =\sum_{t = 1}^{n} \delta_{ir} \delta_{jt} \delta_{kt} \delta_{ls} \\
     & = \delta_{jk} \delta_{ir} \delta_{ls}                             \\
     & = \delta_{jk} \left[ E^{i,l} \right] _{rs}
  \end{align*}

  Donc $E^{i,j} \times E^{k,l} = \delta_{jk} E^{i,l}$.


  Ainsi, pour le calcul de $(E^{i,j})^{2}$, $q \leftarrow n$, $k \leftarrow i$, $l \leftarrow j$.

  \begin{align*}
    (E^{i,j})^{2} = \delta_{ji} E^{i,j} = \left\{
    \begin{array}{ll}
      E^{i,j} \text{ si } i = j \\
      0_{n,p} \text{ si } i \neq j
    \end{array}
    \right.
  \end{align*}

\end{question_kholle}

\begin{question_kholle}
  {Les matrices triangulaires supérieures forment un sous-anneau de $\mathcal{M}_n(\K)$}

  $\mathcal{T}_n^+(\K) \subset \mathcal(M)_n(\K)$ et $(\mathcal{M}_n(\K), +, \times)$ est un anneau. \\
  $\mathcal{T}_n^+(\K) \neq \emptyset$ car $I_n \in \mathcal{T}_n^+(\K)$ ($I_n$ est le neutre multiplicatif de $\mathcal{M}_n(\K)$). \\
  Soient $(A, B) \in \mathcal{T}_n^+(\K)^2$ . \\
  Soient $(i, j) \in [\![1,n]\!]^2$ $\text{ tels que }$ $i > j$.
  \begin{equation*}
    (A - B)_{i,j}
    = \underbrace{A_{i,j}}_{=0 \text{ car } A \in \mathcal{T}_n^+(\K)} - \underbrace{B_{i,j}}_{=0 \text{ car } B \in \mathcal{T}_n^+(\K)}
    = 0
  \end{equation*}
  Donc, $A - B \in \mathcal{T}_n^+(\K)$.

  \begin{equation*}
    \begin{aligned}
      (A \times B)_{i,j}
       & = \sum_{k=1}^{n} A_{i,k} \times_\K B_{k,j}                                                                                        \\
       & = \sum_{k=1}^{j} \underbrace{A_{i,k}}_{=0 \text{ car } i > j \geqslant k \text{ et } A \in \mathcal{T}_n^+(\K)} \times_\K B_{k,j}
      + \sum_{k=j+1}^{n} A_{i,k} \times_\K \underbrace{B_{k,j}}_{=0 \text{ car } k > j \text{ et } B \in \mathcal{T}_n^+(\K)}              \\
       & = 0
    \end{aligned}
  \end{equation*}
  Donc, $A \times B \in \mathcal{T}_n^+(\K)$.
\end{question_kholle}

\begin{question_kholle}
  {Si $A$ est une matrice d'ordre $n$ et $\lambda$ un scalaire non nul d'un corps, alors la transposée de $A$ et $\lambda A$ sont inversibles aussi.}
  Soient $A,\lambda \in \mathcal{GL}_n(\mathbb{K})\times \mathbb{K}^*$, avec $\mathbb{K}$ un corps. \\
  Par définition, il existe $B\in \mathcal{GL}_n(\mathbb{K})$ tel que $AB=BA=I_n$. Ainsi :
  \[
    (AB)^T = I_n^T \ \iff \ B^TA^T = I_n,
  \]
  donc $A^T$ admet un inverse à gauche, $B^T$, donc un inverse tout court et donc $A^T$ est inversible (on notera que $A^T$ reste dans les matrices d'ordre $n$). De même,
  \[
    \lambda AB = \lambda I_n \ \iff \ (\lambda A)B = \lambda I_n \ \iff \ (\lambda A) \left(\frac{1}{\lambda}B \right) = I_n,
  \]
  car les scalaires commutent avec toutes les matrices. Ainsi, $\lambda A$ admet un inverse à droite, donc un inverse tout court, donc est inversible, d'inverse $\frac{1}{\lambda}B$. Concluant la preuve.
\end{question_kholle}

\begin{question_kholle}
  {Si $N$ est une matrice d'ordre $n$ nilpotente, alors $I_n + \lambda N$ est inversible pour tout $\lambda$, scalaire d'un corps.}
  Soient $N$ une matrice d'ordre $n$ à coefficient dans $\mathbb{K}$, un corps, nilpotente, d'indice de nilpotence $k$ (un entier naturel donc) et $\lambda \in \mathbb{K}$. Calculons :
  \[
    I_n^{2k+1} + (\lambda N)^{2k+1} = I_n^{2k+1} - (- \lambda N)^{2k+1} = (I_n + \lambda N)\sum_{i=0}^{2k}(-\lambda N)^i =  (I_n + \lambda N)\sum_{i=0}^{k-1}(-\lambda N)^i,
  \]
  car $\lambda N$ commute avec $I_n$, or le membre de gauche est égal à $I_n$ car $2k+1 > k$, donc $I_n + \lambda N$ est inversible à droite, donc inversible tout court, d'inverse $\sum_{i=0}^{k-1}(-\lambda N)^i$. Ce qui conclut la preuve.
\end{question_kholle}

\begin{question_kholle}
  [$A \in \mathcal{M}_n(\K)$ est inversible si et seulement si pour tout $Y \in \mathcal{M}_{n,1}(\K)$, l'équation $AX = Y$ d'inconnue $X \in \mathcal{M}_{n,1}$ admet une unique solution.
    \begin{equation}
      \forall A \mathcal{M}_n(\K),
      A \in \mathcal{GL}_n(\K) \iff
      \forall Y \in \mathcal{M}_{n,1}(\K), \exists ! X \in \mathcal{M}_{n,1} : AX = Y
    \end{equation}]
  {Caractérisation de l'inversibilité pour les matrices}

  Supposons que $A \in \mathcal{GL}_n(\K)$.
  Soit $Y \in \mathcal{M}_{n,1}(\K)$ \fq. \\
  $AX = Y \iff A^{-1}AX = A^{-1}Y \iff X = A^{-1}Y$ donc l'équation $AX = Y$ d'inconnue $X \in \mathcal{M}_{n,1}$ admet une unique solution.

  Supposons maintenant que $\forall Y \in \mathcal{M}_{n,1}(\K), \exists ! X \in \mathcal{M}_{n,1} : AX = Y$. \\
  Pour $i \in [\![1;n]\!]$, notons  $X_i$ la solution de $AX = E^{i,1}$. \\
  Posons $\displaystyle B = \left[ \begin{array}{c|c|c|c}
            &     &        &     \\
        X_1 & X_2 & \ldots & X_n \\
            &     &        &     \\
      \end{array} \right]$. \\
  Calculons $\displaystyle AB
    = \left[ \begin{array}{c|c|c|c}
             &      &        &      \\
        AX_1 & AX_2 & \ldots & AX_n \\
             &      &        &      \\
      \end{array} \right]
    = \left[ \begin{array}{c|c|c|c}
                &         &        &         \\
        E^{1,1} & E^{2,1} & \ldots & E^{n,1} \\
                &         &        &         \\
      \end{array} \right]
    = I_n$. \\
  Ainsi A est inversible à droite donc $A \in \mathcal{GL}_n(\K)$ et $A^{-1} = B$.
\end{question_kholle}

\begin{question_kholle}
  [Une matrice diagonale est inversible si et seulement si tous ses coefficients diagonaux sont non nuls.
    \begin{equation}
      \forall D = diag(d_1, d_2, \ldots, d_n) \in \mathcal{D}_n(\K),
      D \in \mathcal{GL}_n(\K) \iff \prod_{i=1}^{n} d_i \neq 0
    \end{equation}]
  {Caractérisation des matrices diagonales inversibles}

  Soit $D \in \mathcal{D}_n(\K)$ de coefficients diagonaux $d_1, d_2, \ldots, d_n \in \K^n$. \\
  Soit $Y = \begin{bmatrix} y_1 \\ \ldots \\ y_n \end{bmatrix} \in \mathcal{M}_{n,1}(\K)$.
  \'Etudions l'équation $DX = Y$ d'inconnue $X = \begin{bmatrix} x_1 \\ \ldots \\ x_n \end{bmatrix} \in \mathcal{M}_{n,1}(\K)$.
  \begin{equation*}
    DX = Y \iff
    \left\{ \begin{array}{cccccccccc}
      d_1 x_1 &         &        &         & = & y_1 &     &        &       \\
              & d_2 x_2 &        &         & = &     & y_2 &        &     & \\
              &         & \ddots &         & = &     &     & \ddots &       \\
              &         &        & d_n x_n & = &     &     &        & y_n   \\
    \end{array} \right.
  \end{equation*}

  \begin{itemize}
    \item Si $\exists i_0 \in [\![1;n]\!] : d_{i_0} = 0$, la $i_0$-ème ligne du système ci-dessus deviens une condition de compatibilité $0 = y_{i_0}$ qui ne sera pas respecté pour $Y = E^{i_0,1}$. Donc $D \notin \mathcal{GL}_{n}(\K)$.
    \item Sinon $\forall i \in [\![1;n]\!] : d_i \neq 0$, le système est donc triangulaire à coefficients diagonaux non nuls. Il admet donc une unique solution. Ainsi $D \in \mathcal{GL}_{n}(\K)$.
          \begin{equation*}
            DX = Y \iff
            \left\{ \begin{array}{cccccccccc}
              x_1 &     &        &     & = & d_1^{-1} y_1 &              &        &                \\
                  & x_2 &        &     & = &              & d_2^{-1} y_2 &        &              & \\
                  &     & \ddots &     & = &              &              & \ddots &                \\
                  &     &        & x_n & = &              &              &        & d_n^{-1} y_n   \\
            \end{array} \right.
          \end{equation*}
          Ainsi $D^{-1} = diag\left(d_1^{-1}, d_2^{-1}, \ldots, d_n^{-1}\right)$.
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 20}

\begin{question_kholle}
  [{\begin{equation}
    \K[X] ^\times = \left\{ \lambda X^0, \lambda \in \K^* \right\}
  \end{equation}}]
  {Éléments inversibles de l'anneau $\K[X]$}
  
  Soit P un élément inversible de $\K[X]$.
  Alors $\exists Q \in \K[X] : P \cdot Q = Q \cdot P = 1_{\K[X]}$.
  En prenant les degrés des polynômes, $\text{deg } P + \text{deg } Q = 0$. \\
  Or $\text{deg } : \K[X] \rightarrow \N$ donc $\text{deg } P = \text{deg } Q = 0$.
  Donc $\exists \lambda \in \K^* : P = \lambda$. \\
  Ainsi $\K[X]^\times \subset \left\{ \lambda X^0, \lambda \in \K^* \right\}$.
  
  Soit $\lambda \in \K^*$. Considérons $P = \lambda$.
  Posons $Q = \lambda^{-1}$ (car \K est un corps). $P \cdot Q = \lambda \lambda^{-1} = 1$ et $Q \cdot P = \lambda^{-1} \lambda = 1$ donc $P$ est inversible. Ainsi $\left\{ \lambda X^0, \lambda \in \K^* \right\} \subset \K[X]^\times$.
\end{question_kholle}

\begin{question_kholle}
  [Le problème d'interpolation de Lagrange est, pour $n \in \N$ avec $a \in \K^{n+1}$ et $b \in \K^{n+1}$, l'ensemble des polynômes passant par tous les points de coordonnée $(a_i, b_i)$. C'est-à-dire l'ensemble des {$P \in \K[X]$} vérifiant :
  {\begin{equation}
    \forall i \in [\![0;n]\!], P(a_i) = b_i
  \end{equation}}
  Il existe une unique solution $P$ de degré $\leqslant n$ au problème d'interpolation de lagrange, et elle s'exprime de la manière suivante en posant
  \begin{equation}
    L_{i} = \prod_{\substack{j=0\\j \neq i}}^{n} \frac{X - a_{j}}{a_{i} - a_{j}}
  \end{equation}
  \begin{equation}
    P = \sum_{i=0}^{n}b_{i}L_{i}
  \end{equation}]
  {Théorème d'interpolation de lagrange}
  
  \begin{itemize}
    \item Unicité
    
    Supposons qu'il existe $(P, Q) \in \mathbb{K}_n[X]^{2}$ solutions du problème d'interpolation.
    
    Alors $\forall i \in [ \! [ 0, n ] \!], \tilde{P}(a_{i}) = \tilde{Q}(a_{i}) = b_{i}$
    
    Posons $H = P - Q$, alors, $\forall i \in [ \! [ 0, n ] \!], \tilde{H}(a_{i}) = \tilde{P}(a_{i}) - \tilde{Q}(a_{i}) = 0$.
    
    De plus, $\deg H = \deg(P-Q) \leqslant \max \left\{ \deg P, \deg Q \right\}$
    
    Donc $H$ est un polynôme de degré $\leqslant n$ avec $\lvert [ \! [ 0, n ] \!] \rvert = n+1$ racines.
    
    Donc $H$ est le polynôme nul.
    \item Existence
    Soit $i \in [ \! [ 0, n ] \!]$ fq
    Notons $L_{i}$ une solution de degré $\leqslant n$ au problème $Pb_{i}$ suivant:
    $$
    (Pb_i)
    \left\{ \begin{array}{ll}
      \tilde{P}(a_{0}) = 0   \\
      \vdots                 \\
      \tilde{P}(a_{i-1}) = 0 \\
      \tilde{P}(a_{i}) = 1   \\
      \tilde{P}(a_{n}) = 0   \\
      \vdots                 \\
      \tilde{P}(a_{n}) = 0
    \end{array} \right.
    $$
    On remarque que $(a_{0},\dots ,a_{i-1}, a_{i+1},\dots, a_{n})$ sont $n$ racines deux à deux distinctes de $L_{i}$. Or $L_{i}$ est de degré $\leqslant n$ et n'est pas le polynôme nul (car $\tilde{L_{i}}(a_{i}) = 0$) donc $(a_{0},\dots ,a_{i-1}, a_{i+1},\dots, a_{n})$ sont les \emph{seules} racines de $L_{i}$, toutes simples.
    
    Dès lors,
    $$
    \exists c \in \mathbb{K}^{*} : L_{i} = c\prod_{\substack{j=0 \\ j\neq i}} ^{n}(X-a_{j})
    $$
    Pour trouver le $c$, remarquons que
    
    \begin{align*}
      \tilde{L_{i}}(a_{i}) = 1 & \iff c\prod_{\substack{j=0    \\ j\neq i}} ^{n}(a_{i}-a_{j}) = 1\\
      & \iff c = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{1}{a_{i}-a_{j}} \right)
    \end{align*}
    
    Ainsi, s'il existe une solution au problème $Pb_{i}$ c'est nécéssairement
    $$L_{i} = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{X-a_{j}}}{a_{i}-a_{j}} \right)$$
    
    Réciproquement, cette solution est correcte puisque
    $$\forall k \in [ \! [ 0, n ] \!], k \neq i,  \tilde{L_{i}}(a_{k}) = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{\overbrace{ a_{k}-a_{j} }^{ =0 }}}{a_{i}-a_{j}} \right) = 0$$
    Et
    $$\tilde{L_{i}}(a_{i}) = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{a_{i}-a_{j}}}{a_{i}-a_{j}} \right) = \prod_{\substack{j=0 \\ j\neq i}} ^{n} 1 = 1$$
    
    Posons donc $P = \sum_{i=0} ^{n} b_{i}Li$.
    
    Alors, par construction,
    $$
    \forall k \in [ \! [ 0, n ] \!], \tilde{P}(a_{k}) = \sum_{i=0} ^{n}\left(  b_{i} \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{a_{k}-a_{j}}}{a_{i}-a_{j}} \right) \right) = \sum_{i=0} ^{n}\left(  b_{i} \delta_{ki} \right) = b_{k} \delta_{kk} = b_{k}
    $$
    Nous avons donc construit une solution unique au problème d'interpolation de Lagrange
  \end{itemize}
  
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \pgfmathsetmacro{\a}{2/15}
      \pgfmathsetmacro{\b}{-1.4}
      \pgfmathsetmacro{\c}{64/15}
      \pgfmathsetmacro{\d}{-2}
      
      \def\polynomial(#1){\a*(#1)^3 + \b*(#1)^2 + \c*(#1) + \d}
      
      \draw[->] (-0.5, 0) -- (6.5, 0) node[below] {$x$};
      \draw[->] (0, -0.5) -- (0, 4.5) node[left] {$\tilde P (x)$};
      
      \fill (1, 1) circle (2pt) node[below right] {$(1, 1)$};
      \fill (2, 2) circle (2pt) node[above left] {$(2, 2)$};
      \fill (3.5, 1.5) circle (2pt) node[above right] {$(3.5, 1.5)$};
      \fill (5, 1) circle (2pt) node[below right] {$(5, 1)$};
      
      \draw[thick, smooth, domain=0.4:6.4, samples=100] plot(\x, {\polynomial(\x)});
      
      \draw[gray, dotted] (0,0) grid (6,4);
    \end{tikzpicture}
    \caption{Le polynôme $\frac{2}{15}X^3 - \frac 7 5 X^2 + \frac{64}{15}X - 2$ est l'unique polynôme de degré $\leqslant 3$ passant par les points $\displaystyle \left((1, 1), (2, 2), \left(\frac 7 2, \frac 3 2 \right), (5, 1)\right)$}
  \end{figure}
  
\end{question_kholle}

\begin{question_kholle}
  [Soient $P$ à coefficients dans $\K$ et $a\in \K$. On a :
  \begin{equation}
    P = \sum_{n\in \N}\frac{\widetilde{P^{(n)}}(a)}{n!}(X-a)^n
  \end{equation}
  ]
  {Formule de Taylor dans $\K [X]$ (caractéristique nulle)}
  
  Considérons le prédicat $\PRIME (\cdot)$ défini sur $\N$ par :
  \[
  \PRIME (n ) \ :  \text{\textquotedblleft} \ \forall P \in \K _n[X], \ P = \sum_{k = 0}^n\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^k \ \text{\textquotedblright}
  \]
  Initialisation : pour $n = 0 $, soit $P\in \K _0 [X]$. \\
  $\exists \ p_0 \in \K \ : \ P = p_0 X^0$ et $\sum_{k = 0}^0\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^k = \frac{\widetilde{P^{(0)}}(a)}{1}X^0 = p_0 X^0$, donc $\PRIME (0)$ vrai. \\ \\
  Hérédité : Soit $n\in \N$ tel que $\PRIME (n)$. Soit $P\in \K _{n+1} [X]$. On a donc $\deg P' = \deg P -1 \leq n$ donc $\PRIME (n)$ s'applique à $P'$ :
  \[
  P' = \sum_{k = 0}^n\frac{\widetilde{P'^{(k)}}(a)}{k!}(X-a)^k = \left( \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} \right) ',
  \]
  donc :
  \[
  \left( P - \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} \right) ' = 0 \ \implies \ \exists \ \mu \in \K \ : \ P - \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} = \mu ,
  \]
  ainsi :
  \[
  P = \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{(k+1)!}(X-a)^{k+1} + \mu = \sum_{k = 1}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k} + \mu,
  \]
  donc en $a$ par $\ph _ a$ :
  \[
  \widetilde{P}(a) = \mu \ \implies \ P = \sum_{k = 1}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k} + \widetilde{P}(a) = \sum_{k = 0}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k},
  \]
  donc $\PRIME (n+1) $ vrai. Ainsi par théorème de récurrence sur $\N$, $\PRIME (n)$ est vrai pour tout $n\in \N$.
\end{question_kholle}

\begin{question_kholle}
  [Soit {$P \in \K[X]$}. Soit $a \in \K$.
  \begin{equation}
    a \text{ est une racine de } P \text{ de multiplicité au moins } m
    \iff \left\{ \begin{matrix}
      P(a) = 0  \\
      P'(a) = 0 \\
      \ldots    \\
      P^{(m-1)}(a) = 0
    \end{matrix} \right.
  \end{equation}
  \begin{equation}
    a \text{ est une racine de } P \text{ de multiplicité d'exactement } m
    \iff \left\{ \begin{matrix}
      P(a) = 0         \\
      P'(a) = 0        \\
      \ldots           \\
      P^{(m-1)}(a) = 0 \\
      P^{m}(a) \neq 0
    \end{matrix} \right.
  \end{equation}]
  {Caractérisation de la multiplicité d'une racine}
  
  \begin{enumerate}[label=$\bullet$]
    \item Supposons que $a$ est une racine de $P$ de multiplicité au moins $m$. \\
    Alors $\exists Q \in \K[X] : P = (X-a)^m Q$. D'après la formule de Leibniz, pour tout $k \in [\![0;m-1]\!]$,
    \begin{equation*}
      \begin{aligned}
        P^{(k)}
        & = \sum_{i=0}^{k} \binom{k}{i} \left( (X-a)^m \right) ^{(k-i)} Q^{(i)}       \\
        & = \sum_{i=0}^{k} \binom{k}{i} \frac{m!}{(m-(k-i))!} (X-a)^{m-(k-i)} Q^{(i)} \\
        & = \underbrace{ (X-a)^{(m-k)} }_{\substack{\text{c'est un bien un polynôme}  \\ \text{non constant car } k < m}} \sum_{i=0}^{k} \binom{k}{i} \frac{m!}{(m-(k-i))!} (X-a)^{i} Q^{(i)}
      \end{aligned}
    \end{equation*}
    Donc $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$.
    
    \item Supposons que $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$. \\
    Appliquons la formule de Taylor a.
    \begin{equation*}
      \begin{aligned}
        P & = \sum_{n \in \N} \frac{P^{(n)}(a)}{n!} (X-a)^n                                                   \\
        & = \sum_{n = 0}^{m-1} \underbrace{ \frac{P^{(n)}(a)}{n!} }_{=0} (X-a)^n + \sum_{\substack{n \in \N \\ n \geqslant m}} \frac{P^{(n)}(a)}{n!} (X-a)^n \\
        & = (X-a)^m \sum_{\substack{n \in \N                                                                \\ n \geqslant m}} \frac{P^{(n)}(a)}{n!} \underbrace{(X-a)^{n-m}}_{\in \K[X] \textit{ car } n - m \in \N}
      \end{aligned}
    \end{equation*}
    Donc $(X-a)^m | P$. Donc $a$ est racine de $P$ de multiplicité au moins $m$.
    
    \item Supposons que $a$ est une racine de $P$ de multiplicité exactement $m$. \\
    Nous pouvons appliquer le point précédent car la multiplicité est supérieur à $m$ : $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$. \\
    Par l'absurde, si $P^{(m)}(a) = 0$ alors le point précédent donne que $a$ a une multiplicité supérieur à $m + 1$ donc $m \geqslant m + 1$ ce qui est une contradiction. \\
    Par conséquent, $P^{(m)}(a) \neq 0$.
    
    \item Supposons $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$ et $P^{(m)}(a) \neq 0$. \\
    En reprenant le calcul précédent, pour $k = m$, en sachant que $(X-a)^{(m-k)} = X^0$,
    \begin{equation*}
      P^{(m)} = \binom{m}{0} \frac{m!}{0!} (X-a)^{0} P + \sum_{i=1}^{m} \binom{m}{i} \frac{m!}{i!} (X-a)^{i} Q^{(i)}
    \end{equation*}
    D'où $P^{(m)}(a) = m! \ Q(a)$ donc $Q(a) = \frac{P^{(m)}(a)}{m!}$. Donc $Q(a) \neq 0$. \\
    Par l'absurde, supposons que $(X-a)^{m+1} | P$. Alors $\exists R \in \K[X] : P = (X-a)^{m+1} R$. Donc $(X-a)^{m+1} R = (X-a)^m Q$ d'où $Q = (X-a) R$. Nous obtenons $Q(a) = 0$ ce qui est une contradiction avec $Q(a) = 0$. \\
    Donc $a$ est une racine de $P$ de multiplicité strictement inférieur à $m + 1$ et, d'après le point précédent, supérieur à $m$. Donc $a$ est une racine de $P$ de multiplicité exactement $m$.
  \end{enumerate}
\end{question_kholle}

\begin{question_kholle}
  []
  {Identification de $\K[X]$ à $\K [x]$, par l'injectivité de $\Phi$}
  Montrons que l'application $\Phi$ définie comme suit est injective :
  \[
  \Phi : \left|   \begin{array}{ccc}
    \K[X] & \longrightarrow & \mathcal{F}(\K, \K) \\
    P     & \longmapsto     & \widetilde{P}
  \end{array} \right..
  \]
  Soit donc $P\in \ker \Phi $, on a :
  \[
  \Phi (P)  = \widetilde{0} \ \implies \ \widetilde{P} = \widetilde{0} \text{ sur } \K \ \implies \ P = 0_{\K [X]},
  \]
  donc $\ker \Phi \subset \{ 0_{\K [X]} \}$. \\
  Réciproquement, on calcule l'image du polynôme nul par $\Phi$ :
  \[
  \Phi (0 _{\K [X]} ) = \widetilde{0},
  \]
  donc $0_{\K [X]} \in \ker \Phi$, ainsi on a l'égalité ensembliste et donc cela suffit.
\end{question_kholle}

\begin{question_kholle}
  [Les fonctions symétriques élémentaires $\displaystyle \left( \sigma_k \right)_{k \in [\![0;n]\!]}$ pour une famille $\displaystyle \left( x_k \right)_{k \in [\![1;n]\!]}$ sont définies par
  \begin{equation}
    \sigma_ k = \sum_{1 \leqslant i_1 < \ldots < i_k \leqslant n} \ \prod_{j=1}^{k} x_{i_j}
  \end{equation}]
  {Pour $P = (X-x_1)(X-x_2)(X-x_3)$, exprimer $x_1^3 + x_2^3 + x_3^3$ en fonction des fonctions symétriques élémentaires}
  
  Sous forme développée, $P = X^3 - (x1 + x_2 + x_3) X^2 + (x_1 x_2 + x_1 x_3 + x_2 x_3) X - x_1 x_2 x_3 = X^3 - \sigma_1 X^2 + \sigma_2 X - \sigma_3$. Comme $x_1, x_2, x_3$ sont racines de $P$, nous avons les trois égalité suivantes :
  \begin{equation*}
    \begin{aligned}
      0 = P(x_1) & = x_1^3 - \sigma_1 x_1^2 + \sigma_2 x_1 - \sigma_3 \\
      0 = P(x_1) & = x_2^3 - \sigma_1 x_2^2 + \sigma_2 x_2 - \sigma_3 \\
      0 = P(x_1) & = x_3^3 - \sigma_1 x_3^2 + \sigma_2 x_3 - \sigma_3
    \end{aligned}
  \end{equation*}
  En sommant ces trois équation,
  \begin{equation*}
    0 = x_1^3 + x_2^3 + x_3^3 - \sigma_1 (x_1^2 + x_2^2 + x_3^2) + \sigma_2 (x_1 + x_2 + x_3) - 3 \sigma_3
  \end{equation*}
  Cherchons la somme des carrés.
  \begin{equation*}
    \begin{aligned}
      (x_1 + x_2 + x_3)^2                       & = x_1^2 +  x_2^2 + x_3^2 + 2 x_1 x_2 + 2 x_1 x_3 + 2 x_2 x_3 \\
      \implies x_1^2 +  x_2^2 + x_3^2 + x_1 x_2 & = \sigma_1^2 - 2 \sigma_2
    \end{aligned}
  \end{equation*}
  Ainsi \begin{equation*}
    x_1^3 + x_2^3 + x_3^3 = \sigma_1^3 - 3 \sigma_1 \sigma_2 + 3 \sigma_3
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}
  [Les sommes de Newton $(S_k)_{k\in\Z^*}$ pour une famille $(x_k)_{k \in \N^*}$ sont définies par (sous réserve d'existence pour $k<0$) :
  \begin{equation}
    S_k = \sum_{i=1}^{n} x_i^k
  \end{equation}]
  {Expression de $S_2$, $S_{-1}$ et $S_{-2}$ à l'aide des fonctions élémentaires symétriques.}
  
  \begin{equation*}
    \begin{aligned}
      \sigma_1^2   & = \left( \sum_{i=1}^{n} x_i \right)^2                                                                                      \\
      & = \underbrace{ \sum_{i=1}^{n} x_i^2 }_{S_2} + \ 2 \ \underbrace{ \sum_{1 \leqslant i < j \leqslant n} x_i x_j }_{\sigma_2} \\
      \implies S_2 & = \sigma_1^2 - 2 \sigma_2
    \end{aligned}
  \end{equation*}
  \begin{equation*}
    S_{-1} = \sum_{i=1}^{n} \frac{1}{x_i}
    = \frac{\displaystyle \sum_{i=1}^{n} \prod_{\substack{ j = 1 \\ j \neq i }}^{n} x_j }{\displaystyle \prod_{i=1}^{n} x_i }
    = \frac{\sigma_{n-1}}{\sigma_n}
  \end{equation*}
  \begin{equation*}
    \begin{aligned}
      S_{-2} & = \sum_{i=1}^{n} \frac{1}{x_i^2}                                                                                         \\
      & = \left( \sum_{i=1}^{n} \frac{1}{x_i} \right)^2 - 2 \sum_{1 \leqslant i < j \leqslant n} \frac{1}{x_i} \frac{1}{x_j}     \\
      & = \frac{\sigma_{n-1}^2}{\sigma_n^2} - 2 \frac{\displaystyle \sum_{1 \leqslant i < j \leqslant n} \prod_{\substack{ k = 1 \\ k \notin \{i,j\} }} \frac{1}{x_j} }{\sigma_n} \\
      & = \frac{\sigma_{n-1}^2 - 2 \sigma_{n-2}\sigma_n}{\sigma_n^2}
    \end{aligned}
  \end{equation*}
\end{question_kholle}
\pagebreak\section{Semaine 21}
\begin{question_kholle}
  [Tous les polynômes de degré 1 sont irréductibles, les polynômes irréductibles de degré $2$ ou $3$ sont les polynômes sans racine.s dans le corps de base.]
  {Caractérisation des polynômes irréductibles de degré 1, 2 et 3 dans \cx{\K}.}

  Un polynôme de degré $1$ ne peut s'écrire comme produit de 2 polynômes de \\ degré $\geq 1$ donc il est irréductible. \\
  Soit $P \in \cx{\K}$ un polynôme irréductible de degré $2$ ou $3$. \\
  Par définition, $P$ n'a pas de racine.s dans $\K$, donc la première inclusion. \\
  Soit $P\in \K [X]$ tel que $\deg P = 2$. \\
  Montrons que si $P$ n'a pas de racine dans \K alors $P$ est irréductible. Montrons la contraposée. Supposons $P$ non-irréductible. \\
  \[
    \exists \ A, \ B \in \K [X] \ : \ P = AB \ \text{et} \ \deg A, \ \deg B \geq 1,
  \]
  On a alors, $P=AB \ \implies \ 2 = \deg A + \deg B \ \implies \ \deg A, \ \deg B = 1$ donc :
  \[
    \exists \ \alpha, \ \gamma \in \K^* \times \K \ : \ A = \alpha X + \gamma,
  \]
  ainsi, $P=( \alpha X + \gamma )B= \alpha \left( X + \frac{\gamma}{\alpha} \right)B $, donc $P$ admet $-\frac{\gamma}{\alpha}\in \K$ comme racine, ce qui montre la contraposée.\\
  Soit $P\in \K [X]$ tel que $\deg P = 3$.\\
  Montrons, de même, la contraposée. Supposons $P$ non-irréductible. De même, on a :
  \[
    \exists \ A, \ B \in \K [X] \ : \ P = AB \ \text{et} \ \deg A, \ \deg B \geq 1,
  \]
  Puis encore, $P=AB \ \implies \ 3 = \deg A + \deg B \ \implies \ \deg A, \ \deg B \in \{2,1\}$ (l'un n'étant pas l'autre). Donc l'un des deux est de degré 1 donc $P$ admet une racine dans $\K$, donc encore une fois cela montre la contraposée, ce qui démontre l'inclusion réciproque.
\end{question_kholle}

\begin{question_kholle}
  [Les polynômes irréductibles de $\cx{\C}$ sont les polynômes de degré $1$ et ceux de $\cx{\R}$ sont les polynômes de degré $1$ et les polynômes de degré $2$ de discriminant strictement négatif. ]
  {Polynômes irréductibles de $\C [X]$ et de $\R [X]$.}

  Le premier point est immédiat, les polynômes irréductibles d'un corps contiennent les polynômes de degré $1$ et par le théorème de D'Alembert-Gauss, tout polynôme de $\C [X]$ ($\deg \geq 2$) est scindé dans $\C [X]$, donc non-irréductible. \\ \\
  Pour le second point, le cas du degré $1$ est réglé. Soit $P$ un polynôme irréductible de $\R [X]$. \\
  Supposons que $P$ soit de degré supérieur ou égal à $3$. Si son degré est impair, le TVI conclut quant à l'existence d'une racine, donc non-irréductible. Si son degré est pair, par D'Alembert-Gauss, on obtient $\deg P$ couples de racines possiblement égaux. \\
  Or, $P\in \cx{\R}$ donc $\forall z \in \C, \ P(z) = 0 \ \implies P(\overline{z}) = 0 $ donc les racines se rassemblent 2 à 2 pour former un polynôme scindé dans $\R$, donc non-irréductible. Ainsi, $\deg P = 2$, immédiatement, si le discriminant de $P$ est positif ou nul, $P$ admet une ou deux racines dans $\R$, donc non irréductible. Enfin, son discriminant est alors négatif, de cette manière $P$ n'admet pas de racine dans $\R$ et est donc irréductible. Ce qui achève la preuve.
\end{question_kholle}

\begin{question_kholle}
  [Il s'agit donc de montrer que racine cubique de $2$ n'est pas un rationnel.]
  {$X^3 -2$ est irréductible dans $\cx{\Q}$.}

  Supposons, par l'absurde, qu'il existe $r\in \Q$ tel que $r^3 - 2 = 0 $. Prenons $p,q \in \Z \times \N^*$ \textit{le} représentant irréductible de $r$ dans $\Q$. On a alors, $p^3 = 2q^3$ donc $2\ | \ p^3$ or $2\in \PRIME$ donc $2\ | \ p$, ainsi, il existe $k\in \Z$ tel que $p = 2k$. Par conséquent, $2(2k^3) = q^3$ donc $2 \ | \ q^3$ or $2 \in \PRIME$ donc $2\ |\ q $ donc ceci contredit $p$ et $q$ premiers entre eux, par définition d'un représentant irréductible. Ainsi, $P = X^3 -2$ n'admet pas de racine dans \Q, c'est donc un polynôme irréductible.
\end{question_kholle}

\begin{question_kholle}
  [Pour $\displaystyle P = \prod_{k=1}^p(X-z_k)^{m_k} \in \cx{\C} \setminus \{0_{\C[X]}\}$ avec $m_k \in \N^*$ pour tout $k\in \lient 1,p \rient$, on a
    \begin{equation}
      P \wedge P' = \prod_{k=1}^p(X-z_k)^{m_k - 1}
    \end{equation}
    C'est une conséquence de la définition du pgcd de deux polynômes $P \wedge Q = \prod_{i\in I}P_i^{\min \{ m_i, p_i\} }$, où les $P_i$ sont les facteurs irréductibles de $P$ et $Q$ dans leur décomposition. ]
  {PGCD d'un polynôme de \cx{\C} et son polynôme dérivé}

  Soit $P$ un tel polynôme et $p$ un entier naturel non nul. Naturellement, $P'$ hérite de $P$, $\deg P - p$ racines, lesquelles sont les $z_k$ pour $k\in \lient 1 , p \rient $, de multiplicité $m_k -1$. Ainsi,
  \[
    \exists \ B \in \C [X] \ : \ \left[ P' = \left( \prod_{k=1}^p(X-z_k)^{m_k -1} \right) B \right] \ \wedge \ \left[  \deg B = p  \right],
  \]
  de cette manière on peut écrire :
  \[
    P' = \left( \left( \prod_{k=1}^p(X-z_k)^{m_k -1} \right) B \right) P^0 \ \text{et} \ P = \left( \prod_{k=1}^p(X-z_k)^{m_k} \right) (P')^0,
  \]
  de façon à faire apparaître dans les deux décompositions les mêmes facteurs, possiblement avec une puissance $0$, histoire de coller à la définition de manière explicite. Ceci fait, il ne reste plus qu'à appliquer la définition du pgcd et de remarquer que seuls les $(X-z_k)^{m_k -1}$ subsistent. \\ Notons $\mathfrak{I}$ l'ensemble des facteurs de leur décomposition, on a alors :
  \[
    P \wedge P' = \prod_{D\in \mathfrak{I}}D ^{\min \{ \nu_D (P), \nu _D(P') \}} = \prod_{k=1}^p (X-z_k)^{m_k -1},
  \]
  où $\nu _D (\cdot ) $ est la valuation $D$-adique au sens des polynômes irréductibles. Ce qui conclut.
\end{question_kholle}

\begin{question_kholle}
  [Il s'agit là de vérifier que la définition que l'on souhaiterait le plus, c'est-à-dire la même que pour la dérivée d'une fraction de fonctions, s'applique effectivement aux fractions rationnelles, c'est-à-dire que cette définition ne dépend pas du représentant choisi.]
  {Justifier la bonne définition de la dérivée d'une fraction rationnelle.}
  Montrons que pour $A, \ B \in \K [X] \times \K [X] \backslash \{ 0 _{ \K[X]} \}$, on a :
  \[
    \left( \frac{A}{B} \right) ' = \frac{A'B - B'A}{B^2}.
  \]
  Soient $A$ et $B$ de tels polynômes et $C, \ D \in \K [X] \times \K [X] \backslash \{ 0 _{ \K[X]} \}$ tels que $AD = BC$, en dérivant on obtient $A'D + D'A = B'C + C'B$. Calculons :
  \begin{center}
    $
      \begin{array}{rcl}
        (A'B-B'A)D^2 & = & D(A'BD- (AD)B')  \\
                     & = & D(A'BD- BCB')    \\
                     & = & BD(A'D- CB')     \\
                     & = & BD(C'B - D'A)    \\
                     & = & B(C'BD - (AD)D') \\
                     & = & B(C'BD - BCD')   \\
                     & = & B^2(C'D - D'C),
      \end{array}
    $
  \end{center}
  ce qui prouve que le résultat ne dépend pas du représentant, par définition de $\K(X)$ comme structure quotient.
\end{question_kholle}

\begin{question_kholle}
  [Les racines du polynôme dérivée sont dans l'enveloppe convexe des racines du polynôme. \\
    Soit {$P\in \C[X]$} de degré au moins $2$ et notons $z_1, \dots, z_n$ ses racines répétées avec multiplicité. \\
    Soit $u$ une racine de $P'$.
    Alors :
    \begin{equation}
      \exists \ (c_1, \dots, c_n)\in \R ^*_+ \ : \ \sum_{k=1}^n c_k z_k = u \ \text{et} \ \sum_{k=1}^n c_k = 1.
    \end{equation}]
  {Théorème de Gauss-Lucas et interprétation graphique.}
  \begin{itemize}[label=$\star$]
    \item Si $u$ est une racine de $P$ alors noter $k_0$ son indice et utiliser le symbole de Kronecker. 
  $$ \sum_{k=1}^n \delta_{k, k_0} z_k = u \ \text{et} \ \sum_{k=1}^n \delta_{k, k_0} = 1 $$
  \item Sinon, $u$ n'appartient pas aux racines de $P$, donc $u$ n'est pas pôle de $\frac{P'}{P}$ ce qui permet de prendre l'image par le morphisme d'évaluation en $u$ de cette même fraction rationnelle :
  \[
    0_\K = \frac{P'(u)}{P(u)} = \sum_{k = 1}^n \frac{1}{u - z_k} = \sum_{k = 1}^n \frac{\overline{u}- \overline{z_k}}{|u-z_k|^2} = \sum_{k = 1}^n \frac{\overline{u}}{|u-z_k|^2}- \sum_{k = 1}^n \frac{\overline{z_k}}{|u-z_k|^2}.
  \]
  Donc en passant la seconde somme à gauche et en prenant le conjugué :
  \[
    \sum_{k = 1}^n\frac{u}{|u-z_k|^2} = \sum_{k = 1}^n \frac{z_k}{|u-z_k|^2} \ \implies \ u = \frac{\sum_{k = 1}^n \frac{z_k}{|u-z_k|^2}}{\sum_{k = 1}^n\frac{1}{|u-z_k|^2}}= \sum_{k = 1}^n\underset{= \ c_k}{\underbrace{\frac{\frac{1}{|u-z_k|^2}}{\sum_{i = 1}^n\frac{1}{|u-z_i|^2}}}} z_k = \sum_{k=1}^n c_k z_k ,
  \]
  ce qui démontre la première partie du résultat, il est immédiat de vérifier que $\sum_{k=1}^nc_k = 1$, vérification laissée aux lecteurs. Ce qui achève la preuve. 
\end{itemize}
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      % Draw the axes
      \draw[->] (-3, 0) -- (3, 0) node[anchor=north west] {Re};
      \draw[->] (0, -3) -- (0, 3) node[anchor=south east] {Im};
  
      % Define and draw the 7 scattered points with labels
      \coordinate (A) at (-2, 1);
      \coordinate (B) at (2, 1.5);
      \coordinate (C) at (1, -2);
      \coordinate (D) at (-1.5, -1);
      \coordinate (E) at (-0.5, 2);
      \coordinate (F) at (0.5, -1.5);
      \coordinate (G) at (1.5, 0.5);
  
      % Plot the points and add labels
      \filldraw[blue] (A) circle (1pt) node[anchor=east] {$r_1$};
      \filldraw[blue] (B) circle (1pt) node[anchor=west] {$r_2$};
      \filldraw[blue] (C) circle (1pt) node[anchor=north] {$r_3$};
      \filldraw[blue] (D) circle (1pt) node[anchor=east] {$r_4$};
      \filldraw[blue] (E) circle (1pt) node[anchor=south] {$r_5$};
      \filldraw[blue] (F) circle (1pt) node[anchor=south east] {$r_6$};
      \filldraw[blue] (G) circle (1pt) node[anchor=north] {$r_7$};
  
      % Draw the corrected convex hull outline
      \draw[red, thin, dashed] (A) -- (E) -- (B) -- (C) -- (D) -- (A);
  
      \coordinate (H) at (-1.33618, 0.780693);
      \coordinate (I) at (-0.819814, -0.72006);
      \coordinate (J) at (-0.256534, 1.22291);
      \coordinate (K) at (0.70267, -0.260152);
      \coordinate (L) at (0.822502, -1.77882);
      \coordinate (M) at (1.7445, 1.18401);
  
      \filldraw[orange] (H) circle (1pt) node[anchor=east] {$r'_1$};
      \filldraw[orange] (I) circle (1pt) node[anchor=west] {$r'_2$};
      \filldraw[orange] (J) circle (1pt) node[anchor=north] {$r'_3$};
      \filldraw[orange] (K) circle (1pt) node[anchor=east] {$r'_4$};
      \filldraw[orange] (L) circle (1pt) node[anchor=south] {$r'_5$};
      \filldraw[orange] (M) circle (1pt) node[anchor=east] {$r'_6$};
  
    \end{tikzpicture}
  \end{figure}
    
    \begin{itemize}[label=$\star$]
      \item En {\color{blue} bleu}, les racines du polynôme\\$P = (X - (-2 + i))(X - (2 + 1.5i))(X - (1 -2i))(X - (-1.5 - i))(X - (-0.5 + 2i))(X - (0.5 - 1.5i))(X- (1.5 + 0.5i))$\\
        \item Délimitée en {\color{red} rouge}, l'enveloppe convexe des racines.\\
        \item En {\color{orange} orange}, les racines du polynôme dérivé\\
         $P' = (21.8125 + 40.1875 i) - (3.5 + 21.375 i) X + (7.125 - 25.125 i) X^2 + (16 - 31.5 i) X^3 + (3.75 + 5 i) X^4 - (6 + 3 i) X^5 + 7 X^6$\\
    \end{itemize}
    Les racines de $P'$ se retrouvent bien dans l'enveloppe convexe des racines de $P$.
\end{question_kholle}

\begin{question_kholle}
  {Deux expressions du coefficient associé à un pôle simple dans une décomposition en éléments simples.}

  Soient $(P, Q) \in \K [X] \times \left( \K [X] \backslash \{ 0_{\K[X] }\} \right) $ tels que la fraction rationnelle $\frac{P}{Q}$ soit \\ irréductible et en prenant $\deg P < \deg Q$. En appliquant le théorème de décomposition en éléments simples, on obtient un expression de la forme :
  \[
    \exists \ R \in \K (X) \ : \ \frac{P}{Q} = \sum_{k=1}^n \frac{a_k}{X - z_k} + R,
  \]
  où les $z_k$ pour $k\in \lient 1 ,n \rient$ sont racines de $Q$. Ainsi, en prenant $k_0 \in \lient 1, n \rient $ tel que $z_{k_0}$ soit racine simple,
  \[
    \frac{P(X-z_{k_0})}{Q} = a _ {k_0} + \sum_{k=0 \ \text{et} \ k \neq k_0} \frac{a_k(X-z_{k_0})}{X - z_{k_0}} + R(X - z_{k_0}),
  \]
  une première expression se trouvera en notant $\widetilde{Q} = \displaystyle \prod_{\substack{k=1 \\ k\neq k_0}}^n(X-z_k)^{\nu_{(X-z_k)}(Q)}$, on a alors :
  \[
    \frac{P(z_{k_0})}{\widetilde{Q}(z_{k_0})} = a_{k_0}.
  \]
  Une autre expression est possible en explicitant $\widetilde{Q}$. Pour ce faire, remarquons plutôt :
  \[
    Q' = \sum_{k=1}^n \nu_{(X-z_k)}(Q) (X- z_k)^{\nu_{(X-z_k)}(Q) -1}\prod _{\substack{i = 1 \\ i \neq k}}^n(X-z_i)^{\nu_{(X-z_i)}(Q)},
  \]
  donc en prenant l'image par le morphisme d'évaluation en $z_{k_0}$ on obtient :
  \[
    Q'(z_{k_0}) = \prod _{\substack{i = 1 \\ i \neq k_0}}^n(z_{k_0}-z_i)^{\nu_{(X-z_i)}(Q)},
  \]
  il s'agit exactement de $\widetilde{Q}(z_{k_0})$. Ainsi,
  \begin{equation}
    \frac{P(z_{k_0})}{Q'(z_{k_0})} =a_{k_0},
  \end{equation}
  ce qui suffit.
\end{question_kholle}

\begin{question_kholle}
  []
  {Expressions des deux coefficients associés à un pôle double dans une décomposition en éléments simples.}
  Soient $(P, Q) \in \K [X] \times \left( \K [X] \backslash \{ 0_{\K[X] }\} \right) $ tels que la fraction rationnelle $\frac{P}{Q}$ soit \\ irréductible et en prenant $\deg P < \deg Q$. En appliquant le théorème de décomposition en éléments simples on obtient un expression de la forme suivante en considérant $z_{k_0}$, une racine double de $Q$ :
  \[
    \exists \ R \in \K (X) \ : \ \frac{P}{Q} = \frac{a_1}{X - z_{k_0}}  + \frac{a_2}{(X - z_{k_0})^2} + R \quad \quad (\star)
  \]
  puis de même,
  \[
    \frac{P(X - z_{k_0})^2}{Q} = a_2  + \left( \frac{a_1}{X - z_{k_0}} + R \right)(X - z_{k_0})^2,
  \]
  donc en notant $\widetilde{Q} = \displaystyle \prod_{\substack{k=1 \\ k\neq k_0}}^n(X-z_k)^{\nu_{(X-z_k)}(Q)}$, on a :
  \[
    \frac{P(z_{k_0})}{\widetilde{Q}(z_{k_0})} = a_2,
  \]
  c'est une première expression. Pour la suivante, encore une fois, explicitons $\widetilde{Q}$. Remarquons que :
  \[
    \exists \ A \in \K [X] \ : \ \left[ Q'' = 2\prod_{\substack{k=1 \\ k \neq k_0}}^n (X-z_k)^{\nu_{(X-z_k)}(Q)} + A \right] \ \wedge \ \left[ A(z_{k_0} ) = 0 \right],
  \]
  donc, en remarquant que :
  \[
    2\widetilde{Q}(z_{k_0}) = Q''(z_{k_0}),
  \]
  on a finalement :
  \[
    \frac{2P(z_{k_0})}{Q''(z_{k_0})} = a_2.
  \]
  Pour récupérer $a_1$, on multiplie $(\star )$ par $(X- z_{k_0})^2$ puis on dérive :
  \[
    \left( \frac{P  ( X - z_{k_0})^2}{Q}\right)' =  a_1 + R'(X-z_{k_0})^2 + 2R(X - z_{k_0}),
  \]
  soit,
  \[
    \frac{((P'(X - z_{k_0})^2 + 2P(X-z_{k_0}))Q - Q'P(X-z_{k_0})^2 }{Q^2}  = a_1 + R'(X-z_{k_0})^2 + 2R(X - z_{k_0})
  \]
\end{question_kholle}

\pagebreak\section{Semaine 22}

Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$, $I$ un ensemble quelconque non vide.

\begin{question_kholle}
  [Une famille est liée si et seulement si l'un de ses vecteurs est une combinaison linéaires d'autres vecteurs de la famille.
    \begin{equation}
      (x_i)_{i \in I} \text{ est liée}
      \iff \exists i_0 \in I : \exists (\lambda_i)_{i \in I\setminus\{i_0\}} \in \K^{\left( I \setminus \{i_0\} \right)} :
      x_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i
    \end{equation}]
  {Caractérisation d'une famille liée}

  Supposons que $(x_i)_{i \in I}$ est liée. \\
  Par définition, $\displaystyle \exists (\mu_i) \K^{(I)} :
    \left\{ \begin{array}{ccc}
      \sum_{i \in I} \mu_i x_i & = 0_E                 \\
      (\mu_i)_{i \in I}        & \neq (0_\K)_{i \in I}
    \end{array} \right.$ \\
  Donc $\exists i_0 \in I : \mu_{i_0} \neq 0_\K$. Fixons un tel $i_0$. \\
  $\displaystyle \mu_{i_0} x_{i_0} + \sum_{i \in I \setminus \{i_0\}} \mu_i x_i = 0_E$ \\
  Or $\mu_{i_0} \neq 0$, donc $\displaystyle x_{i_0} = \sum_{i \in I \setminus \{i_0\}} \left( \mu_{i_0}^{-1} \times (-\mu_i) \right) \ldotp x_i$. \\
  En posant $\lambda_i = \mu_{i_0}^{-1} \times (-\mu_i)$, on obtient $x_{i_0} = \displaystyle \sum_{i \in I \setminus \{i_0\}} \lambda_i \ldotp x_i$.

  Supposons maintenant que $\exists i_0 \in I : \exists (\lambda_i)_{i \in I\setminus\{i_0\}} \in \K^{\left( I \setminus \{i_0\} \right)} :
    x_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i$. \\
  Alors $-x_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i = 0_E$.
  Posons $\mu_{i_0} = - 1_\K$ et $\forall i \in I \!\setminus\! \{i_0\}, \mu_i = \lambda_i$.
  Ainsi, $(\mu_i)_{i \in I} \in \K^{(I)}$ et $\sum_{i \in I} \mu_i \ldotp x_i = 0_\K$. Or $\mu_{i_0} \neq 0_\K$ donc $(\mu_i)_{i \in I} \neq (0_\K)_{i \in I}$. \\
  Donc $(\mu_i)_{i \in I}$ est liée.
\end{question_kholle}

\begin{question_kholle}
  [Soit $\mathcal{F}$ une famille de vecteurs de $E$. Les propositions suivantes sont équivalentes :
    \begin{propositions}
      \item $\mathcal{F}$ est une base.
      \item Tout vecteur de $E$ se décompose de manière unique dans $\mathcal{F}$.
      \item $\mathcal{F}$ est génératrice minimale (au sens de l'inclusion)
      \item $\mathcal{F}$ est libre maximale (au sens de l'inclusion)
    \end{propositions}]
  {Caractérisations d'une base}

  Notons $(e_i)_{i \in I}$ la famille $\mathcal{F}$.


  $(i) \implies (ii)$ Supposons que $\mathcal{F}$ est une base de $E$. \\
  Soit $x \in E$ \fq. Montrons que $x$ s'écrit de manière unique comme une combinaison linéaire des vecteurs de $\mathcal{F}$. \\
  $\mathcal{F}$ est une base donc elle est une famille génératrice et libre de $E$. La propriété génératrice donne, par définition, l'existence d'une telle écriture tandis que la propriété libre donne l'unicité d'une telle écriture.

  $(ii) \implies (iii)$ Supposons que tout vecteur de $E$ s'écrit de manière unique comme une combinaison linéaire de vecteurs de $\mathcal{F}$. \\
  L'existence d'un telle décomposition permet d'affirmer que $\mathcal{F}$ est génératrice. \\
  Supposons que $\mathcal{F}$ ne soit pas génératrice minimale c'est-à-dire qu'il existe une famille $\mathcal{F}'$ de vecteurs de $E$ telle que $\mathcal{F}' \subsetneq \mathcal{F}$ et $\mathcal{F}'$ engendre $E$. \\
  Alors $\exists i_0 \in I : e_{i_0} \notin \mathcal{F}'$. Comme $\mathcal{F}'$ est génératrice, $\exists (\lambda_i)_{i \in I \setminus \{i_0\}} \in \K^{(I \setminus \{i_0\})} : e_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp e_i$.
  Donc \begin{equation*}
    \begin{aligned}
      e_{i_0} & = 0_\K \ldotp e_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp e_i \\
      e_{i_0} & = 1_\K \ldotp e_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} 0_\K \ldotp e_i
    \end{aligned}
  \end{equation*}
  $e_{i_0}$ peut donc s'écrire de deux manières différentes au moins comme combinaison linéaire de vecteurs de $\mathcal{F}$ ce qui contredit le caractère libre de $\mathcal{F}$. \\
  Par conséquent, $\mathcal{F}$ est génératrice et minimale parmi les familles génératrices.

  $(iii) \implies (iv)$ Supposons que $\mathcal{F}$ est une famille génératrice minimale.
  Par l'absurde, supposons que $\mathcal{F}$ est liée. Alors il existe un $i_0 \in I$ tel que $e_{i_0}$ s'écrit comme une combinaison linéaire d'autres vecteurs de $\mathcal{F}$ donc $(e_i)_{i \in I \setminus \{i_0\}}$ est génératrice de $E$ . Or cette famille est strictement incluse dans $\mathcal{F}$ ce qui contredit la propriété de génératrice minimale. \\
  Donc $\mathcal{F}$ est libre. \\
  Par l'absurde, supposons que $\mathcal{F}$ n'est pas libre maximale c'est-à-dire qu'il existe une famille $\mathcal{F}'$ de vecteurs de $E$ telle que $\mathcal{F} \subsetneq \mathcal{F}'$ et $\mathcal{F}'$ est libre. \\
  Alors $\exists x \in \mathcal{F}' : x \notin \mathcal{F}$. Or $\mathcal{F}$ est génératrice d'où :
  \begin{equation*}
    \exists \bdak :
    x = \sum_{i \in I} \lambda_i \ldotp x_i
    = 0_\K \ldotp x + \sum_{i \in I} \lambda_i \ldotp x_i + \sum_{\substack{y \in \mathcal{F}' \\ y \notin \mathcal{F} \\ y \neq x}} 0_\K \ldotp y
  \end{equation*}
  Puisque $x \in \mathcal{F}'$,
  \begin{equation*}
    \exists \bdak :
    x = 1_\K \ldotp x + \sum_{i \in I} 0_\K \ldotp x_i + \sum_{\substack{y \in \mathcal{F}' \\ y \notin \mathcal{F} \\ y \neq x}} 0_\K \ldotp y
  \end{equation*}
  Donc $x$ s'écrit de deux manières différentes au moins comme combinaison linéaire de vecteurs $\mathcal{F}'$, ce qui contredit la liberté de $\mathcal{F}'$. \\
  Par conséquent, $\mathcal{F}$ est libre maximale.

  $(iv) \implies (i)$ Supposons que $\mathcal{F}$ est une famille libre maximale. \\
  Par hypothèse même, $\mathcal{F}$ est libre.
  Par l'absurde, supposons que $\mathcal{F}$ n'est pas génératrice. Alors il existe $x \in E$ tel que $x \notin \Vect \mathcal{F}$. Donc $\mathcal{F} \wedge \{x\}$ est libre et contient strictement $\mathcal{F}$, ce qui contredit la propriété de liberté maximale. \\
  Par conséquent, $\mathcal{F}$ est aussi génératrice, donc une base.

  \begin{equation*}
    \begin{matrix}
      (i)      & \!\!\!\implies\!\!\!   & (ii)       \\
      \Uparrow &                        & \Downarrow \\
      (iv)     & \!\!\!\impliedby\!\!\! & (iii)      \\
    \end{matrix}
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}
  [Soit $f \in \mathcal{L}_\K(E, F)$.
    \begin{equation}
      \begin{aligned}
        \ker f & = \left\{ x \in E \;|\; f(x) = 0_F \right\} = f^{-1} (\{0_F\}) \\
        \Im f  & = \left\{ y \in F \;|\; \exists x \in E : f(x) = y \right\}
      \end{aligned}
    \end{equation}
    Nous démontrerons le résultat plus général suivant :
    \begin{propositions}
      \item $f(E')$ est un \sev de $F$.
      \item $f^{-1}(F')$ est un \sev de $E$.
    \end{propositions}
  ]
  {Le noyau et l'image d'une application linéaire sont des \sevs}

  $(i)$ $0_E \in E'$ et $f(0_E) = 0_F$ donc $0_F \in f(E')$ d'où $f(E') \neq \emptyset$ \\
  Soit $(\alpha, \beta, y, y') \in \K^2 \times f(E')^2$ \fqs. \\
  Par définition, $\exists (x, x') \in E'^2 : f(x) = y \wedge f(x') = y$.
  \begin{equation*}
    \begin{aligned}
      \alpha y + \beta y'
       & = \alpha f(x) + \beta f(x')                                                                     \\
       & = f( \alpha x + \beta x' ) \quad \text{ car } f \in \mathcal{L}_\K(E, F)                        \\
       & \in f(E') \quad \text{ car } \alpha x + \beta x' \in E' \text{ puisque } E' \text{ est un \sev}
    \end{aligned}
  \end{equation*}
  Donc $f(E')$ est un \sev.

  $(ii)$ $0_F \in F'$ et $f(0_E) = 0_F$ donc $0_E \in f^{-1}(F')$ d'où $f(F') \neq \emptyset$ \\
  Soit $(\alpha, \beta, x, x') \in \K^2 \times f^{-1}(F')^2$ \fqs. \\
  Par définition, $\exists (y, y') \in F'^2 : f(x) = y \wedge f(x') = y$. \\
  Or $F'$ est \sev donc $\alpha y + \beta y' \in F'$. $f \in \mathcal{L}_\K(E, F)$ d'où $f(\alpha x + \beta x') = \alpha y + \beta y'$. Donc $\alpha x + \beta x' \in f^{-1}(F')$. \\
  Ainsi, $f^{-1}(F')$ est un \sev.

  En appliquant pour $E' = E$ et $F' = \{0_F\}$, nous obtenons que $\ker f$ et $\Im f$ sont des \sevs.
\end{question_kholle}


\begin{question_kholle}
  [{Soient $(E,F)$ deux $\mathbb{K}$-espaces vectoriels $f \in \mathcal{L}_\K(E, F)$, $\mathcal{F}=(x_{i})_{i\in I}$ une base de $E$.

        Alors \begin{equation}
          \text{Vect} \!\!\!\! \underbrace{ f(\mathcal{F}) }_{ \left\{ f(x_{i}) \mid i \in I \right\}  } = f(\Vect\mathcal{ F})
        \end{equation}
      }]
  {L’image par une application linéaire d’une partie génératrice engendre l’image de l’application linéaire}
  Soit $y \in \text{Vect}f(\mathcal{F})$
  Alors $\exists (\lambda_{i})_{i \in I} \in \mathbb{K}^{(I)}$ tel que $y = \sum_{i \in I} \lambda_{i}f(x_{i})$
  Mais
  \begin{align*}
    y & =  \sum_{i \in I} \lambda_{i}f(x_{i})                                                           \\
      & = f\left( \sum _{i \in I} \lambda _{i} x_{i} \right) \implies y \in f(\text{Vect}{\mathcal{F}})
  \end{align*}


  Réciproquement soit $y \in f(\text{Vect}{\mathcal{F}})$ fq.
  $$\exists x \in \text{Vect} \mathcal{F} : f(x) = y \implies \exists (x_{i})_{i \in I} : x = \sum_{i \in I} \lambda_{i}x_{i}$$
  Donc:

  \begin{align*}
    y = f(x) & = f\left( \sum_{i \in I} \lambda_{i} x_{i} \right)                     \\
             & = \sum _{i \in I} \lambda_{i} f(x_{i}) \in \text{Vect} f ( \mathcal F)
  \end{align*}
\end{question_kholle}

\begin{question_kholle}
  [Nous donnerons les caractérisations au fur et à mesure de la démonstration.]
  {Caractérisation inj/surj/bij d'une application linéaire par l'image d'une base de l'espace de départ.}
  Soient donc pour la suite, $f \in \mathcal{L}_\K(E,F)$, $\mathcal{B} = (e_i)_{i\in I}$ une base de $E$, $\mathcal{B}' = (e'_i)\ii$ une base de $F$,$\mathcal{F} = (x_i)_{i\in I}$ une famille libre de $E$ et $\mathcal{G} = (y_i)_{i \in I}$ une famille génératrice de $E$, ces objets servent ici de notation et seront utilisés indépendamment lors de la preuve. \\ \\
  Montrons que l'image d'une base $\mathcal{B}$ par une application injective est une famille libre $\mathcal{F}$. \\
  Supposons $f$ injective, donc pour $\bdak$,
  \[
    0_F = \sum_{i\in  I}\bda_i f(e_i) = f \left( \sum_{i\in  I} \bda_i e_i \right) \ \overset{f \text{ inj}}{\implies} \ \sum_{i\in  I} \bda_i e_i = 0_E \ \overset{\mathcal{B} \text{ base donc libre}}{\implies} \ (\bda _i)_{i\in  I} = \widetilde{0_\K},
  \]
  donc $f(\mathcal{B})= \mathcal{F}$ libre. \\
  Supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})$ soit libre, montrons qu'alors $f$ est injective. \\
  Soit $x \in \ker f$ :
  \[
    \exists \ \bdak \ : \ 0_F = f(x) = f\left( \sum\ii \bda_i e_i \right) =\sum \ii \bda _i f(e_i) \  \overset{f(\mathcal{B}) \text{ libre}}{\implies} \ (\bda _i)_{i\in  I} = \widetilde{0_\K},
  \]
  donc $x = 0_E$ donc $\ker f = \{0_E\}$ et $f$ injective. \\ \\
  Montrons que l'image d'une base $\mathcal{B}$ par une application surjective est une famille génératrice $\mathcal{G}$. \\
  Supposons $f$ surjective. Ainsi, $\Im f = F$, or $\mathcal{B}$ est une base donc est génératrice donc :
  \[
    \Vect f(\mathcal{B}) = f\left( \Vect \mathcal{B} \right) = f( E) = \Im f = F,
  \]
  donc $f(\mathcal{B}) = \mathcal{G}$ est génératrice. \\
  Supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})$ soit génératrice, montrons que $f$ est surjective. \\
  On a ainsi,
  \[
    F = \Vect f(\mathcal{B}) = f \left( \Vect \mathcal{B} \right) = f(E) = \Im f,
  \]
  donc $f$ surjective. \\ \\
  Montrons que l'image d'une base $\mathcal{B}$ par un isomorphisme est une base $\mathcal{B}'$. \\
  Supposons que $f$ soit un isomorphisme. $f$ est injective et $\mathcal{B}$ est une base donc $f(\mathcal{B})$ est libre. $f$ est surjective et $\mathcal{B}$ est une base donc $f(\mathcal{B})$ est génératrice. Ainsi, $f(\mathcal{B})=\mathcal{B}'$ est une base. \\
  Réciproquement, supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})=\mathcal{B}'$ soit une base, montrons que $f$ est un isomorphisme.\\
  $\mathcal{B}'$ est une base donc est libre donc $f$ est injective. $\mathcal{B}'$ est une base donc est génératrice donc $f$ est surjective.
  Ainsi, $f$ est un isomorphisme.
\end{question_kholle}

\begin{question_kholle}
  [Il existe une unique application linéaire de $E$ dans $F$ qui envoie une base donnée de $E$ sur une famille de $F$ imposée. \\
    Soient $(e_i)\ii$ une base de $E$ et $(y_i)\ii$ une famille de $F$.
    \begin{equation}
      \exists ! f \in \mathcal{L}_\K(E, F) : \forall i \in I, f(e_i) = y_i
    \end{equation}
    Nous pouvons expliciter une telle application :
    \begin{equation}
      f \left| \;\; \begin{matrix}
        E                                          & \rightarrow & F                                          \\
        \displaystyle \sum\ii \lambda_i \ldotp e_i & \mapsto     & \displaystyle \sum\ii \lambda_i \ldotp y_i
      \end{matrix} \right.
    \end{equation}]
  {Caractérisation d'une application linéaire par l'image d'une base}
  ~\newline
  \underline{Analyse} Supposons qu'il existe $f \in \mathcal{L}_\K(E, F)$ \tq $\forall i \in I, f(e_i) = y_i$. \\
  Tout vecteur de $E$ peut se décomposer de manière unique dans la base $(e_i)\ii$, ce qui détermine son image. Ainsi, $f$ est unique.
  \newline

  \noindent \underline{Synthèse} Posons une telle application $f$. \\
  \begin{itemize}
    \item $(e_i)\ii$ est une base donc $(\lambda_i)\ii$ est presque nulle et unique donc $\sum\ii \lambda_i \ldotp y_i$ existe et unique.
          Ainsi, $f$ est bien définie.
    \item Soient $(\alpha, \beta, x, x') \in \K^2 \times E^2$ \fqs. Notons $(\lambda_i)\ii$ et $(\lambda'_i)\ii$ les coordonnées de $x$ et $x'$ dans $(e_i)\ii$.
          \begin{equation*}
            \begin{aligned}
              f( \alpha x + \beta x' )
               & = f\left( \alpha \sum\ii \lambda_i \ldotp e_i + \beta \sum\ii \lambda'_i \ldotp e_i \right)               \\
               & = f\left( \sum\ii \left( \alpha \lambda_i + \beta \lambda'_i \right) \ldotp e_i \right)                   \\
               & = \sum\ii \left( \alpha \lambda_i + \beta \lambda'_i \right) \ldotp y_i \quad \text{ par définiton de } f \\
               & = \alpha \sum\ii \lambda_i y_i + \beta \sum\ii \lambda'_i y_i                                             \\
               & = \alpha f(x) + \beta f(x')
            \end{aligned}
          \end{equation*}
          Donc $f$ est linéaire.
    \item Soit $j \in I$ \fq.
          \begin{equation*}
            \begin{aligned}
              f(e_j)
               & = f \left(\sum\ii \delta_{i,j} \ldotp e_i \right) \\
               & = \sum\ii \delta_{i,j} \ldotp y_i                 \\
               & = y_j
            \end{aligned}
          \end{equation*}
  \end{itemize}
\end{question_kholle}

\pagebreak\section{Semaine 23}

Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$, $I$ un ensemble quelconque non vide.

\begin{question_kholle}
  {L'ensemble des automorphisme d'un espace vectoriel muni de la loi de composition forme un groupe}

  Montrons que $(\mathcal{GL}_\K(E), \circ)$ est un sous-groupe de $(\mathcal{S}(E), \circ)$.
  \begin{itemize}
    \item $\mathcal{GL}_\K(E) \subset \mathcal{S}(E)$ et $(\mathcal{S}(E), \circ)$ est bien un groupe.
    \item $\mathcal{GL}_\K(E) \neq \emptyset$ puisque $Id_E \in \mathcal{GL}_\K$.
    \item Soit $(f, g) \in \mathcal{GL}(E)$. Montrons que $f \circ g^{-1} \in \mathcal{GL}(E)$. \\
          Soit $(\alpha, \beta, x, y) \in \K^2 \times E^2$ \fqs. \\
          \begin{equation*}
            \begin{aligned}
              \left(f \circ g^{-1}\right) \left(\alpha x + \beta y\right)
               & = f \left( g^{-1} \left(\alpha x + \beta y\right) \right)                                                                             \\
               & = f \left( g^{-1} \left(\alpha g^{-1}(g(x)) + \beta g^{-1}(g(y))\right) \right)                                                       \\
               & = f \left( g^{-1} \left( \alpha g\left(g^{-1}(x)\right) + \beta g\left(g^{-1}(y)\right) \right) \right)                               \\
               & = f \left( g^{-1} \left( g \left( \alpha g^{-1}(x) + \beta g^{-1}(y) \right) \right) \right) \quad \text{car } g \text{ est linéaire} \\
               & = f \left( \alpha g^{-1}(x) + \beta g^{-1}(y) \right)                                                                                 \\
               & = \alpha f \left( g^{-1}(x) \right) + \beta f \left( g^{-1}(y) \right)                                                                \\
               & = \alpha \left(f \circ g^{-1}\right) (x) + \beta \left(f \circ g^{-1}\right) (y)
            \end{aligned}
          \end{equation*}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}
  [Soit $\ffinie{E}{p}{E}$ $p$ \sev de E avec $p \in \N^*$ \fq. \\
    Par définition, cette famille est en somme directe si tout vecteur de $E_1 + E_2 + \ldots + E_p$ peut s'écrire comme une somme unique d'élément de $E_1 \times E_2 \times \ldots \times E_p$. Formellement :
    \begin{equation}
      \forall x \in \sum_{i=1}^{p} E_i,
      \exists ! x \in \! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i :
      x = \sum_{i=1}^{p} x_i
    \end{equation}
    Nous allons démontrer que $E_1, E_2, \ldots$ et $E_p$ sont en somme directe \ssi
    {\begin{equation}
        \forall x \in \! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i,
        \left( \sum_{i=1}^{p} x_i = 0_E \implies \forall i \in [\![1;p]\!], x_i = 0_E \right)
      \end{equation}}]
  {Caractérisation de la somme directe de $p$ \sevs}

  Supposons que $E_1, E_2, \ldots E_p$ sont en somme directe. \\
  Soient $x \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ \fqs \tqs $x_1 + x_2 + \ldots + x_p = 0_E$. \\
  Or $0_E = \underbrace{0_E}_{\in E_1} + \underbrace{0_E}_{\in E_2} + \ldots + \underbrace{0_E}_{\in E_p}$.
  Par unicité de l'écriture de x comme somme d'éléments de $\overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$, $\forall i \in [\![]1;p\!], x_i = 0_E$.

  Supposons maintenant l'équation de la caractérisation. \\
  Soit $x \in \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ \tq $x$ puisse s'écrire comme somme de $x' \!\! \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ et somme de $x'' \!\! \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$. Montrons que $x' = x''$.
  \begin{equation*}
    \sum_{i=1}^{p} x'_i = x = \sum_{i=1}^{p} x''_i
  \end{equation*}
  Donc
  \begin{equation*}
    \sum_{i=1}^{p} \left( x''_i -x''_i \right) = 0_E
  \end{equation*}
  D'après l'équation de la caractérisation, $\forall i \in [\![1;p]\!], x'_i - x''_i = 0_E$. \\
  Donc $\forall i \in [\![1;p]\!], x'_i = x''_i$
\end{question_kholle}
\pagebreak\section{Semaine 24}

Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$.

Nous rappelons que $\dim \{0_E\} = 0$ et que $\{0_E\} = \Vect \emptyset$.

\begin{question_kholle}
  [Pour tout \sev de $E$, il existe un \sev complémentaire.]
  {Existence d'un supplémentaire en dimension finie}

  ~\newline
  \textit{Théorème de la base incomplète} (admis ici mais démontré dans le cours) : pour toute famille libre de E, nous pouvons y adjoindre une partie d'une famille quelconque génératrice de $E$ (généralement une base, la base canonique si elle a un sens) pour en faire une base de $E$. \\

  Posons $n = \dim E$ et $p = \dim E'$. Ainsi, il existe $(e_1, \ldots, e_p)$ base de $E'$.
  Appliquons le théorème de la base incomplète pour cette famille.
  Il existe $(e_{p+1}, \ldots, e_n)$ $n-p$ vecteurs de E \tq $(e_1, \ldots, e_n)$ est un base de $E$.
  Posons $E'' = \Vect \{ e_{p+1}, \ldots, e_n \}$ et vérifions qu'il est supplémentaire à $E'$.

  Par définition de \Vect\!\!, $E''$ est un \sev.
  Trivialement, $E' + E'' = E$.
  $\{0_E\} \subset E' \cap E''$ car $E'$ et $E''$ sont deux \sevs.
  Soit $x \in E' \cap E''$.
  $X \in E' \implies \exists (\lambda_1, \ldots, \lambda_p) \in \K^p : x = \sum_{i=1}^{p} \lambda_i e_i$ et
  $X \in E'' \implies \exists (\lambda_{p+1}, \ldots, \lambda_n) \in \K^{n-p} : x = \sum_{i=p+1}^{n} \lambda_i e_i$.
  Par différence, $\sum_{i=1}^{p} \lambda_i e_i + \sum_{i=p+1}^{n} \left(-\lambda_i\right) e_i = 0_E$.
  Or $\famille{e}{\lient 1 ; n \rient}$ est une base de $E$ donc $\forall i \in \lient 1 ; p \rient, \lambda_i = 0_\K$.
  donc $x = 0_E$.
  Ainsi, $E' \cap E'' = \{0_E\}$.
\end{question_kholle}

\begin{question_kholle}
  [$\mathcal{L}_\K(E,F)$ est dimension finie et
    \begin{equation}
      \dim \mathcal{L}_\K(E,F) = \dim E \times \dim F
    \end{equation}]
  {Dimension de $\mathcal{L}_\K(E,F)$}

  Notons $n = \dim E$ et $\famille{e}{\lient 1 , n \rient}$ une base de $E$. Considérons
  \begin{equation*}
    \varphi
    \left| \begin{array}{ccl}
      \mathcal{L}_\K(E,F) & \to     & F^n                                            \\
      f                   & \mapsto & \Big( f(e_i) \Big)_{i \in \lient 1 , n \rient}
    \end{array} \right.
  \end{equation*}

  $\varphi$ est linéaire et, d'après le théorème de création des applications linéaires, bijective.
  Ainsi, $\mathcal{L}_\K(E,F)$ et $F^n$ sont isomorphes. $F^n$ est de dimension finie, ce qui conclut.
\end{question_kholle}

\begin{question_kholle}
  [Supposons $E$ de dimension finie. \\
    Soient $E_1$ et $E_2$ deux \sevs. Alors $E_1 + E_2$ est de dimension finie et
    \begin{equation}
      \dim E_1 + E_2 = \dim E_1 + \dim E_2 - \dim E_1 \cap E_2
    \end{equation}]
  {Formule de Grassman}

  Commençons par prouver une version simplifier de la somme directe. Supposons que $E_1$ et $E_2$ sont en somme directe.

  Fixons $\mathcal{B}_1$ et $\mathcal{B}_2$ deux bases de $E_1$ et $E_2$.
  Alors $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ engendre $E_1 + E_2$. Or $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est finie donc $E_1 + E_2$ est de dimension finie.

  Posons $n = \dim E_1$ et $p = \dim E_2$. Notons $\famille{e}{\lient 1 ; n \rient}$ la base $\mathcal{B}_1$ et $\famille{f}{\lient 1 ; n \rient}$ la base $\mathcal{B}_2$.
  Soient $\lambda_1, \ldots, \lambda_n, \mu_1, \ldots, \mu_p) \in \K^{n+p}$ \fqs \ \tqs \ $\displaystyle \sum_{i=1}^{n} \lambda_i e_i + \sum_{i=1}^{p} \mu_i f_i = 0_E$.
  Alors $\sum_{i=1}^{n} \lambda_i e_i = \sum_{i=1}^{p} (-\mu_i) f_i$.
  Or $\sum_{i=1}^{n} \lambda_i e_i \in E_1$ et $\sum_{i=1}^{n} (-\mu_i) e_i \in E_2$ donc $\sum_{i=1}^{n} \lambda_i e_i \in E_1 \cap E_2 = \{0_E\}$.
  Donc $\lambda = \widetilde{0}$. De même, $\mu = \widetilde{0}$.
  Donc $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est libre.

  Ainsi, $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est une base de $E_1 \oplus E_2$.
  Donc $ \dim E_1 \oplus E_2 = |(\mathcal{B}_1, \mathcal{B}_2)| = |\mathcal{B}_1| + |\mathcal{B}_2| = \dim E_1 + \dim E_2$. \\

  Enlevons l'hypothèse que $E_1$ et $E_2$ sont en somme directe.
  $E_1 \cap E_2$ est un \sev de $E_2$. Comme $E_2$ et un \K\!\!-espace vectoriel de dimension finie, il existe $E_2'$ \sev de $E_2$ \tq $E_2 = (E1 \cap E_2) \oplus E_2'$.

  Montrons que $E_1 + E_2 = E_1 \oplus E_2'$.
  \begin{equation*}
    \begin{aligned}
      E_1 \cap E_2' & = E_1 \cap \left( E_2' \cap E_2 \right) \text{ car } E_2' \subset E_2                            \\
                    & = \left( E_1 \cap E_2 \right) \cap E_2' \text{ car } \cap \text{ est associative et commutative} \\
                    & = {0_E} \text{ car $E_1$ et $E_2$ sont en somme directe et $E_2'$ sev}
    \end{aligned}
  \end{equation*}
  Donc $E_1$ et $E_2'$ sont en somme directe.

  $E_2' \subset E_2$ donc $E_1 + E_2' \subset E_1 + E_2$.
  Soit $x \in E_1 + E_2$.
  Alors $\exists (x_1, x_2) \in E_1 \times E_2 : x = x_1 + x_2$. \\
  Or $E_2 = \left( E_1 \cap E_2 \right) \oplus E_2'$ donc $\exists (x_{21}, x_2') \times E_2' : x_2 = x_{21} + x_2'$.
  D'où $x = x_1 + x_{21} + x_2'$. Or $x_1 + x_{21} \in E_1$ et $x_2' \in E_2$ donc $x \in E_1 + E_2'$.

  Ainsi, $E_1$ et $E_2'$ étant des \sev de dimension finie, $\dim E_1 \oplus E_2' = \dim E_1 + \dim E_2'$.
  De plus, $\dim E_2 = \dim (E_1 \cap E_2) \oplus E_2' = \dim E_1 \cap E_2 + \dim E_2'$.
  Donc $\dim E_1 + E_2 = \dim E_1 + \dim E_2 - \dim E_1 \cap E_2$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $f \in \mathcal{L}_{\mathbb{K}}(E, F)$.
    \begin{propositions}
      \item Si $E$ est de dimension finie
      \begin{equation}
        f \text{ injective } \iff \mathrm{rg} f = \dim E
      \end{equation}
      \item Si $F$ est de dimension finie
      \begin{equation}
        f \text{ surjective } \iff \mathrm{rg} f = \dim F
      \end{equation}
      \item Si $E$ et $F$ sont de même dimension finie $$f \text{ bijective } \iff f \text{ injective } \iff f \text{ sujective }$$
      C'est \textit{l'accident de la dimension finie} !
    \end{propositions}]
  {Caractérisation injectivité/bijectivité/surjectivité par le rang}

  ~
  \begin{propositions}
    \item Supposons $E$ de dimension finie, fixons $(e_{1}, \dots, e_{n})$ une base de $E$ (avec $n = \dim E$)
    Supposons $f$ injective :
    $$
      \mathrm{rg} f = \dim \mathrm{Im} f = \dim \text{Vect} \left\{ f(e_{1}) \dots f(e_{n}) \right\}
    $$

    Donc $(f(e_{1}), \dots f(e_{n}))$ est génératrice.
    $(f(e_{1}), \dots f(e_{n}))$ est de plus libre car $f$ est injective.
    Donc c'est une base, donc
    $$
      \dim \text{Vect} \left\{ f(e_{1}) \dots f(e_{n}) \right\}  =n = \dim E
    $$
    donc $\mathrm{rg} f = \dim E$.
    Réciproquement, supposons que $\mathrm{rg} f = \dim E = n$.
    Alors $$n = \mathrm{rg} f = \dim \text{Vect} \left\{ f(e_{1}),\dots,f(e_{n}) \right\}$$
    Donc $(f(e_{1}), \dots f(e_{n}))$ est génératrice de cardinal $n$, égal à la dimension du sous-espace vectoriel engendré. C'est donc une base du sous-espace vectoriel engendré.
    Donc $(f(e_{1}),\dots , f(e_{n}))$ est libre, donc $f$ est injective.

    \item Supposons $F$ de dimension finie
    $$
      f \text{ surjective } \iff \mathrm{Im} f = F \iff \dim \mathrm{Im} f = \dim F
    $$

    \item Supposons $E$ et $F$ de même dimension finie
    $$
      f \text{ injective } \iff \mathrm{rg} f = \dim E \iff \mathrm{rg} f = \dim F \iff f \text{ surjective}
    $$
    D'où la bijectivité.
  \end{propositions}
\end{question_kholle}

\begin{question_kholle}
  [Si $E$ est de dimension finie alors pour toute $f \in \mathcal{L}_\K(E, F)$ application linéaire,
    \begin{equation}
      \dim E = \rg f + \dim \ker f
    \end{equation}]
  {Théorème du rang}

  Démontrons d'abord le lemme suivant.
  Soient $f \in \mathcal{L}_\K(E, F)$ et $H$ un supplémentaire de $\ker f$ dans $E$.
  Alors $f_{|H}^{|\Im f}$ est un isomorphisme de $H$ sur $\Im f$.

  Notons $\hat{f}$ un telle restriction et corestriction. Cette application est bien définie (car $f(H) \subset \Im f$) et $\hat{f} \in \mathcal{L}_\K(H, \Im f)$.

  Calculons son noyau. $\ker \hat{f} = \{ x \in H \;|\; \hat{f}(x) = 0_E \} = \{ x \in H | x \in \ker f \} = H \cap \ker f = \{0_E\}$  car $H$ et $\ker f$ sont complémentaire.
  Donc $\hat{f}$ est injective.

  Soit $y \in \Im f$. D'où $\exists x \in E: y = f(x)$.
  Décomposons $x$ dans $E = H \oplus \ker f$, $\exists (x_H, x_k) \in H \times \ker f : x = x_H + x_k$.
  Ainsi, $y = f(x) = f(x_H) + f(x_k)= f(x_H)$ car $x_k \in \ker f$.
  Donc $y$ admet un antécédent par $\hat{f}$ (qui est $x_H$).
  Donc $\hat{f}$ est surjective.

  Donc $f_{|H}^{|\Im f}$ est un isomorphisme de $H$ sur $\Im f$.
  \newline

  Supposons maintenant que $E$ est de dimension finie.
  Soit $f \in \mathcal{L}_\K(E, F)$.
  D'après le théorème d'existence d'un supplémentaire en dimension finie, $\ker f$, étant un \sev de $E$, admet un supplémentaire $H$ c'est-à-dire $E = H  \oplus \ker f$.
  En prenant la dimension sur cette égalité, $\dim E = \dim \ker f + \dim H$.
  D'après le lemme précédent, $\dim H = \dim \Im f = \rg f$.
  D'où $\dim E = \rg f + \dim \ker f$.
\end{question_kholle}

\begin{question_kholle}
  [Soit $G$ un \K-espace vectoriel et $(u,v) \in \mathcal{L}_\K(E, F) \times \mathcal{L}_\K(F, G)$. Si $E$ et $F$ sont de dimension finie alors
    \begin{equation}
      \rg u = \rg v \circ u + \dim \ker v \cap \Im u
    \end{equation}]
  {Rang d'une composition d'applications linéaires}

  Considérons que $E$ et $F$ sont de dimension finie.
  Soient de tels objets.
  Appliquons le théorème du rang à $v_{|\Im u}$ ce qui est autorisé puisque $v_{|\Im u}$ est une application linéaire et $\Im u$ est un \ev de dimension finie (car sev de $F$).
  \begin{equation*}
    \dim \Im u = \rg v_{|\Im u} + \dim \ker v_{|\Im u}
  \end{equation*}
  Ainsi, $\ker v_{|\Im u} = \left\{ y \in \Im u \;|\; v(y) = 0_G \right\} = \left\{ y \in \Im u \;|\; y \in \ker v \right\} = \Im u \cap \ker v$
  et $\Im v_{|\Im u} = v(Im u) = \Im v \circ u$ (cette égalité est vraie pour deux fonctions de $E$ dans $F$ et de $F$ dans $G$ quelconques, pas forcément linéaires).
  Ce qui conclut.
\end{question_kholle}

\begin{question_kholle}
  [Soit $H$ un \sev de $E$.
    Les conditions suivantes sont équivalentes :
    \begin{propositions}
      \item $H$ est un hyperplan de $E$ :
      $\exists \varphi \in E^* : H = \ker \varphi$
      \item $H$ admet une droite vectorielle comme supplémentaire :
      $\exists a \in E \setminus \{0_E\} : H \oplus \Vect{\{a\}} = E$
    \end{propositions}]
  {Caractérisation des hyperplans}

  $(i) \implies (ii)$ Supposons que $H$ est un hyperplan de $E$.
  Appliquons la définition de l'hyperplan, $\exists \varphi \in E^* : H = \ker \varphi$.
  Par l'absurde, supposons que $E \setminus H = \emptyset$. Or $H \subset E$ donc $E = H$. Donc $\varphi = 0_{E^*}$ ce qui est une contradiction.

  Ainsi fixons $a \in E \setminus H$ quelconque.
  Montrons que $E = H \oplus \Vect{\{a\}}$.
  Trivialement, $\{0_E\} \subset H \cap \Vect{\{a\}}$.
  Soit $x \in H \cap \Vect{\{a\}}$.
  $x \in \Vect{\{a\}}$ donc $\exists \lambda \in  \K : x = \lambda$. De plus, $x \in H = \ker \varphi$ donc $0_\K = \varphi(x) = \lambda \varphi(a)$.
  Si $\lambda \neq 0_\K$, alors $a \in \ker \varphi$ ce qui est impossible car $a \notin H$.
  Donc $\lambda = 0_\K$, d'où $x = 0_E$.
  Ainsi, $H \cap \Vect{\{a\}} = \{0_E\}$. $H$ et $\Vect{\{a\}}$ sont en somme directe.

  Trivialement, $H + \Vect{\{a\}} \subset E$.
  Soit $x \in E$ \fq.
  $a \notin H$ donc $\varphi(a) \neq 0_\K$. $\varphi(a)$ est inversible dans \K d'où :
  \begin{equation*}
    \varphi(x)
    = \frac{\varphi(x)}{\varphi(a)} \cdot \varphi(a)
    = \varphi\left( \frac{\varphi(x)}{\varphi(a)} \times a \right)
  \end{equation*}
  Donc $x - \frac{\varphi(x)}{\varphi(a)} \!\cdot\! a \in H$. D'où
  \begin{equation*}
    x =
    \underbrace{x - \frac{\varphi(x)}{\varphi(a)} \!\cdot\! a}_{\in H}
    + \underbrace{\frac{\varphi(x)}{\varphi(a)} \!\cdot\! a}_{\in \Vect{\{a\}}}
  \end{equation*}
  Ainsi, $E = H + \Vect{\{a\}}$.

  $(ii) \implies (i)$ Supposons maintenant que $H$ soit un sous-espace vectoriel tel que $\exists a \in E \setminus \{0_E\} : E = H \oplus \Vect{\{a\}}$.
  Posons $\displaystyle \varphi : \begin{matrix}
      E & = & H \oplus \Vect{\{a\}}   & \rightarrow & \K        \\
      x & = & h_x + \lambda_x \cdot a & \mapsto     & \lambda_x
    \end{matrix}$.
  Montrons que $\varphi$ est une forme linéaire non triviale dont $H$ est le noyau.
  \

  $\varphi$ est
  bien définie (car $h_x$ et $\lambda_x$ sont uniques),
  linéaire,
  à valeur dans le corps de base \K
  donc $\varphi$ est un forme linéaire.
  $\varphi \neq 0_{E^*}$ car $\varphi(a) = 1_\K \neq 0_\K$.
  Soit $x \in E$ \fq. Alors $\exists (h_x, \lambda_x) \in H \times \K : x = h_x + \lambda_x \cdot a$.
  \begin{equation*}
    x \in \ker \varphi
    \iff \varphi(x) = 0_\K
    \iff \lambda_x = 0_\K
    \iff x \in H
  \end{equation*}
  donc $\ker \varphi = H$.
  Donc $H$ est un hyperplan de $E$.
  \bigbreak

  \noindent Si $E$ est de dimension finie, alors les deux conditions sont équivalentes à
  \begin{enumerate}[label=$(\roman*)$, leftmargin=1.5cm]
    \setcounter{enumi}{2}
    \item $H$ est de codimension 1 c'est-à-dire de dimension $n - 1$.
  \end{enumerate}
  $(ii) \implies (iii)$ Il faut prendre la dimension de l'égalité $H \oplus \Vect{\{a\}}$. \\
  $(iii) \implies (ii)$ Supposons que $\dim H = n - 1$.
  Comme $E$ est de dimension finie, $H$ admet un supplémentaire $I$ dans $E$ : $H \oplus I = E$.
  En prenant la dimension, $\dim I = 1$. Donc $I$ est une droite vectorielle.
  D'où $\exists a \in E : I = \Vect{\{a\}}$.
  $a \notin H$ car sinon $I \subset H$ ce qui contredit $I \cap H = \{0_E\}$ ($I$ et $H$ sont en somme directe).
\end{question_kholle}

\begin{question_kholle}
  [\textit{Lemme fondamental dans l'étude des formes linéaires} \ \
    Soit $\varphi \in E^* \setminus \{0_{E^*}\}$. \\
    Tout vecteur de $E$ n'appartenant pas au noyau de $\varphi$ engendre une droite qui est supplémentaire au noyau de $\varphi$ dans $E$.
    \begin{equation}
      \forall a \in E \setminus \ker \varphi, \
      E = \ker \varphi \oplus \Vect{\{a\}}
    \end{equation}

    Deux formes linéaires non nulles $\varphi$ et $\psi$ ont le même noyau si est seulement si elles sont proportionnelles ce qui revient à dire que la famille $\left(\varphi,\psi\right)$ est liée.
    \begin{equation}
      \forall \left(\varphi,\psi\right) \in \left( E^* \setminus \{0_{E^*}\} \right) \!^2, \
      \ker \varphi = \ker \psi \iff \exists \lambda \in \K^* : \varphi = \lambda \cdot \psi
    \end{equation}]
  {Proportionnalité des formes linéaires ayant le même noyau}

  Commençons par prouver le lemme.
  Soit $a \in E \setminus \ker \varphi$. \\
  Soit $x \in E$ \fq.
  Exhibons la décomposition unique de $x$ dans $\ker \varphi + \Vect{\{a\}}$.

  \textit{Analyse} Supposons qu'il existe $(x_k, \lambda) \in \ker \varphi \times \K$ \tq $x = x_k + \lambda a$.
  Puisque $x_k \in \ker \varphi$, $\varphi(x) = \lambda \cdot \varphi(a)$. Or $\varphi(a) \neq 0_\K$ (car $a \notin \ker \varphi$) donc $\varphi(a)$ est inversible dans \K.
  D'où $\lambda = \nicefrac{\varphi(x)}{\varphi(a)}$ et $x_k = x - \nicefrac{\varphi(x)}{\varphi(a)} \cdot a$.

  Ainsi, sous réserve d'existence, $\lambda$ et $x_k$ sont uniques.

  \textit{Synthèse} Posons $\displaystyle \left\{ \begin{matrix}
      \lambda & = & \frac{\varphi(x)}{\varphi(a)}       \\
      x_k     & = & x - \frac{\varphi(x)}{\varphi(a)} a
    \end{matrix} \right.$
  Nous avons bien
  $x = x_k + \lambda \cdot a$,
  $\lambda \cdot a \in \Vect{\{a\}}$ (car $\lambda \in \K$)
  et $x_k \in \ker \varphi$ (car $\varphi(x_k) = \varphi(x) - \varphi\left( \frac{\varphi(x)}{\varphi(a)} a \right) = \varphi(x) - \frac{\varphi(x)}{\varphi(a)} \varphi(a) = 0_\K$).
  Ainsi $E = \ker \varphi \oplus \Vect{\{a\}}$.
  \newline \newline

  \noindent Soient $\left(\varphi,\psi\right) \in \left( E^* \setminus \{0_{E^*}\} \right) ^2$ \fqs.

  \textit{Sens direct
  } Supposons que $\ker \varphi = \ker \psi$.
  $\varphi \neq 0_{E^*}$ donc $\ker \varphi \neq E$ donc $\exists a \in E : a \notin \ker \varphi$. Appliquons la lemme ci-dessus :
  \begin{equation*}
    \begin{array}{ccccccccc}
       &           & E & = & \substack{\ker \varphi                                                                                                      \\ \shortparallel \\ \ker \psi} &\oplus& \Vect{\{a\}} & \rightarrow & \K \\
       & \varphi : & x & = & \left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) & + & \frac{\varphi(x)}{\varphi(a)} \cdot a & \mapsto & \varphi(x) \\
       & \psi :    & x & = & \left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) & + & \frac{\varphi(x)}{\varphi(a)} \cdot a & \mapsto & \psi(x)
    \end{array}
  \end{equation*}
  Or $\left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) \in \ker \psi$ donc $\psi(x) = \frac{\psi(a)}{\varphi(a)} \varphi(x)$.
  Ainsi, $\psi = \frac{\psi(a)}{\varphi(a)} \varphi$. Donc $\varphi$ et $\psi$ sont proportionnelles.

  \textit{Sens réciproque} Supposons que $\varphi$ et $\psi$ sont proportionnelles. Alors $\exists \lambda \in \K^* : \varphi = \lambda \psi$.
  $\varphi = \lambda \psi \implies \ker \psi \subset \ker \varphi$ et
  $\psi = \lambda^{-1} \varphi \implies \ker \varphi \subset \ker \psi$.
  Ce qui donne l'égalité.
\end{question_kholle}

\begin{question_kholle}
  [\indent Soit $\varphi \in E^{*}$ une forme linéaire \emph{non nulle}. Soit $F$ un sous-espace vectoriel de $E$ de dimension finie $p \in \mathbb{N}$, alors
    \begin{equation}
      \dim_{\mathbb{K}}F \cap \ker \varphi =
      \left\{ \begin{array}{ll}
        p   & \text{ si } F \subset \ker \varphi \\
        p-1 & \text{ sinon}
      \end{array}\right.
    \end{equation}
    En particulier, on a toujours $\dim_{\mathbb{K}}F \cap \ker \varphi \geqslant p-1$
    \newline

    Supposons que  $E$ un \ev de dimension finie $n \in \mathbb{N}^{*}$.
    Soient $m \in \mathbb{N}^{*}$ et $(H_{i})_{n \in [ \! [ 1,m ] \!]}$, $m$ hyperplans de $E$.
    Alors
    \begin{equation}
      \dim_{\mathbb{K}} \bigcap_{i=1}^{m}H_{i} \geqslant n-m
    \end{equation}]
  {Intersection d'hyperplans}
  % {Lemme sur les formes linéaires non nulles} ???

  Si $F \subset \ker\varphi$, $F \cap \ker \varphi = F$ donc $\dim F \cap \ker \varphi = p$

  Sinon, il existe $a \in F$ tel que $a \not\in \ker \varphi$. Ainsi,
  $$
    \text{Vect}\left\{ a \right\}  \oplus \ker \varphi = E
  $$
  Montrons alors que $F = \text{Vect} \left\{ a \right\} \oplus (F \cap \ker \varphi)$.
  $$\text{Vect} \left\{ a \right\} \cap (F \cap \ker \varphi) = \underbrace{ \text{Vect} \left\{ a \right\} \cap F }_{ =\text{Vect}\left\{ a \right\}  } \cap \ker \varphi = \text{Vect}\left\{ a \right\}  \cap \ker \varphi = \left\{ 0_{E} \right\} $$ car les deux espaces sont supplémentaires donc en somme directe.

  Par double inclusion, montrons que $\text{Vect} \left\{ a \right\} + (F \cap \ker \varphi) = F$.
  Pour l'inclusion directe, remarquons que $a \in F$ donc $\text{Vect}\left\{ a \right\} \subset F$ or $F \cap \ker \varphi \subset F$ donc leur somme est bien incluse $\text{Vect} \left\{ a \right\} + (F \cap \ker \varphi) \subset F$.
  Réciproquement, soit $x \in F$ \fq.
  Puisque $\text{Vect} \left\{ a \right\} \oplus \ker \varphi = E$
  $$
    \exists (\lambda, x_{K}) \in \mathbb{K} \times \ker \varphi : x = \lambda .a+x_{K}
  $$
  De plus, $x_{K} = x - \lambda.a \in F$ car $(a, x) \in F^{2}$ donc
  $$
    x = \underbrace{ \lambda . a }_{ \in \text{Vect} \left\{ a \right\}  } + \underbrace{ x_{K} }_{ \in F \cap \ker \varphi } \in \text{Vect} \left\{ a \right\} + (F \cap \ker \varphi)
  $$
  D'où l'inclusion réciproque.

  Donc $F = \text{Vect} \left\{ a \right\} \oplus (F \cap \ker \varphi)$.
  En passant à la dimension :
  $$
    \underbrace{ \dim F }_{= p } = \underbrace{ \dim \text{Vect} \left\{ a \right\} }_{ =1 } + \dim (F \cap \ker \varphi)
  $$
  Donc $\dim (F \cap \ker \varphi) = p - 1$.
  \newline \newline

  Considérons la propriété $\mathcal{P}(\cdot)$ définie pour tout $m \in \mathbb{N}^{*}$ par :
  $$
    \mathcal{P}(m) : \text{\textquotedblleft} \text{ pour tous } H_{1},\dots, H_{m} \text{ hyperplans de } E, \dim_{\mathbb{K}} \bigcap_{i=1}^{m}H_{i} \geqslant n-m \text{\textquotedblright}
  $$
  Soit $H_{1}$ un hyperplan de $E$ fixé quelconque. D'après la caractérisation des hyperplans en dimension finie,
  $$
    \dim_{\mathbb{K}} \bigcap_{i=1}^{1}H_{i} = \dim_{\mathbb{K}}H_{1}= n-1 \geqslant n-1
  $$
  Donc $\mathcal{P}(1)$ est vraie.

  Soit $m \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{P}(m)$ est vraie.
  Soient $H_{1},\dots,H_{m}$ et $H_{m+1}$ $m+1$ hyperplans de $E$.
  D'après la définition d'un hyperplan, il existe $\varphi \in E ^{*}$ non nulle telle que $H_{m+1} = \ker \varphi$.

  Appliquons donc le lemme précédent pour $F \leftarrow \bigcap_{i=1}^{m}H_{i}$ (autorisé car c'est un sous espace de l'espace $E$, qui est de dimension finie, donc ses sous espaces les sont aussi) et $\varphi \leftarrow \varphi$ (autorisé car c'est une forme linéaire non nulle) :

  $$
    \dim_{\mathbb{K}} \underbrace{ \left( \bigcap_{i=1}^{m}H_{i} \right) \cap \ker \varphi  }_{ =\left( \bigcap_{i=1}^{m}H_{i}  \right)\cap H_{m+1} }\geqslant \dim_{\mathbb{K}} \left( \bigcap_{i=1}^{m}H_{i} \right) - 1 \underbrace{ \geqslant n - m - 1 }_{ \text{ en appliquant } \mathcal{P}(m) \text{ pour } H_{1},\dots,H_{m} }
  $$

  Donc par associativité de l'intersection, $\dim_{\mathbb{K}} \bigcap_{i=1}^{m+1}H_{i} \geqslant n - (m+1)$.
  Donc $\mathcal{P}(m+1)$ est vraie.
\end{question_kholle}

\pagebreak\section{Semaine 25}

\begin{question_kholle}
  [Soit $A \in \mathcal{M}_{n}(\mathbb{K})$
    \begin{itemize}
      \item S'il existe $B \in \mathcal{M}_{n}(\mathbb{K}):A\times B = I_{n}$, alors $A\in GL_{n}(\mathbb{K})$ et $A^{-1}=B$
      \item S'il existe $B \in \mathcal{M}_{n}(\mathbb{K}):B \times A = I_{n}$, alors $A\in GL_{n}(\mathbb{K})$ et $A^{-1}=B$
    \end{itemize}
  ]
  {S'il existe un inverse à droite (ou à gauche) pour une matrice carrée, alors celle ci est inversible}


  Supposons $\exists B \in \mathcal{M}_{n}(\mathbb{K}):A\times B = I_{n}$. Notons $(\hat{a}, \hat{b}) \in \mathcal{L}(\mathbb{K}^{n})$ les endomorphismes canoniquement associés à $A$ et à $B$.


  \begin{align*}
    \Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}(\hat{a} \circ \hat{b}) & = \mathrm{mat}(\hat{a} \circ  \hat{b}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}})                                                                                                                                 \\
                                                                           & = \mathrm{mat}(\hat{a}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) \times_{\mathcal{M_{n}(\mathbb{K})}} \mathrm{mat}(\hat{b}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) \\
                                                                           & = A \times B                                                                                                                                                                                                                                               \\
                                                                           & = I_{n}                                                                                                                                                                                                                                                    \\
                                                                           & = \mathrm{mat}(\mathrm{Id}_{\mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) = \Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}(\mathrm{Id}_{\mathbb{K}^{n}})
  \end{align*}


  D'où, par injectivité de $\Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}$, $\hat{a} \circ \hat{b} = \mathrm{Id}_{\mathbb{K}^{n}}$.

  Ainsi, $\hat{a} \circ \hat{b}$ est surjective, donc $\hat{a}$ est surjective, mais par l'accident de la dimension finie, $\hat{a}$ est bijective, donc c'est un automorphisme, donc toutes ses matrices associées sont inversibles. On effectue un même raisonnement pour l'inversibilité à gauche, en utilisant cette fois l'injectivité.
\end{question_kholle}

\begin{question_kholle}
  [{Soient $E, F, G$, trois $\mathbb{K}$-espaces vectoriels de dimension finie $(p, q, r) \in (\mathbb{N}^{*})^{3}$
  $\mathcal{B}_{E}, \mathcal{B}_{F}, \mathcal{B}_{G}$ des bases respectives de ces trois espaces vectoriels, et  $u \in \mathcal{L}_{\mathbb{K}}(E, F)$ $v \in \mathcal{L}_{\mathbb{K}}(F, G)$.

  Alors
  \begin{equation}
    \mathrm{mat}(v \circ u, \mathcal{B}_{E}, \mathcal{B}_{G}) = \mathrm{mat}(v, \mathcal{B}_{F}, \mathcal{B}_{G}) \times \mathrm{mat}(u, \mathcal{B}_{E}, \mathcal{B}_{F})
  \end{equation}
  }]{Lien composée des applications linéaires et produit des matrices les représentant vis-à-vis de certaines bases}

  Posons $W = \mathrm{mat}(v \circ u, \mathcal{B}_{E}, \mathcal{B}_{G}) \in \mathcal{M}_{r, p}(\mathbb{K})$, $V=\mathrm{mat}(v, \mathcal{B}_{F}, \mathcal{B}_{G}) \in \mathcal{M}_{r, q}(\mathbb{K})$, $U = \mathrm{mat}(u, \mathcal{B}_{E}, \mathcal{B}_{F}) \in \mathcal{M}_{q, p}(\mathbb{K})$

  Donc $V \times U$ a un sens et $V \times U \in \mathcal{M}_{r, p}(\mathbb{K})$

  Pour montrer l'égalité matricielle, nous allons utiliser la propriété suivante:

  Soient $(M, M') \in \mathcal{M}_{r, p}(\mathbb{K})^{2}$ telles que $\forall X \in \mathcal{M}_{p, 1}(\mathbb{K}): MX = M'X$, alors $M = M'$. (Cela se prouve facilement en particularisant pour les matrices de la base canonique de $\mathcal{M}_{p,1}(\mathbb{K})$)

  Soit $X \in \mathcal{M}_{p, 1}(\mathbb{K})$, montrons que $W \times X = V \times U \times X$.
  Posons $x \in E$ de sorte que $X = \mathrm{mat}(x, \mathcal{B}_{E})$.

  \begin{align*}
    WX & = \mathrm{mat}(v \circ  u, \mathcal{B}_{E}, \mathcal{B}_{G})\times \mathrm{ mat}(x, \mathcal{B}_{E})  \\
       & = \mathrm{mat}((v \circ  u)(x), \mathcal{B}_{G})                                                      \\
       & = \mathrm{mat}(v(u(x)), \mathcal{B}_{G})                                                              \\
       & = \mathrm{mat}(v, \mathcal{B}_{F}, \mathcal{B}_{G}) \times \mathrm{mat}(u(x), \mathcal{B}_{F})        \\
       & \text{ d'après l'expression matricielle de l'image d'un vecteur par une application linéaire}         \\
       & = V \times \mathrm{mat}(u, \mathcal{B}_{E}, \mathcal{B}_{F})\times \mathrm{ mat }(x, \mathcal{B}_{E}) \\
       & = V\times U\times X
  \end{align*}

  Ce qui prouve l'égalité matricielle
\end{question_kholle}

\begin{question_kholle}[{	Soit $H$ un $\mathbb{K}$-espace vectoriel de dimension $d \in \mathbb{N}^{*}$.
        $\mathcal{B}_{H}$, une base de $H$ et $(h_{1}, \dots, h_{d})$, $d$ vecteurs de $H$.

        \begin{equation}
          (h_{1}, \dots, h_{d}) \text{ base de }H \iff \mathrm{mat}((h_{1}, \dots, h_{d}), \mathcal{B}_{H})\in GL_{n}(\mathbb{K})
        \end{equation}
      }]{Montrer qu'une famille de $d$ vecteurs d'un espace de dimension $d$ est une base si et seulement si la matrice de ces vecteurs dans une base (donc dans toute) est inversible.}

  Notons $(e_{1}, \dots, e_{d})$ la base de $H$
  Cherchons à interpréter $\mathrm{mat}((h_{1}, \dots, h_{d}), \mathcal{B}_{H})$ comme la matrice d'une application linéaire.
  Notons $u$ l'unique endomorphisme de $H$ dans $H$ tel que $\forall i \in[ \! [ 1, d ] \!] , u(e_{i}) = h_{i}$

  \begin{align*}
    \mathrm{mat}(u, \mathcal{B}_{H}) & = \bigg[ \begin{array}{c|c|c|c}
                                                    \mathrm{mat}(u(e_{1}), \mathcal{B}_{H}) & \mathrm{mat}(u(e_{2}), \mathcal{B}_{H}) & \dots & \mathrm{mat}(u(e_{d}), \mathcal{B}_{H})
                                                  \end{array} \bigg] \\
                                     & = \bigg[ \begin{array}{c|c|c|c}
                                                    \mathrm{mat}(h_{1}, \mathcal{B}_{H}) & \mathrm{mat}(h_{2}, \mathcal{B}_{H}) & \dots & \mathrm{mat}(h_{d}, \mathcal{B}_{H})
                                                  \end{array} \bigg]          \\
                                     & = \mathrm{mat}((h_{1}, \dots, h_{d}), \mathcal{B}_{H})
  \end{align*}

  Si bien que

  \begin{align*}
    (h_{1}, \dots, h_{d}) \text{ base de }H & \iff (u(e_{1}), \dots, u(e_{d})) \text{ base de }H                               \\
                                            & \iff u \in \mathcal{GL}_{\mathbb{K}}(H)                                          \\
                                            & \iff \mathrm{mat}(u, \mathcal{B}_{H}) \in GL_{d}(\mathbb{K})                     \\
                                            & \iff \mathrm{mat}((h_{1}, \dots, h_{d}), \mathcal{B}_{H}) \in GL_{d}(\mathbb{K})
  \end{align*}

\end{question_kholle}

\begin{question_kholle}[
    Soient $(E, F)$ deux $\mathbb{K}$-espaces vectoriels de dimension finie, $u \in \mathcal{L}_{\mathbb{K}}(E, F)$, $\mathcal{B}_{E}$ et $\mathcal{B}_{E}'$ deux bases de $E$, $\mathcal{B}_{F}$ et $\mathcal{B}_{F}'$ deux bases de $F$

    Posons $U = \mathrm{mat}(u, \mathcal{B}_{E}, \mathcal{B}_{F})$ et $U' = \mathrm{mat}(u, \mathcal{B}_{E}', \mathcal{B}_{F}')$, $P = \mathcal{P}(\mathcal{B}_{E} \to \mathcal{B}_{E}')$, et $Q = \mathcal{P}(\mathcal{B}_{F} \to \mathcal{B}_{F}')$

    Alors
    \begin{equation}
      U' = Q^{-1} U P
    \end{equation}
  ]{Preuve de la formule de changement de base pour une application linéaire, cas particulier d'un endomorphisme lu dans la même base au départ et à l'arrivée.}


  Soit $X \in \mathcal{M}_{n, 1}(\mathbb{K})$, où $\dim E = n$.
  Posons $x = \Psi_{\mathcal{B}_{E}}^{-1}(X)$ et $Y = \Psi_{\mathcal{B}_{F}}(u(x))$.

  Puisque $U = \mathrm{mat}(u, \mathcal{B}_{E}, \mathcal{B}_{F})$, $Y = UX$

  Posons $X' = \Psi_{\mathcal{B}_{E}'}(x)$et $Y' = \Psi_{\mathcal{B}_{F}'}(u(x))$.
  La formule pour le changement de base pour les vecteurs donne $X = PX'$ et $Y =QY'$
  Donc, puisque $U' = \mathrm{mat}(u, \mathcal{B}_{E}', \mathcal{B}_{F}')$

  \begin{align*}
    Y'                 & = U' X'                               \\
    \implies Q^{-1}Y   & = U' P^{-1} X                         \\
                       & \text{ puisque }Y=UX                  \\
    \implies Q^{-1}U X & = U' P^{-1}X                          \\
                       & \text{ en particularisant pour }I_{n} \\
    \implies Q^{-1}U   & = U' P^{-1}                           \\
    \implies U'        & = Q^{-1}UP
  \end{align*}

\end{question_kholle}

\begin{question_kholle}{Montrer que la trace de $AB$ est égale à la trace de $BA$ (deux matrices carrées), et application à la définition de la trace de deux endomorphismes}
  Soient $(A, B) \in \mathcal{M}_{n, p}(\mathbb{K})\times \mathcal{M}_{p,n}(\mathbb{K})$. Alors $\mathrm{Tr}(AB)= \mathrm{Tr}(BA)$

  \begin{itemize}[label=$\lozenge$]
    \item Preuve de l'égalité de la trace
          \begin{align*}
            \mathrm{Tr}(AB) = \sum_{i=1}^{n}[A\times B]_{i,i} = \sum_{i=1}^{n}\sum_{k=1}^{p}A_{i,k}B_{k,i}=\sum_{k=1}^{p}\sum_{i=1}^{n}B_{k, i}A_{i, k}= \sum_{k=1}^{p}[B\times A]_{kk}= \mathrm{Tr}(BA)
          \end{align*}


    \item Soit $E$ un espace vectoriel de dimension finie $u \in \mathcal{L}_{\mathbb{K}}(E)$. Soit $\mathcal{B}_{0}$ une base de $E$ fixée quelconque
          Posons $\lambda = \mathrm{Tr}(\mathrm{mat}(u, \mathcal{B}_{0})$)
            Soit $\mathcal{B}$ une autre base de $E$ fixée quelconque, considérons $P = \mathcal{P}(\mathcal{B}_{0} \to \mathcal{B})$ la matrice de passage de $\mathcal{B}_{0}$ à $\mathcal{B}$.
          D'après la formule de changement de base
          $$
            \mathrm{mat}(u, \mathcal{B}) = P^{-1} \times \mathrm{mat}(u, \mathcal{B}_{0})\times P
          $$
          donc

          \begin{align*}
            \mathrm{Tr}(\mathrm{mat}(u, \mathcal{B})) & = \mathrm{Tr}(P^{-1} \mathrm{mat}(u, \mathcal{B}_{0})P)                                      \\
                                                      & = \mathrm{Tr}(\mathrm{mat}(u, \mathcal{B}_{0})P P^{-1}) \text{ d'après la preuve précédente} \\
                                                      & = \mathrm{Tr}(\mathrm{mat}(u, \mathcal{B}_{0}))
          \end{align*}

          D'où l'existence de la trace $\lambda$ commune à toutes les matrices représentant $u$ dans la même base au départ et à l'arrivée.
          On a évidemment unicité de ce scalaire, que l'on apelle la trace de l'endomorphisme $u$.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Égalité rang trace pour un projecteur}
  Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie $n \in \mathbb{N}^{*}$, et $p$ un projecteur de $E$.

  Puisque $p$ est un projecteur, on peut l'expliciter selon son image et son noyau

  $$
    p \left|\begin{matrix} E &= &\mathrm{Im}(p) &\oplus &\mathrm{Ker}(p) &\to &E \\ x &= &x_{I} &+ &x_{K} &\mapsto &x_{I} \end{matrix}\right.
  $$
  Notons donc $r = \dim \mathrm{Im}(p) = \mathrm{rg}(p)$. Le théorème d'existence de base assure l'existence de $(e_{1}, \dots, e_{r})$ base de $\mathrm{Im}(p)$, de même, en notant $(e_{r+1}, \dots, e_{n})$ une base de $\mathrm{Ker}(p)$, puisque les espaces sont supplémentaires, on sait que $\mathcal{B}= (e_{1}, \dots, e_{r}, e_{r+1},\dots, e_{n})$ est une base de $E$.

  Ainsi
  $$
    \mathrm{mat}(p, \mathcal{B}) = \left[ \begin{array}{cccc|ccc}
        1        & 0      & \dots  & 0      & 0      & \dots  & 0      \\
        0        & 1      & \dots  & 0      & 0      & \dots  & 0      \\
        \vdots   & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        0        & 0      & \dots  & 1      & 0      & \dots  & 0      \\
        \hline 0 & 0      & \dots  & 0      & 0      & \dots  & 0      \\
        \vdots   & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        0        & 0      & \dots  & 0      & 0      & \dots  & 0
      \end{array} \right] = \left[ \begin{array}{c|c}
        I_{r}             & 0_{r, n-r}   \\
        \hline 0_{n-r, r} & 0_{n-r, n-r}
      \end{array} \right] = J_{r}(n,n)
  $$
  Donc $\mathrm{Tr}(p) = \mathrm{Tr}(\mathrm{mat}(p, \mathcal{B})) = r = \mathrm{rg}(p)$

\end{question_kholle}

\begin{question_kholle}[{
        Soit $(n, p) \in \left(\N^*\right)^2$.
        Soit $A \in \mathcal{M}_{n, p}(\mathbb{K})$. Posons $r = \mathrm{rg} A$.
        \begin{equation}
          \exists (P, Q) \in GL_n(\K) \times GL_p(\K): \
          A = P J_r Q
        \end{equation}
        où $J_r$ (avec $r \in \lient 0; \min(n,p) \rient$) est la notation raccourcie de $J_r(n, p)$ définie par
        \begin{equation*}
          J_r(n, p)
          = \left[ \begin{array}{c|c}
              I_r        & 0_{r, p-r}   \\
              \hline
              0_{n-r, r} & 0_{n-r, p-r}
            \end{array} \right]
          = \left[ \begin{array}{cccc|ccc}
              1        & 0      & \dots  & 0      & 0      & \dots  & 0      \\
              0        & 1      & \dots  & 0      & 0      & \dots  & 0      \\
              \vdots   & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
              0        & 0      & \dots  & 1      & 0      & \dots  & 0      \\
              \hline 0 & 0      & \dots  & 0      & 0      & \dots  & 0      \\
              \vdots   & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
              0        & 0      & \dots  & 0      & 0      & \dots  & 0
            \end{array} \right]
        \end{equation*}
      }]
  {Décomposition $P J_r Q$}

  Commençons par démontrer le lemme suivant, $\forall r \in \lient 0; \min(n,p) \rient, \ \mathrm{rg}(J_r(n, p)) = r$
    \begin{equation*}
      \mathrm{rg}(J_r(n, p))
      = \dim \; \Vect \left\{
      \begin{bmatrix}
        1 \\ 0 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \\ 0
      \end{bmatrix} ,
      \begin{bmatrix}
        0 \\ 1 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \\ 0
      \end{bmatrix} ,
      \ldots ,
      \begin{bmatrix}
        0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0
      \end{bmatrix} ,
      0_{n, 1},
      \ldots,
      0_{n, 1}
      \right\}
      = \dim \; \Vect \hspace{-10mm} \underbrace{ \left\{
        E^{1,1},
        E^{2,1},
        \ldots,
        E^{r,1}
        \right\} }
      _{ \begin{array}{c}
          \text{sous-famille de la base canonique de} \\
          \mathcal{M}_{n,1}(\K) \text{ donc famille libre}
        \end{array} } \hspace{-10mm}
      = r
    \end{equation*}

    \bigbreak \bigbreak
    Notons  $\hat{a}$ l'application linéaire de $\K^p$ dans $\K^n$ canoniquement associée à $A$ de sorte que $A = \mathrm{mat}(\hat{a}, \mathcal{B}_{c, \K^p}, \mathcal{B}_{c, \K^n})$. Nous devons chercher deux bases $\mathcal{B}_1$ de $\K^p$ et $\mathcal{B}_2$ de $\K^n$ \tq+*
    $$
    \mathrm{mat}(\hat{a}, \mathcal{B}_1, \mathcal{B}_2)
    = \left[ \begin{array}{cccc|ccc}
        1      & 0      & \ldots & 0      & 0      & \ldots & 0      \\
        0      & \ddots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0      & \vdots & \ddots & \vdots \\
        0      & \ldots & 0      & 1      & 0      & \ldots & 0      \\
        \hline
        0      & \ldots & \ldots & 0      & 0      & \ldots & 0      \\
        \vdots & \ddots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        0      & \ldots & \ldots & 0      & 0      & \ldots & 0
      \end{array} \right]
    = J_r
    $$

    Le théorème du rang appliqué à $\hat{a}$ donne $\dim \K^p = \mathrm{rg} \hat{a} + \dim \ker \hat{a}$. Or $\mathrm{rg} \hat{a} = \mathrm{rg} A = r$. Donc $\dim \ker \hat{a} = p - r$.
    Fixons $E_1$ un sous-espace de $\K^p$ supplémentaire de $\ker \hat{a}$ ainsi $\K^p = E_1 \oplus \ker \hat{a}$ et $\dim E_1 = \dim \K^p - \dim \ker \hat{a} = p - (p-r) = r$.

    Choisissons $\left(f_1, \ldots, f_r\right)$ une base de $E_1$ et $\left(f_{r+1}, \ldots, f_p\right)$ une base $\ker \hat{a}$.
    Posons $\mathcal{B}_1 = \left(f_1, \ldots, f_r, f_{r+1}, \ldots f_p\right)$ ce qui est bien une base car $E_1$ et $\ker \hat{a}$ sont supplémentaires.

    $\Im \hat{a}
    = \Vect \left\{\hat{a}(f_1), \ldots, \hat{a}(f_r), \underbrace{\hat{a}(f_{r+1})}_{= 0_{\K^n}}, \ldots, \underbrace{\hat{a}(f_p)}_{= 0_{\K^n}}\right\}
    = \Vect \left\{\hat{a}(f_1), \ldots, \hat{a}(f_r)\right\}$. Donc $\left(\hat{a}(f_1), \ldots, \hat{a}(f_r)\right)$ est un famile génératrice de cardinal $r$ de $\Im \hat{a}$ qui est un espace de dimension $r$. Donc $\left(\hat{a}(f_1), \ldots, \hat{a}(f_r)\right)$ est une base de $\Im \hat{a}$.

  D'après le théorème de la base incomplète dans $\K^n$, il existe une famille $\left(g_{r+1}, \ldots, g_n\right) \in \left(\K^n\right)^{n-r}$ \tq+ $\left(\hat{a}(f_1), \ldots, \hat{a}(f_r), g_{r+1}, \ldots, g_n\right)$ est une base de $\K^n$ que l'on notera $\mathcal{B}_2$.

  Ainsi
  \begin{equation*}
    \mathrm{mat}(\hat{a}, \mathcal{B}_1, \mathcal{B}_2)
    = \left[ \begin{array}{c|c}
        I_r        & 0_{r, p-r}   \\
        \hline
        0_{n-r, r} & 0_{n-r, p-r}
      \end{array} \right]
    = J_r(n, p)
  \end{equation*}

  Posons $P = \mathcal{P} \left( \mathcal{B}_{c, \K^n} \to \mathcal{B}_2 \right) \in GL_n(\K)$ et $Q = \mathcal{P} \left( \mathcal{B}_2 \to \mathcal{B}_{c, \K^p} \right) \in GL_p(\K)$.
  Appliquons la formule de changement de base :
  $\mathrm{mat}(\hat{a}, \mathcal{B}_{c, \K^p}, \mathcal{B}_{c, \K^n})
    = \mathcal{P} \left( \mathcal{B}_{c, \K^n} \to \mathcal{B}_2 \right)
    \times \mathrm{mat}(\hat{a}, \mathcal{B}_1, \mathcal{B}_2)
    \times \mathcal{P} \left( \mathcal{B}_1 \to \mathcal{B}_{c, \K^p} \right)$.
  Ainsi
  \begin{equation*}
    A = P \times J_r(n, p) \times Q
  \end{equation*}
\end{question_kholle}

\begin{question_kholle}[{
        Soient $(A, B) \in \mathcal{M}_{n, p}(\mathbb{K})^2$. $A$ est équivalente à $B$ s'il existe $(P, Q) \in GL_n(\K) \times GL_p(\K)$ \tq* $B = Q^{-1} A P$.

        Montrons que deux matrices sont équivalentes \ssi elles ont le mêmes rangs, que "être équivalente à" est une relation d'équivalence, qu'il y a $\min(n, p) + 1$ classes et que $\left(J_r(n, p)\right)_{r \in \lient 0; \min(n, p) \rient}$ est un système de représentants de classes.
      }]
  {Deux matrices sont équivalentes \ssi elles ont le mêmes rangs. Système de représentants des classes de la relation d'équivalence "être équivalente à"}

  Notons $\sim$ la relation "être équivalente à".

  $\sim$ est :
  \begin{liste}
    \item réflexive car $\forall A \in \mathcal{M}_{n, p}(\K), A = I_n^{-1} A I_p$ et $(I_n, I_p) \in Gl_n(\K) \times Gl_p(\K)$.
    \item symétrique car soit $(A, B) \in \mathcal{M}_{n, p}(\K)^2$ \tq+* $A \sim B$.
    Alors $\exists (Q, P) \in GL_n(\K) \times GL_p(\K) : B = Q^{-1} A P$.D'où $A = Q B P^{-1}$.
    Donc $A = \left(Q^{-1}\right)^{-1} B p^{-1}$.
    Or $\left(Q^{-1}, P^{-1}\right) \in GL_n(\K) \times GL_p(\K)$. Ainsi $B \sim A$.
    \item transitive car soient $A, B, C) \in \mathcal{M}_{n, p}(\K)^3$ \tq+* $A \sim B$ et $B \sim C$.
    Alors $\exists (P, Q) \in GL_n(\K) \times GL_p(\K) : B = Q^{-1} A P$ et $\exists (S, R) \in GL_n(\K) \times GL_p(\K) : C = S^{-1} B R$.
    D'où $C = S^{-1} \left(Q^{-1} A P\right) R = (QS)^{-1} A (PR)$.
    Or $(QS, PR) \in GL_n(\K) \times GL_p(\K)$.
    Donc $A \sim C$.
  \end{liste}

  Soient $(A, B) \in \mathcal{M}_{n, p}(\K)^2$ \tq+* $\mathrm{rg}(A) = \mathrm{rg}(B) = r$.
  La multiplication par une matrice à droite et à gauche par des matrices inversibles ne modifie pas le rang donc, d'après la décomposition $P J_r Q$, $J_r \sim A$ et $J_r \sim B$.
  Par symétrie et par transitivité, $A \sim B$.

  Ainsi, $\left(J_r(n, p)\right)_{r \in \lient 0; \min(n, p) \rient}$ est bien un système de représentants de classes.
  Cette famille a pour cardinal $\min(n, p) + 1$.
\end{question_kholle}
\pagebreak\section{Semaine 26}

%Cette semaine avait trois jours fériés : les khôlles furent annulées.
\emph{La semaine décimée}
\begin{verse}
  Sous les cieux printaniers de mai,\\
  Vient la semaine tant attendue,\\
  Où l'Ascension, d'un souffle léger,\\
  Réduit les cours, moments suspendus.\\
\end{verse}
\begin{verse}
  Les classes prépas, si intensives,\\
  Prennent une pause, presque inédite,\\
  Les jours se parent de l'harmonie,\\
  D'une trêve aux heures décrépites.\\
\end{verse}
\begin{verse}
  Trois jours, juste une poignée,\\
  Suffisent à briser la cadence,\\
  Les esprits se libèrent, apaisés,\\
  De l’ordinaire lourde exigence.\\
\end{verse}
\begin{verse}
  On appelle cette semaine, décimée,\\
  Une parenthèse dans la rigueur,\\
  Où les horloges semblent arrêter,\\
  Le temps, éphémère douceur.\\
\end{verse}
\begin{verse}
  Les étudiants, d'un souffle profond,\\
  Respirent la clémence du printemps,\\
  Ils laissent de côté leurs crayons,\\
  Pour goûter à l’instant présent.\\
\end{verse}
\begin{verse}
  Oh, douce pause, lumineuse évasion,\\
  Dans la frénésie de l’éducation,\\
  Tu offres un repos bien mérité,\\
  Avant de replonger dans la densité.\\
\end{verse}
\begin{verse}
  Que chaque année revienne encore,\\
  Cette semaine au charme éthéré,\\
  Bénie par l’Ascension d’alors,\\
  Une oasis dans l’immensité.\\
\end{verse}

\pagebreak\section{Semaine 27}

\begin{question_kholle}
  [{Soit $f \in \ContM{[a;b]}{}$. \\
        L'ensemble $\{ |f(t)| \;|\; t \in [a;b] \}$ admet une borne supérieur notée $\norminf{f}$.}]
  {Norme uniforme d'une fonction continue par morceaux}

  Montrons que sur chaque morceau, $f$ est bornée.

  Soit $\sigma = (x_i)_{0 \leqslant i \leqslant N} \in \mathcal{S}([a;b])$ adaptée à $f$.
  Soit $i \in \lient 0; N-1 \rient$. Posons $f_i = f_{|]x_i;x_{i+1}[}$.
  $f$ étant continue par morceaux, $\exists (l_i^+, l_{i-1}^-) \in \R^2 : \textlim{x}{x_i^+} f_i(x) = l_i^+ \wedge \textlim{x}{x_{i+1}^-} f_i(x) = l_{i+1}^-$.
  Nous pouvons donc prolonger $f_i$ en $\tilde{f_i}$ par continuité en $x_i$ et en $x_{i+1}$.
  Comme $f \in \Cont{0}{[a;b]}{}$, le théorème de Weierstrass s'applique : $\Im \tilde{f_i}$ est bornée (donc $f_i$ aussi). Ainsi $\norminf{f_i}$ est bien défini.

  \noindent $\{ |f(t)| \;|\; t \in [a;b] \}$ est : \begin{itemize}
    \item une partie de \R
    \item non vide car contenant $|f(x)|$.
    \item majorée par $\max \left( \{\norminf{f_i} | i \in \lient 0; N-1 \rient\} \cup \{\norminf{f_i} | i \in \lient 0; N-1 \rient\} \right)$ (ensemble admettant bien un plus grand élément puisque fini)
  \end{itemize}
  Donc $\norminf{f}$ est bien définie.
  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \draw[->] (-0.5, 0) -- (5, 0);
      \draw[->] (0, -0.5) -- (0, 5);
      \draw (4, -0.1) node[anchor=north] {$1$} -- (4, 0.1);
      \draw (2, -0.1) node[anchor=north] {$\nicefrac{1}{2}$} -- (2, 0.1);
      \draw (-0.1, 4) node[anchor=east] {$1$} -- (0.1, 4);

      \draw[red] (0, 0) -- (2, 4) -- (4, 0);
      \filldraw[white, draw=red, densely dotted] (2, 3.95) circle (3pt);
      \filldraw[red] (2, 0) circle (2pt);
    \end{tikzpicture}
    \caption{$\norminf{f}$ peut ne pas être atteinte}
  \end{figure}
\end{question_kholle}

\begin{question_kholle}
  [Soit {$f \in \Cont{0}{[a;b]}{}$}.
    \begin{propositions}
      \item $\forall \varepsilon \in \R_+^*, \
        \exists \chi \in \mathcal{E}({[a;b]}, \R) :
        \norminf{f - \chi} \leqslant \varepsilon$
      \item $\forall \varepsilon \in \R_+^*, \
        \exists (\varphi, \psi) \in \mathcal{E}({[a;b]}, \R)^2 :
        \left\{ \begin{matrix}
          \varphi \leqslant f \leqslant \psi \\
          \norminf{\psi - \varphi} \leqslant \varepsilon
        \end{matrix} \right.$
    \end{propositions}]
  {Lemme d'approximation uniforme d'un fonction continue sur un segment par une fonction en escalier}

  Soit $f \in \Cont{0}{[a;b]}{}$. Soit $\varepsilon \in \R_+^*$ \fq. \\
  $(i)$ D'après le théorème de Heine, $f \in \ContU{[a;b]}{}$. Écrivons la définition de uniformément continue pour $\varepsilon$ :
  \begin{equation*}
    \exists \eta \in \R_ +^* : \ \forall (x, y) \in [a;b]^2, \
    |x - y| \leqslant \eta \implies |f(x) - f(y)| \leqslant \varepsilon
  \end{equation*}

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}
      \begin{axis}[
          axis lines = center,
          xlabel = $x$,
          ylabel = {$f(x)$},
          width=12cm,
          height=5cm
        ]
        \addplot[
          domain=0:20,
          samples=200,
          color=green,
        ]
        {2*cos((floor(x/2)*2+1)*45) + floor(x/2)+(1/2)};
        \addplot[
          domain=0:20,
          samples=200,
          color=red,
        ]
        {2*cos(x*45) + x/2};
      \end{axis}
    \end{tikzpicture}
    \caption{Fonction en escalier "approximant" une fonction continue}
  \end{figure}

  \noindent Cherchons $N$ \tq $\frac{b - a}{N} \leqslant 2 \eta$. C'est-à-dire $N \geqslant \frac{b - a}{2 \eta}$.
  Posons donc $N = \lceil \frac{b - a}{2 \eta} \rceil$ et $\eta' = \frac{b-a}{N}$ de sorte que $\eta' \leqslant 2\eta$.

  Définissons $\chi \in \mathcal{E}({[a;b]}, \R)$ par
  \begin{equation*}
    \chi \left| \begin{array}{ccl}
      [a;b] & \rightarrow & \R                                                                                                                                                     \\
      x     & \mapsto     & \left\{ \begin{array}{ll}
                                      f(x)                                                                                        & \text{ si } \exists n \in \N : \ x = a + n \eta' \\
                                      f\left(a + \eta' \left( \lfloor \frac{x-a}{\eta'} \rfloor + \nicefrac{1}{2} \right) \right) & \text{ sinon }
                                    \end{array} \right.
    \end{array} \right.
  \end{equation*}
  Ceci est bien une fonction en escalier car $(a + k \eta')_{0 \leqslant k \leqslant N}$ est une subdivision adaptée. En effet, $\forall k \in \lient 0; N-1 \rient, \ f_{|]a+k\eta';a+(k+1)\eta'[} =f\left( a + \eta' \left( \lfloor \frac{x-a}{\eta'} \rfloor + \nicefrac{1}{2} \right) \right) \cdot \widetilde{1}_{]a+k\eta';a=(k+1)\eta'[}$.

  Soit $x \in [a; b]$.
  Si $\exists n \in \N : \ x = a + n \eta'$ alors $|f(x) - \chi(x)| = 0$.
  Sinon $0 \leqslant \frac{x - a} {\eta'} - \lfloor \frac{x-a}{\eta'} \rfloor \leqslant 1$.
  D'où $0 \leqslant (x - a) - \eta' \lfloor \frac{x-a}{\eta'} \rfloor \leqslant \eta'$.
  Donc, en enlevant $\nicefrac{\eta'}{2}$, $- \frac{\eta'}{2} \leqslant a + \eta' \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \leqslant \frac{\eta'}{2}$.
  Par définition de $\eta'$, \ $- \eta \leqslant a + \eta' \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \leqslant \eta$.
  Par définition de $\eta$, on a $|f(x) - f\left(a + 2 \eta \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \right)| \leqslant \varepsilon$.

  Ainsi, nous avons bien $\norminf{f - \chi} \leqslant \varepsilon$.
  \bigbreak

  $(ii)$ Écrivons la définition de uniformément continue pour $\varepsilon$ :
  \begin{equation*}
    \exists \eta \in \R_ +^* : \ \forall (x, y) \in [a;b]^2, \
    |x - y| \leqslant \eta \implies |f(x) - f(y)| \leqslant \varepsilon
  \end{equation*}
  Définissons $\varphi \in \mathcal{E}({[a;b]}, \R)$ par
  \begin{equation*}
    \left| \begin{matrix}
      [a;b] & \rightarrow & \R                                                                                                                                                                                                \\
      x     & \mapsto     & \left\{ \begin{matrix}
                                      f(x)                                                                                                                                    & \text{ si } \exists n \in \N : \ x = a + n \eta \\
                                      \inf f\left( \; ]a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1 \right) [ \; \right) & \text{ sinon }
                                    \end{matrix} \right.
    \end{matrix} \right.
  \end{equation*}
  Définissons $\psi \in \mathcal{E}({[a;b]}, \R)$ par
  \begin{equation*}
    \left| \begin{matrix}
      [a;b] & \rightarrow & \R                                                                                                                                                                                               \\
      x     & \mapsto     & \left\{ \begin{matrix}
                                      f(x)                                                                                                                                   & \text{ si } \exists n \in \N : \ x = a + n \eta \\
                                      \sup f\left( \; ] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[ \; \right) & \text{ sinon }
                                    \end{matrix} \right.
    \end{matrix} \right.
  \end{equation*}
  Ces deux fonctions sont bien définies car $f_{|] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[}$ est continue donc, d'après le théorème de Weiertraß, son image admet une borne inférieure et une borne supérieure.
  Elle sont bien en escalier.

  Par définition des bornes inférieures et supérieures, nous avons $\varphi \leqslant f \leqslant \psi$.
  De plus, pour $x \in [a;b]$ \fq, $f_{|] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[}$ se prolonge par continuité et, d'après le théorème de Weiertraß, atteint ses bornes. Notons $f_i$ et $f_s$ les antécédents respectifs des bornes.
      $(f_i, f_s) \in ] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[ ^2$ donc $\left|f_i - f_s\right| \leqslant \eta$.
  D'où $\left| f(f_i) - f(f_s) \right| \leqslant \varepsilon$.

  Ainsi, nous avons bien $\norminf{\psi - \varphi} \leqslant \varepsilon$.
\end{question_kholle}
\begin{question_kholle}[{Soit $f \in \mathcal{CM}([a, b], \mathbb{R})$
        Posons

        \begin{align*}
          \mathcal{E}^{-}(f)=\{ \varphi \in \mathcal{E}([a, b],\mathbb{R}) \mid \varphi \leqslant f \} &
                                                                                                       & \mathcal{E}^{+}(f)=\{ \varphi \in \mathcal{E}([a, b],\mathbb{R}) \mid \varphi \geqslant f \}
        \end{align*}

        et

        \begin{align*}
          I^{-}(f)= \left\{ \int_{a}^{b} \varphi(u) \, \mathrm du \Bigg| \varphi \in \mathcal{E}^{-}(f)  \right\}
           &  &
          I^{+}(f)= \left\{ \int_{a}^{b} \varphi(u) \, \mathrm du \Bigg| \varphi \in \mathcal{E}^{+}(f)  \right\}
        \end{align*}

        Alors $\sup I^{-}(f) = \inf I^{+}(f)$ que l'on notera $\int_{a}^{b} f(u) \, \mathrm du$}]{Définition de l'intégrale de Darboux}

  \begin{itemize}[label=$\lozenge$]
    \item Bonne définition des objets
          \begin{itemize}[label=$\star$]
            \item $I^{-}(f)$ est une partie de $\mathbb{R}$
            \item Non vide car:
                  $$
                    \forall x \in [a, b], f(x) \geqslant - \|f\|_{\infty, [a, b]} \implies \left( \int_{a}^{b} - \|f\|_{\infty,[a, b]} \, \mathrm dt  \right)  \in I^{-}(f)
                  $$
            \item majorée: soit $\varphi \in \mathcal{E}^{-}(f)$ fixé quelconque.

                  \begin{align*}
                    \forall x \in [a, b], \varphi(x)\leqslant f(x) & \leqslant \|f\|_{\infty, [a, b]}                              \\
                    \implies \varphi                               & \leqslant \|f\|_{\infty[a, b]}                                \\
                    \implies \int_{a}^{b} \varphi(u) \, \mathrm du & \leqslant  \int_{a}^{b} \|f\|_{\infty, [a, b]}  \, \mathrm dt
                  \end{align*}
          \end{itemize}

          On procède de la même manière pour la borne inf de $I^{+}(f)$

    \item De plus, $\sup I^{-}(f) \leqslant \inf I^{+}(f)$

          Soient $(\varphi, \psi) \in \mathcal{E}^{-}(f)\times \mathcal{E}^{+}(f)$ fixés quelconques


          \begin{align*}
             & \forall x \in [a, b], \varphi(x) \leqslant f(x) \leqslant \psi(x)                                                                                                           \\
             & \implies \forall \varphi \in \mathcal{E}^{-}(f), \varphi \leqslant \psi                                & \implies \psi \text{ majore } \mathcal{E}^{-}(f)                   \\
             &                                                                                                        & \implies \int_{a}^{b} \psi(u) \, \mathrm du  \text{ majore } I^{-} \\
             & \implies \sup I^{-} \leqslant \int_{a}^{b} \psi(u) \, \mathrm du                                                                                                            \\
             & \implies \forall \psi \in \mathcal{E}^{+}(f), \sup I^{-} \leqslant \int_{a}^{b} \psi(u)  \, \mathrm du & \implies \sup I^{-} \text{ minore } I^{-}                          \\
             &                                                                                                        & \implies \sup I^{-} \leqslant \inf I^{+}
          \end{align*}


    \item Soit $\varepsilon > 0$ fixé quelconque.
          Appliquons le lemme d'approximation uniforme d'une fonction continue par morceaux par une fonction en escalier pour $\varepsilon \leftarrow \frac{\varepsilon}{b-a}$ quelconque:

          $$
            \exists (\varphi, \psi) \in \mathcal{E}([a, b], \mathbb{R})^{2} : \left\{ \begin{array}{ll}
              \varphi \leqslant f \leqslant \psi \\
              \|\varphi - \psi\| _{\infty, [a, b]} \leqslant \frac{\varepsilon}{b-a}
            \end{array}\right.
          $$

          Cela implique nécessairement que $\varphi \in \mathcal{E}^{-}(f)$ et $\psi \in \mathcal{E}^{+}(f)$.

          $$
            \left\{ \begin{array}{ll}
              \varphi \in \mathcal{E}^{-}(f) \\
              \psi \in \mathcal{E}^{+}(f)
            \end{array}\right. \implies \left\{ \begin{array}{ll}
              \int_{a}^{b} \varphi (u) \, \mathrm du \leqslant \sup I^{-} \\
              \int_{a}^{b} \psi(u) \, \mathrm du \geqslant \inf I^{+}
            \end{array}\right.
          $$

          Donc

          \begin{align*}
            \int_{a}^{b} \varphi(u) \, \mathrm du  \leqslant \sup I^{-} \leqslant \inf I^{+} \leqslant \int_{a}^{b} \psi(u) \, \mathrm du     \\
            \implies 0 \leqslant \inf I^{+} - \sup I^{-} \leqslant \int_{a}^{b} \psi(u) \, \mathrm du - \int_{a}^{b} \varphi(u) \, \mathrm du \\
            \implies 0 \leqslant \inf I^{+} - \sup I^{-} \leqslant \int_{a}^{b} \psi(u) - \varphi(u) \, \mathrm du                            \\
            \implies 0 \leqslant \inf I^{+} - \sup I^{-} \leqslant \int_{a}^{b} \frac{\varepsilon}{b-a} \, \mathrm du                         \\
            \implies 0 \leqslant \inf I^{+} - \sup I^{-} \leqslant \frac{(b-a)\varepsilon}{b-a} = \varepsilon
          \end{align*}


          Donc en passant à la limite, on retrouve $\inf I^{+} - \sup I^{+} = 0$, d'où l'égalité attendue.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer qu'une fonction positive ou nulle, continue sur un segment et d'intégrale nulle sur ce segment est identique- ment nulle sur ce segment}
  Par l'absurde supposons qu'il existe $f \in \mathcal{C}^{0}([a, b], \mathbb{R})$ telle que $\int_{a}^{b} f(u) \, \mathrm du = 0$ et $f \geqslant 0$ sur $[a, b]$ et $f \neq \tilde{0}$
  Alors $\exists x_{0} \in [a, b]: f(x_{0}) \neq 0 \implies f(x_{0})>0$
  Appliquons la définition de la continuité de $f$ en $x_{0}$ pour $\varepsilon \leftarrow \frac{f(x_{0})}{2}$
  $$
    \exists \eta >0 : \forall x \in [a, b] \cap [x_{0}-\eta, x_{0}+\eta], \lvert f(x)-f(x_{0}) \rvert \leqslant \frac{f(x_{0})}{2}
  $$
  donc
  $$
    \forall x \in [a, b] \cap [x_{0}-\eta, x_{0}+\eta], f(x) \geqslant \frac{f(x_{0})}{2}>0
  $$

  \begin{itemize}
    \item Ainsi, si $x_{0} \in ]a, b[$

          \begin{align*}
            \int_{a}^{b} f(u) \, \mathrm du & =  \underbrace{ \int_{a}^{x_{0}-\eta} f(u) \, \mathrm du }_{ \geqslant 0 } + \int_{x_{0}-\eta}^{x_{0}+\eta} f(u) \, \mathrm du + \underbrace{ \int_{x_{0}+\eta}^{b} f(u) \, \mathrm du }_{ \geqslant 0 } \\
                                            & \geqslant \int_{x_{0}-\eta}^{x_{0}+\eta}f(u)  \, \mathrm du \geqslant \int_{x_{0}-\eta}^{x_{0}+\eta} \frac{f(x_{0})}{2}  \, \mathrm du \geqslant 2 \eta \frac{f(x_{0})}{2} \geqslant \eta f(x_{0}) > 0
          \end{align*}


    \item Si $x_{0} \in \{ a, b \}$ on effectue le même raisonnement
          $$
            \int_{a}^{a+\eta} f(u) \, \mathrm du \geqslant \frac{\eta f(x_{0})}{2} > 0
          $$
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Inégalité de Cauchy Schwartz pour les fonctions continues par morceaux}
  Soient $(f, g) \in \mathcal{CM}([a, b], \mathbb{R})^{2}$
  Posons
  $$
    \forall t \in \mathbb{R}, P(t) = \int_{a}^{b} (f+tg)^{2}(u) \, \mathrm du
  $$
  \begin{align*}
    P(t) = \int_{a}^{b} (f(u)+t\times g(u))^{2} \, \mathrm du & =
    \int_{a}^{b} f(u)^{2} + 2tg(u)f(u)+ t^{2} g(u)^{2} \, \mathrm du                                                                                                                      \\
                                                              & = \int_{a}^{b} f(u)^{2} \, \mathrm du + 2t\int_{a}^{b} f(u)g(u) \, \mathrm du + t^{2} \int_{a}^{b} g(u)^{2} \, \mathrm du
  \end{align*}
  \begin{itemize}[label=$\lozenge$]
    \item Si $\int_{a}^{b} g(u)^{2} \, \mathrm du = 0$, $P$ est un polynôme affine de signe positif (intégrale d'une fonction positive) donc sa pente est nulle, donc $\int_{a}^{b} f(u)g(u) \, \mathrm du =0$ donc l'inégalité de cauchy schwartz est vraie

    \item Sinon $P$ est un polynôme de degré $2$, positif ou nul, donc le discriminant $\Delta \leqslant 0$ et $\int_{a}^{b} g(u)^{2} \, \mathrm du \geqslant 0$
          donc
          $$
            4\left( \int_{a}^{b} g(u)f(u) \, \mathrm du  \right)^{2} - 4 \int_{a}^{b} f^{2}(u) \, \mathrm du \int_{a}^{b} g^{2}(u) \, \mathrm du \leqslant 0
          $$
          Ce qui prouve l'égalité attendue.
  \end{itemize}
  \begin{itemize}[label=$\lozenge$]
    \item Supposons qu'il y a égalité dans l'inégalité de Cauchy-Schwartz, alors
          \begin{itemize}
            \item Si $\int_{a}^{b} g^{2}(u) \, \mathrm du \neq 0$
                  $$
                    \Delta = 4\left( \int_{a}^{b} g(u)f(u) \, \mathrm du  \right)^{2} - 4 \int_{a}^{b} f^{2}(u) \, \mathrm du \int_{a}^{b} g^{2}(u) \, \mathrm du =0
                  $$
                  Donc $P$ est un polynôme de degré 2 de discriminant nul : il admet une racine double $t_{0}$
                  Ainsi,
                  $$
                    P(t_{0}) = 0 \implies \int_{a}^{b} (f+t_{0}g)^{2}(u) \, \mathrm du = 0
                  $$
                  Mais, $(f+t_{0}g)^{2}$ est une fonction positive, et continue sur $[a, b]$, donc elle est nulle sur $[a, b]$.
                  Donc $f+t_{0}g = \tilde{0}$ dong $(f, g)$ est liée.

            \item Sinon, si $\int_{a}^{b} g^{2}(u) \, \mathrm du = 0$, en remarquant que $g^{2}\geqslant 0$ sur $[a, b]$ et que $g^{2}$ est continue, on retrouve que $g = \tilde{0}$ et donc que $(f, g)$ est liée
          \end{itemize}
    \item Un calcul simple montre que s'il existe une relation de liaison entre deux fonctions continues par morceaux, il y a égalité dans l'inégalité.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Théorème de convergence des sommes de Riemann}
  $f \in \mathcal{C}^{0}([a, b], \mathbb{C}) a\leqslant b$
  En notant $S(f, \sigma, \pi)$ la somme de Riemann de $f$ pour la subdivision pointée $(\sigma, \pi)$.
  $$
    \forall \varepsilon >0, \exists \eta >0: \forall \left\{ \begin{array}{ll}
      \sigma = (x_{i})_{0 \leqslant i \leqslant N} \in \mathcal{S}([a, b]) \\
      \pi = (x_{i}')_{0\leqslant i\leqslant N-1} : x_{i}' \in [x_{i}, x_{i+1 }]
    \end{array}\right. , \delta(\sigma) \leqslant \eta \implies \left\lvert  S(f, \sigma, \pi) - \int_{a}^{b} f(u) \, \mathrm du   \right\rvert \leqslant \varepsilon
  $$

  Soit $\sigma = (x_{j})_{0\leqslant j \leqslant N} \in \mathcal{S}([a, b])$ et $\pi = (x_{i}')_{0 \leqslant i \leqslant N - 1}$ une famille vérifiant $\forall i \in [ \! [ 0, N-1 ] \!], x_{i}' \in [x_{i}, x_{i+1}]$


  \begin{align*}
    \left\lvert  S(f, \sigma, \pi) - \int_{a}^{b} f(t) \, \mathrm dt   \right\rvert & = \left\lvert  \sum_{i=0}^{N-1}f(x_{i}')(x_{i+1}-x_{i}) - \sum_{i=0}^{N - 1}\int_{x_{i}}^{x_{i+1}} f(t) \, \mathrm dt   \right\rvert \\
                                                                                    & = \left\lvert  \sum_{i=0}^{N-1}\int_{x_{i}}^{x_{i+1}} (f(x_{i}')-f(t)) \, \mathrm dt   \right\rvert                                  \\
                                                                                    & \leqslant \sum_{i=0}^{N-1}\left\lvert  \int_{x_{i}}^{x_{i+1}} (f(x_{i}')-f(t)) \, \mathrm dt   \right\rvert                          \\
                                                                                    & \leqslant \sum_{i=0}^{N-1}\int_{x_{i}}^{x_{i+1}} \lvert f(x_{i}') - f(t) \rvert \, \mathrm dt
  \end{align*}


  Soit $\varepsilon > 0$ fixé quelconque, appliquons la continuité uniforme de $f$ pour $\varepsilon \leftarrow \frac{\varepsilon}{b-a}$
  $$
    \exists \eta >0: \forall(x, y) \in [a, b], \lvert x-y \rvert \leqslant \eta \implies \lvert f(x)-f(y) \rvert \leqslant \frac{\varepsilon}{b-a}
  $$
  Fixons un tel $\eta$. Soit $(\sigma, \pi)$ une subdivision pointée de $[a, b]$ fixée quelconque telle que $\delta(\sigma) \leqslant \eta$.
  Soit $i \in [ \! [ 0, N-1 ] \!]$ fixé quelconque.
  $$
    \forall t \in [x_{i}, x_{i+1}], \lvert x_{i}' - t \rvert  \underbrace{ \leqslant }_{ x_{i}\leqslant x_{i}' \leqslant x_{i+1} } \lvert x_{i}-x_{i+1} \rvert \leqslant \delta(\sigma) \leqslant \eta \implies \lvert f(x_{i}') - f(t) \rvert \leqslant \frac{\varepsilon}{b-a}
  $$
  donc,
  $$
    \int_{x_{i}}^{x_{i+1}} \lvert f(x_{i}')-f(t) \rvert  \, \mathrm dt \leqslant \int_{x_{i}}^{x_{i+1}} \frac{\varepsilon}{b-a} \, \mathrm dt = \frac{\varepsilon}{b-a}(x_{i+1}-x_{i})
  $$
  donc
  $$
    \left\lvert  S(f, \sigma, \pi) - \int_{a}^{b} f(t) \, \mathrm dt   \right\rvert \leqslant \sum_{i=0}^{N-1} \int_{x_{i}}^{x_{i+1}} \lvert f(x_{i}')-f(t) \rvert  \, \mathrm dt \leqslant \sum_{i=0}^{N-1} \frac{\varepsilon}{b-a}(x_{i+1}-x_{i})  = \frac{\varepsilon}{b-a} \underbrace{ \sum_{i=0}^{N-1}(x_{i+1}-x_{i}) }_{ b-a } = \varepsilon
  $$
  donc
  $$
    \left\lvert  S(f, \sigma, \pi) - \int_{a}^{b} f(t) \, \mathrm dt   \right\rvert  \leqslant \varepsilon
  $$
\end{question_kholle}

\begin{question_kholle}{Inégalité triangulaire pour les fonctions continues par morceaux à valeurs complexes}

  Soit $f \in \mathcal{CM}([a, b], \mathbb{C})$ cela implique donc que $\lvert f \rvert \in \mathcal{CM}([a, b], \mathbb{C})$
  D'après le lemme d'approximation uniforme d'une fonction par une fonction uniforme
  $$
    \exists (\chi_{k})_{k \in \mathbb{N}} \in (\mathcal{E}([a, b], \mathbb{R}))^{\mathbb{N}} : \left\{ \begin{array}{ll}
      \forall k \in \mathbb{N}, \|f-\chi_{k}\|_{\infty, [a, b]} \leqslant \frac{1}{2^{k}} \\
      \int_{a}^{b} \chi_{k}(u) \, \mathrm du \xrightarrow[k \to \infty]{} \int_{a}^{b} f(u) \, \mathrm du
    \end{array}\right.
  $$

  $$
    \forall k \in \mathbb{N}, \forall x \in [a, b], \lvert \lvert \chi_{k}(x) \rvert - \lvert f(x) \rvert  \rvert \leqslant \lvert f(x) - \chi_{k}(x) \rvert \leqslant \frac{1}{2^{k}}
  $$
  donc
  $$
    \|\lvert f \rvert - \lvert \chi_{k} \rvert \|_{\infty}\leqslant \frac{1}{2^{k}} \implies \int_{a}^{b} \lvert \chi_{k} \rvert (u) \, \mathrm du \xrightarrow[k \to \infty]{} \int_{a}^{b} \lvert f \rvert (u) \, \mathrm du
  $$
  Donc, d'après l'inégalité triangulaire continue pour les fonctions en escalier appliquée aux $\chi_{k}$
  $$
    \underbrace{ \left\lvert  \int_{a}^{b} \chi_{k}(u) \, \mathrm du   \right\rvert }_{ \xrightarrow[k \to \infty]{} \left\lvert  \int_{a}^{b} f(u) \, \mathrm du   \right\rvert }  \leqslant \underbrace{ \int_{a}^{b} \lvert \chi_{k} \rvert (u) \, \mathrm du }_{ \xrightarrow[k \to \infty]{} \int_{a}^{b} \lvert f \rvert (u) \, \mathrm du  }
  $$
  donc par passage à la limite dans l'inégalité
  $$
    \left\lvert  \int_{a}^{b} f(u) \, \mathrm du   \right\rvert  \leqslant \int_{a}^{b} |f(u)| \, \mathrm du
  $$
\end{question_kholle}


\begin{question_kholle}{Existence et unicité de la primitive de $f$ qui s'annule en $a$}
  \begin{itemize}[label=$\lozenge$]
    \item Notons d'abord que
          $$
            F_{a} \left|\begin{array}{ll} I &\to \mathbb{C} \\ t &\mapsto \int_{a}^{x} f(u) \, \mathrm du \end{array}\right.
          $$ est bien définie:
          $$
            \left. \begin{array}{ll}
              f \in \mathcal{C}^{0}(I, \mathbb{C}) \implies f \in \mathcal{CM}(I, \mathbb{C}) \\
              \forall t \in I, [a, t] \subset I \text{ ou } [t, a] \subset I
            \end{array}\right\} \implies \int_{a}^{b} f(u) \, \mathrm du \text{ est bien définie}
          $$

    \item Montrons que $F_{a} \in \mathcal{D}^{1}(I, \mathbb{C})$ et $F_{a}' = f$

          \begin{align*}
            \left\lvert  \frac{F_{a}(t)-F_{a}(t_{0})}{t-t_{0}} -f(t_{0}) \right\rvert & = \left\lvert  \frac{1}{t-t_{0}}\int_{t_{0}}^{t} f(u) \, \mathrm du - f(t_{0})
            \right\rvert                                                                                                                                                                                                    \\
                                                                                      & \leqslant \left\lvert  \frac{1}{\lvert t - t_{0} \rvert }\int_{t}^{t_{0}} \lvert f(u)-f(t_{0}) \rvert  \, \mathrm du   \right\rvert
          \end{align*}


          Soit $\varepsilon > 0$ fixé quelconque. Appliquons la définition de la continuité de $f$ en $t_{0}$
          $$
            \exists \eta > 0: \forall t \in I, \lvert  t  - t_{0} \rvert  \leqslant \eta \implies \lvert f(t)-f(t_{0}) \rvert \leqslant \varepsilon
          $$

          Soit $t \in I$ tel que $\lvert t - t_{0} \rvert \leqslant \eta$, alors $\forall u \in [t_{0}, t] \cup [t, t_{0}], \lvert f(u)-f(t_{0}) \rvert \leqslant \varepsilon$
          \begin{itemize}[label=$\star$]
            \item Si $t_{0} \leqslant t$
                  $$
                    0 \leqslant \int_{t_{0}}^{t} \lvert f(u)-f(t_{0}) \rvert  \, \mathrm du \leqslant \int_{t_{0}}^{t} \varepsilon \, \mathrm du = \varepsilon \lvert t-t_{0} \rvert
                  $$
            \item Si $t \leqslant t_{0}$
                  $$
                    0 \leqslant - \int_{t_{0}}^{t} \lvert f(u)-f(t_{0}) \rvert  \, \mathrm du \leqslant \int_{t}^{t_{0}} \varepsilon \, \mathrm du = \varepsilon \lvert t - t_{0} \rvert
                  $$
          \end{itemize}
          Ainsi, on a montré que
          $$
            \left\lvert  \frac{F_{a}(t)-F_{a}(t_{0})}{t-t_{0}} -f(t_{0}) \right\rvert  \leqslant \varepsilon
          $$
          d'où la convergence du taux d'accroissement.
          Donc $F_{a} \in \mathcal{D}^{1}(I, \mathbb{C})$, et $F_{a}' = f$
          Donc $F_{a}$ est une primitive de $f$ et $F_{a}(a) = \int_{a}^{a} f(u) \, \mathrm du = 0$
    \item Soit $H$ une primitive qui s'annule en $a$.
          $H - F_{a} \in \mathcal{D}^{1}(I, \mathbb{C})$ et $(H - F_{a})' = H' - F_{a}' = f - f = \tilde{0}$
          Ainsi, la dérivée de $H - F_{a}$ est nulle sur $I$, qui est un intervalle, donc
          $$
            \exists c \in \mathbb{C}: \forall t \in I, H(t) - F_{a}(t) = c
          $$ et en particularisant en $a$, on montre que $H(a) - F_{a}(a) = 0 - 0 = 0$.
          Donc $H - F_{a} = \tilde{0}$ donc $H = F_{a}$, ce qui montre l'unicité.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Formule de Taylor avec reste intégral}
  Posons
  $$
    \mathcal{H}_{n} : \forall f \in \mathcal{C}^{n+1}(I, \mathbb{C}), f(x) = \sum_{k=0}^{n} \frac{(x-a)^{k}}{k!}f^{(k)}(a) +\int_{a}^{x} \frac{(x-u)^{n}}{n!}f^{(n+1)}(u) \, \mathrm du
  $$
  \begin{itemize}[label = $\lozenge$]
    \item Initialisation $n \leftarrow 0$
          $$f(x) = f(a) + \int_{a}^{x} f'(u) \, \mathrm du$$ est effectivement vrai d'après le théorème fondamental du calcul intégral

    \item Hérédité: soit $n \in \mathbb{N}$ tel que $\mathcal{H}_{n}$ est vraie.

          Soit $f \in \mathcal{C}^{n+2}$, $f$ est en particulier de classe $\mathcal{C}^{n+1}$ donc en appliquant la propriété de récurrence:
          $$
            f(x) = \sum_{k=0}^{n} \frac{(x-a)^{k}}{k!}f^{(k)}(a) + \int_{a}^{x} \frac{(x-u)^{n}}{n!}f^{(n+1)}(u) \, \mathrm du
          $$
          Intégrons le reste intégral par parties, les fonctions $u : t \mapsto f^{(n+1)}(t)$ et $v : t \mapsto - \frac{(b-t)^{n+1}}{(n+1)!}$ sont de classe $\mathcal{C}^{1}$ sur $I$, avec $u'(t) = f^{(n+2)}(t)$ et $v'(t)= \frac{(x-t)^{n}}{n!}$

          \begin{align*}
            \int_{a}^{x} \frac{(x-u)^{n}}{n!}f^{(n+1)}(u) \, \mathrm du & = \left[ -\frac{(x-u)^{n+1}}{(n+1)!}f^{(n+1)}(u) \right]  _{a}^{x} - \int_{a}^{x} - \frac{(x-u)^{n+1}}{(n+1)!}f^{(n+2)}(u) \, \mathrm du \\
                                                                        & = \frac{(x-a)^{n+1}}{(n+1)!}f^{(n+1)}(a)+ \int_{a}^{x} \frac{(x-u)^{n+1}}{(n+1)!}f^{(n+2)}(u) \, \mathrm du
          \end{align*}

          donc
          $$
            f(x) = \sum_{k=0}^{n+1} \frac{(x-a)^{k}}{k!}f^{(k)}(a) + \int_{a}^{x} \frac{(x-u)^{n+1}}{(n+1)!}f^{(n+2)}(u) \, \mathrm du
          $$
          donc $\mathcal{H}_{n+1}$ est vérifiée.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Calcul de $\lim_{ n \to \infty } \sum_{k=0}^{n} \frac{(-1)^{k}}{k}x^{k}$}
  Appliquons la formule de Taylor, avec reste intégral pour

  $$
    \left\{ \begin{array}{ll}
      f \leftarrow (x \mapsto \ln(1+x)) \in \mathcal{C^{\infty}} \\
      n \leftarrow n                                             \\
      a\leftarrow 0
    \end{array}\right.
  $$


  \begin{align*}
    \left\lvert  \ln(1+x) - \sum_{k=0}^{n} \frac{f^{(k)}(0)}{k!}(x-0)^{k}   \right\rvert = \left\lvert  \int_{0}^{x} \frac{(x-t)^{n}}{n!}f(t) \, \mathrm dt   \right\rvert
  \end{align*}


  Puisque $f^{(n)}(t) = \frac{(-1)^{n+1}}{(t+1)^{n}}(n-1)!$, on en déduit que $f^{(n)}(0) = (-1)^{n+1}(n-1)!$ et $\frac{f^{(n)}(0)}{n!}= \frac{(-1)^{n+1}}{n}$

  donc

  \begin{align*}
    \left\lvert  \ln(1+x) - \sum_{k=0}^{n} \frac{(-1)^{k+1}}{k}x^{k}  \right\rvert & = \left\lvert  \int_{0}^{x} \frac{(x-t)^{n}}{\cancel{ n! }}\times \frac{(-1)^{n} \times \cancel{ n! }}{(t+1)^{n+1}} \, \mathrm dx   \right\rvert \\
                                                                                   & \leqslant \int_{0}^{x} \left\lvert  \frac{(x-t)^{n}}{(t+1)^{n+1}}  \right\rvert  \, \mathrm dt                                                   \\
                                                                                   & \leqslant \left\| \frac{1}{t+1} \right\|_{\infty} \int_{0}^{x} (x-t) \, \mathrm dt                                                               \\
                                                                                   & \leqslant \frac{x^{n+1}}{n+1} \xrightarrow[x \to \infty]{}0
  \end{align*}

  Ce qui prouve la convergence de la série $\sum_{n\geqslant 0} \frac{(-1)^{n}}{n}x^{n}$, et $\sum_{n=0}^{\infty} \frac{(-1)^{n}}{n}x^{n} = \ln(1+x)$

\end{question_kholle}

\pagebreak\section{Semaine 28}

\begin{question_kholle}{Condition nécessaire de convergence de $\sum_{n \geqslant n_0} u_n$}
  Soit $u \in \K^{[ \! [ n_{0}, + \infty [ \![}$.
  Si la série $\sum_{n\geqslant n_{0}}u_{n}$ converge, alors la suite $u$ converge vers $0$.

  Supposons que la série converge. Notons $(S_{n})_{n\geqslant n_{0}}$ la suite des sommes partielles.
  $$\forall n \in [ \! [ n_{0}+1, +\infty [ \![, u_{n} = S_{n} - S_{n-1}$$
  Puisque $S$ converge, on en déduit que $u$ converge vers $0$.
\end{question_kholle}

\begin{question_kholle}[
    \begin{equation}
      \forall q \in \C, \
      \sum_{n \geqslant 0} q^n \text{ cv.}
      \iff |q| < 1
    \end{equation}
    Dans ce cas $\displaystyle \sum_{n=0}^{+\infty}q^{n}=\frac{1}{1-q}$ et $R_n = \frac{q^{n+1}}{1-q}$.
  ]{Condition nécessaire et suffisante de convergence de $\sum_{n\geqslant 0}q^{n}$ pour $q \in \C$ et calcul de la somme et du reste lorsqu'ils existent.}
  \begin{itemize}[label=$\star$]
    \item Si $\lvert q \rvert<1$
          $$\forall n \in \N, S_{n} = \sum_{k=0}^{n}q^{k} = \frac{1-q^{n+1}}{1-q}$$
          De plus, $\lvert q^{n+1} \rvert=\lvert q \rvert^{n+1}= \left\{ \begin{array}{ll}0 \text{ si }q=0\\ e^{(n+1)\ln \lvert q \rvert} \text{ si }q\neq 0\end{array}\right.$

          \begin{align*}
            \left\{ \begin{array}{ll}
                      0 \\
                      e^{(n+1)\ln \lvert q \rvert }
                    \end{array}\right. \xrightarrow[n\to \infty]{} \left\{ \begin{array}{ll}
                                                                             0 \\
                                                                             0
                                                                           \end{array}\right.
          \end{align*}

          Ainsi, $\sum_{n\geqslant 0}q^{n}$ converge et $\sum_{n=0}^{+\infty}q^{n}=\frac{1}{1-q}$
          $$S_{n} = \frac{1-q^{n+1}}{1-q} \implies R_{n} = S_{n} - \sum_{n=0}^{+\infty}q^{n}= \frac{q^{n+1}}{1-q}$$

    \item Si $\lvert q \rvert=1$

          $$\forall n \in \N, \lvert q \rvert ^{n} = 1^{n}=1 \xrightarrow[n \to \infty]{}1$$

          Ainsi, $\lvert q \rvert^{n}$ ne converge pas vers 0 donc $(q^{n})_{n\geqslant 0}$ ne converge pas vers 0, donc la série est grossièrement divergente.

    \item Si $\lvert q \rvert>1$
          $\lvert q \rvert^{n}=\exp(n \ln \lvert q \rvert) \xrightarrow[n\to +\infty]{} +\infty$
          Donc $(q^{n})_{n\geqslant 0}$ ne converge pas vers $0$. Donc la série est grossièrement divergente.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[{Soit $\alpha \in \R$.
        \begin{equation}
          \text{La série } \sum_{n\geqslant 1} \frac{1}{n^{\alpha}} \text{ converge } \iff \alpha >1
        \end{equation}
      }]{Caractérisation de la convergence des séries de Riemann}
  \begin{itemize}[label=$\lozenge$]
    \item Supposons $\alpha < 0$ alors, $\frac{1}{n^{\alpha}}=n^{\lvert \alpha \rvert} \xrightarrow[n\to +\infty]{}+\infty$ donc la série est grossièrement divergente.
    \item Supposons $\alpha = 0$ alors $\frac{1}{n^{\alpha}}=1 \xrightarrow[n \to \infty]{} 1$ donc la série est grossièrement divergente.
    \item Supposons $\alpha > 0$
          Cherchons un équivalent de
          $$
            \frac{1}{(n+1)^{\beta}}-\frac{1}{n^{\beta}}
          $$
          en fonction de $\beta \in \R^{*}$

          \begin{align*}
            \frac{1}{(n+1)^{\beta}} - \frac{1}{n^{\beta}} & =
            \frac{1}{n^{\beta}}\left( \frac{1}{\left( 1+\frac{1}{n} \right)^{\beta}} - 1 \right)                                                                \\
                                                          & = \frac{1}{n^{\beta}}\left[ \left( 1+\frac{1}{n} \right)^{-\beta} - 1\right]                        \\
                                                          & \underset{ \overset {\infty} {} } {\sim} \frac{1}{n^{\beta}} \times \left( -\frac{\beta}{n} \right) \\
                                                          & \underset{ \overset {\infty} {} } {\sim} - \frac{\beta}{n^{\beta+1}}
          \end{align*}

          \begin{itemize}[label=$\star$]
            \item Pour $\alpha \in ]0, +\infty[ \setminus \left\{ 1 \right\}$
                        Appliquons le calcul ci-dessus pour $\beta \leftarrow \alpha -1$ (autorisé car $\alpha \neq 1\implies \alpha-1 \neq 0$)
                        $$\frac{1}{(n+1)^{\alpha-1}}-\frac{1}{n^{\alpha-1}} \underset{ \overset {\infty} {} } {\sim}
                          -\frac{\alpha-1}{n^{\alpha}}$$
                        De plus, $\left( -\frac{\alpha-1}{n^{\alpha}} \right)_{n\geqslant 1}$ est de signe constant donc, d'après le critère d'équivalence, $\sum_{n\geqslant 1} \frac{-(\alpha -1)}{n^{\alpha}}$ est de même nature que la série télescopique $\sum_{n\geqslant 1}(\frac{1}{(n+1)^{\alpha-1}}-\frac{1}{n^{\alpha-1}} )$. Or, la série télescopique est de même nature que $\left( \frac{1}{n^{\alpha-1}} \right)_{n\geqslant 1}$.

                        Donc par transitivité, puisque $\sum_{n\geqslant 1} \frac{1}{n^{\alpha}}$ est de même nature que $\sum_{n\geqslant 1} \frac{-(\alpha-1)}{n^{\alpha}}$, la série de Riemann est de même nature que $\left( \frac{1}{n^{\alpha-1}} \right)_{n\geqslant 1}$
                        Or, $\left( \frac{1}{n^{\alpha-1}} \right)_{n\geqslant 1}$ converge pour $\alpha>1$ et diverge pour $\alpha \in ]0, 1[$.

            \item Si $\alpha = 1$
                  Appliquons la comparaison série intégrale pour $f \leftarrow (x \mapsto \frac{1}{x}) \left\{ \begin{array}{ll} \in \mathcal{C}^{0}([1, +\infty[, \R)\\ \text{décroissante sur }[1, +\infty[ \end{array}\right.$

                  $$\forall n \in \N^{*}, \int_{1}^{n+1}  \frac{\mathrm{d}u}{u} \leqslant  \sum_{k=1}^{n} \frac{1}{k}$$
                  Ainsi,
                  $$\forall n \in \N^{*}, \underbrace{ \ln(n+1) }_{ \xrightarrow[n \to +\infty]{} +\infty } \leqslant \sum_{k=1}^{n} \frac{1}{k}$$
                  Donc la série diverge.
          \end{itemize}
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[
  Soit $n_0 \in \N$.
  Soit $f:[n_0, +\infty[ \to \R$ une fonction continue et décroissante. Nous avons l'encadrement suivant :
      \begin{equation}
        \int_{n_{0}}^{n+1} f(t) \, \mathrm dt \leqslant
        \sum_{k=n_{0}}^{n} f(k)
        \leqslant f(n_{0})+ \int_{n_{0}}^{n} f(t) \, \mathrm dt
      \end{equation}
    ]{Comparaison série-intégrale}
    Soit $k \in [ \! [ n_{0}, +\infty [\![$ fixé quelconque.

  \begin{align*}
    \forall t \in [k, k+1], \ f(k+1)    & \leqslant f(t) \leqslant f(k)                                                            \\
    \int_{k}^{k+1} f(k+1) \, \mathrm dt & \leqslant \int_{k}^{k+1} f(t) \, \mathrm dt \leqslant \int_{k}^{k+1}  f(k) \, \mathrm dt \\
    f(k+1)                              & \leqslant \int_{k}^{k+1} f(t) \, \mathrm dt \leqslant f(k)
  \end{align*}

  Ainsi

  \begin{align*}
    \sum_{k=n_{0}}^{n}\int_{k}^{k+1}f(t)  \, \mathrm dt & \leqslant \sum_{k=n_{0}}^{n}f(t) \\
    \int_{n_{0}}^{n+1} f(t) \, \mathrm dt               & \leqslant \sum_{k=n_{0}}^{n}f(k)
  \end{align*}


  De même,

  \begin{align*}
    \forall k \in [ \! [ n_{0}+1 , \ +\infty[ \![, f(k) & \leqslant\int_{k-1}^{k} f(t) \, \mathrm dt                      \\
    \sum_{k=n_{0}+1}^{n}f(k)                            & \leqslant \sum_{k=n_{0}+1}^{n}\int_{k-1}^{k} f(t) \, \mathrm dt \\
    \sum_{k=n_{0}}^{n} f(k)                             & \leqslant f(n_{0})+ \int_{n_{0}}^{n} f(t) \, \mathrm dt
  \end{align*}

  D'où l'encadrement.
\end{question_kholle}

\begin{question_kholle}[{Soit $n_{0} \in \N$ et $f:[n_{0}, +\infty[ \to \R$ une fonction continue, décroissante et minorée par $m \in \R$.
  Alors la série de terme général
  $$\left( f(n)- \int_{n}^{n+1} f(u) \, \mathrm du  \right)_{n\geqslant n_{0}}$$
  est à termes positifs ou nuls et converge.
  }]{Pour $f$ continue sur $[n_0, +\infty[$, décroissante et minorée, $\displaystyle\sum_{n\geqslant n_{0}}\left( f(n)- \int_{n}^{n+1} f(u) \, \mathrm du  \right)$ converge. Application au DA en $o(1)$ de la somme partielle de la série harmonique}

  Montrons que la suite $(S_{n})_{n\geqslant n_{0}}$ est majorée, et que la suite est à termes $\geqslant 0$
  
  La décroissance de $f$ donne l'encadrement suivant

  $$\forall n \in [ \! [ n_{0} , +\infty[\![,  f(n) - \int_{n}^{n+1} f(t) \, \mathrm dt \geqslant 0$$

  La comparaison série intégrale s'applique donc à $f$ qui est décroissante et continue et donne

  \begin{align*}
    \forall n \in [ \! [ n_{0}, +\infty [ \![ f(n+1)\leqslant \int_{n}^{n+1} f(t) \, \mathrm dt \leqslant f(n) & \implies -f(n+1) \geqslant -\int_{n}^{n+1} f(t) \, \mathrm dt                          \\
                                                                                                               & \implies f(n) - f(n+1) \geqslant f(n) - \int_{ n}^{n+1} f(t) \, \mathrm dt \geqslant 0
  \end{align*}

  En sommant sur $k \in [ \! [ n_{0}, n ] \!]$

  $$\sum_{k=n_{0}}^{n} (f(k) - f(k+1)) \geqslant  \sum_{k=n_{0}}^{n}\left( f(k) - \int_{k}^{k+1} f(t) \, \mathrm dt  \right) = S_{n}$$

  En reconnaissant un phénomène télescopique

  $$S_{n} \leqslant f(n_{0})-f(n+1)\leqslant f(n_{0})- m $$

  Donc $(S_{n})_{n\geqslant n_{0}}$ est croissante et majorée, elle converge.
  \\

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}

      \draw[->] (-0.5,0) -- (8.5,0) node[right] {$x$}; % x-axis
      \draw[->] (0,-0.5) -- (0,5.5) node[above] {$f(x) = \frac{1}{x} + 1$}; % y-axis
      
      \draw[domain=1:8,samples=100,smooth,thick,blue] plot (\x, {1/\x*4 + 1}) node[right] {};
      
      \foreach \n in {1,2,...,7} {
          
          \draw[dashed] (\n, 0) -- (\n, {4/\n + 1}); 
          \filldraw (\n, {4/\n + 1}) circle (2pt) node[above right] {$f(\n)$};
          
          
          \draw[red, thick] (\n, {4/\n + 1}) -- (\n+1, {4/\n + 1}) -- (\n+1, {1 + 4/(\n + 1)});
          
          \fill[gray, opacity=0.3] 
              plot[domain=\n:\n+1, samples=50] (\x, {1 + 4/\x}) -- 
              (\n+1, {1 + 4/\n}) -- (\n, {1 + 4/\n}) -- cycle;
          
          \fill[gray, opacity=0.3] 
              plot[domain=\n:\n+1, samples=50] (\x - \n + 9, {1+ 4/\x}) -- 
              (10, {1 + 4/\n}) -- (9, {1+ 4/\n}) -- cycle;
      
          \draw[blue] plot[domain = \n:\n+1, samples = 50] (\x - \n + 9, {1 + 4/\x});
          \draw[red] (9, {1 + 4/\n}) -- (10, {1 + 4/\n}) -- (10, {1 + 4/(\n+1)});
      }
      
      \draw[dashed] (9, 5) -- (9, 1) -- (10, 1) -- (10, 2);
      \draw[dotted] (0, 1) node[left] {$m = 1$} -- (9, 1);
      
      \draw[<->] (10.2, 1) -- (10.2, 5) node[pos=0.5, right] {$f(1) - 1$};
      
      \end{tikzpicture}
      \caption{Une visualisation graphique de la bornitude de $(S_n)_{n \geqslant n_0}$ pour le cas particulier de $f \leftarrow (x \mapsto \nicefrac 1 x + 1), m \leftarrow 1, n_0 \leftarrow 1$. On interprète chaque terme de la somme comme l'aire du rectangle à laquelle on retranche l'aire sous la courbe : cela correspond à l'aire grise, qui peut être aisément installée dans un rectangle d'aire $f(1) - 1 = f(n_0) - m$.}
  \end{figure}

  \textit{Application au DA en $o(1)$ de la somme partielle de la série harmonique.}
  Appliquons ce qui précède pour $f = x \mapsto \nicefrac{1}{x}$ et $n_0 = 1$. $f$ est bien continue, décroissante et minorée (par 0). sur $[1; + \infty[$.
  Donc $\displaystyle \sum_{ n \geqslant 1} \left( \frac{1}{n} - \int_{n}^{n+1} \frac{\mathrm du}{u} \right)$ converge.

  Notons $\gamma$ sa somme. Ainsi $\displaystyle \sum_{k=1}^{n} \left( \frac{1}{k} - \int_{k}^{k+1} \frac{\mathrm du}{u} \right) \underset{n \to +\infty}{=} \gamma + o(1)$.

  Remarquons que, pour tout $n \in \N$,
  $\displaystyle \sum_{k=1}^{n} \left( \frac{1}{k} - \int_{k}^{k+1} \frac{\mathrm du}{u} \right)
    = H_n - \sum_{k=1}^{n} \left( \ln(k+1) - \ln(k) \right)
    = H_n - \ln(n+1) + \ln 1
    = H_n - \ln(n+1)$.

  Donc $H_n \underset{n \to +\infty}{=} \ln(n+1) + \gamma + o(1) = \ln n + \ln \left( 1 + \nicefrac{1}{n} \right) + \gamma + o(1)$.
  \begin{equation*}
    H_n \underset{n \to +\infty}{=} \ln n + \gamma + o(1)
  \end{equation*}
  La constante $\gamma$ est appelé la constante d'Euler-Mascheroni et vaut environ $0,5772156649$.
\end{question_kholle}

\begin{question_kholle}[{Soit $(a_{n})_{n\geqslant n_{0}} \in \R^{[ \! [ n_{0}, +\infty [ \![}$ une suite réelle.
  Si
  $$
    \left\{ \begin{array}{ll}
      \forall n \in [ \! [ n_{0}, +\infty [\![, a_{n} \geqslant 0 \\
      (a_{n})_{n\geqslant n_{0}} \text{ est décroissante}         \\
      \lim_{ n \to \infty } a_{n}=0
    \end{array}\right.
  $$
  alors $\sum_{n\geqslant n_{0}}(-1)^{n}a_{n}$}]{Théorème des séries alternées}

  \begin{itemize}[label=$\lozenge$]
    \item Traitons le cas $n_{0}\equiv 0 [2]$ il existe $p_{0} \in \N: n_{0}=2p_{0}$
          \begin{itemize}[label=$\star$]
            \item  Les suites $(S_{2p})_{p\geqslant p_{0}}$ et $(S_{2p+1})_{p\geqslant p_{0}}$ sont adjacentes:

                  \begin{align*}
                    \forall p \in [ \! [ p_{0}, +\infty[ \![, S_{2(p+1)} - S_{2p} & = S_{2p+2}- S_{2p}                                                        \\
                                                                                  & = \sum_{k=2p_{0}}^{2p+2}(-1)^{k}a_{k} + \sum_{k=2p_{0}}^{2p}(-1)^{k}a_{k} \\
                                                                                  & = -a_{2p+1}+a_{2p+2} \leqslant 0 \text{ car }a\downarrow
                  \end{align*}

                  \begin{align*}
                    S_{2(p+1)+1} - S_{2p+1} & = S_{2p+3} - S_{2p+1} = (-1)^{2p+2}a_{2p+2}+(-1)^{2p+3}a_{2p+3} = a_{2p+2}- a_{2p+3} \geqslant 0 \text{ car }a\downarrow
                  \end{align*}


                  Donc $(S_{2p})$ est décroissante et $(S_{2p+1})$ est croissante.
                  De plus
                  $$
                    S_{2p+1} - S_{2p} = (-1)^{2p+2}a_{2p+1} = \underbrace{ -a_{2p+1} }_{ \xrightarrow[p \to \infty]{} 0 } \leqslant 0 \text{ car a positive}
                  $$
                  Ainsi $(S_{2p})_{p\geqslant p_{0}}$ et $(S_{2p+1})_{p\geqslant p_{0}}$ sont adjacentes.
            \item Donc d'après le théorème des suites adjacentes, $(S_{2p})$ et $(S_{2p+1})$ convergent vers une même limite $\ell$, si bien que $(S_{n})$ converge vers $\ell$.

            \item De plus, les suites $(S_{2p})_{p\geqslant p_{0}}$ et $(S_{2p+1})_{p\geqslant p_{0}}$ étant adjacentes, pour $n \geqslant n_{0}$ posons $R_{n} = \ell -S_{n}$
                  \begin{itemize}
                    \item Si $n \equiv 0 [2]$, $\exists p \in [ \! [ p_{0}, +\infty [ \![:n=2p$ donc, puisque $(S_{2p})$ est décroissante et $(S_{2p+1})$ est croissante, on a
                          $$
                            S_{2p+1}\leqslant \ell \leqslant S_{2p} \implies S_{2p+1} - S_{2p} \leqslant \ell - S_{2p} \leqslant 0 \implies \lvert R_{2p} \rvert = \lvert \ell - S_{2p} \rvert \leqslant a_{2p+1}
                          $$

                    \item Si $n \equiv 1 [2]$ $\exists p \in [ \! [ p_{0}, +\infty [ \![:n=2p+1$
                          $$
                            S_{2p+1} \leqslant \ell \leqslant S_{2p+2} \implies 0 \leqslant \ell - S_{2p+1} \leqslant S_{2p+2} - S_{2p+1} = (-1)^{2p+2}a_{2p+2}= a_{2p+2}
                          $$
                          donc $\lvert R_{2p+1} \rvert = \lvert \ell-S_{2p+1} \rvert \leqslant a_{2p+2}$
                  \end{itemize}
          \end{itemize}
          Bonus, par croissance de $(S_{2p+1})$ qui converge vers $\ell$, $S_{2p+1} \leqslant \ell$  donc $a_{2p_{0}}-a_{2p_{0} +1}\leqslant\ell$
          Donc $\ell \geqslant 0$ qui est bien le signe du premier terme de la série $(-1)^{n_{0}}a_{n_{0}}$ car $n_{0}\equiv 0[2]$.

    \item Le cas $n_0 \equiv 1 [2]$ se traite de la même manière
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[{Soit $u \in \K^{[ \! [ n_{0}, +\infty [ \![}$
  Si la série $\sum_{n\geqslant n_{0}}u_{n}$ est absolument convergente, alors la série $\sum_{n\geqslant n_{0}}u_{n}$ est convergente.}]{L'absolue convergence implique la convergence}



  \begin{itemize}[label=$\lozenge$]
    \item Supposons que $u$ est le terme général réel d'une série absolument convergente.
          Posons, pour tout $n \in [ \! [ n_{0}, +\infty [ \![$, $u_{n}^{+}= \max(u_{n}, 0)$ et $u_{n}^{-}=- \min (u_{n}, 0)$
          Avec ces notations, $u_{n}^{+}- u_{n}^{-} = u_{n}$ et $u_{n}^{+}+u_{n}^{-} = \lvert u_{n} \rvert$.
          $$
            \forall n \in [ \! [ n_{0}, +\infty [ \![, u_{n}^{+}\geqslant 0 \text{ et } u_{n}^{-}\geqslant 0
          $$

          $$
            \left. \begin{array}{ll}
              \forall n \geqslant n_{0}, 0\leqslant u_{n}^{+} \leqslant \lvert u_{n} \rvert = u_{n}^{+}+ u_{n}^{-}           \\
              \sum_{n\geqslant n_{0}} u_{n} \text{ est ACV} \implies \sum_{n\geqslant n_{0}} \lvert u_{n} \rvert  \text{ CV} \\
              \forall n \geqslant n_{0}, u_{n}^{+}\geqslant 0 \text{ et } \lvert u_{n} \rvert \geqslant 0
            \end{array}\right\}\implies \sum_{n\geqslant n_{0}}u_{n} ^{+}\text{ converge}
          $$

          On montre de même que $\sum_{n\geqslant n_{0}}u_{n}^{-}$ converge, donc, par structure vectorielle de l'ensemble des termes généraux de suites convergentes, $\sum_{ n\geqslant n_{0}}(u_{n}^{+} - u_{n}^{-})= \sum_{n\geqslant n_{0}}u_{n}$ converge
          \begin{figure}[H]
            \centering
            \begin{tikzpicture}
              \def\numbers{{2.5, -1.8, 3.2, -2.5, -1.5, 3, 2.8, -1.2, 3.5, -2}}
          
          
              \foreach \y in {-3,-2,-1,0,1,2,3}
              \draw[gray!30] (-0.5,\y) -- (10.5,\y);
              \foreach \x in {1,2,3,4,5,6,7,8,9,10}
              \draw[gray!30] (\x,-4) -- (\x,4);
          
              \foreach \i in {0,1,2,3,4,5,6,7,8,9} {
                  \pgfmathtruncatemacro{\number}{\numbers[\i]}
          
                  \fill[black] (\i, \number) circle (2pt);
          
                  \ifnum\number>0
                      \draw[blue] (\i, \number) circle (4pt) node[above right] {$u_\i^+$};
                      \draw[red] (\i, 0) circle (4pt) node[above right] {$u_\i^-$};
                  \else        
                      \draw[red] (\i, -\number) circle (4pt) node[above right] {$u_\i^-$};
                      \draw[blue] (\i, 0) circle (4pt) node[above right] {$u_\i^+$};
                  \fi
              }
              
          
              \draw[->] (-1, 0) -- (10, 0) node[right] {$n$};
              \draw[->] (0, -4) -- (0, 4) node[above] {$u_n, {\color{blue} u_n^+}, {\color{red} u_n^-}$};
              
            \end{tikzpicture}
            \caption{Décomposition de la suite $u$ en $u^+$ et $u^-$, les "partie positive" et "partie négative". Graphiquement, on retrouve $u_n^+ + u_n^- = |u_n|$ et $u_n^+ - u_n^- = u_n$}
          \end{figure}
    \item Cas d'une série complexe,

          Posons, $\forall n\geqslant n_{0}, x_{n} = \mathrm{Re}(u_{n})$ et $y_{n} = \mathrm{Im}(u_{n})$
          Alors,
          $$
            \left. \begin{array}{ll}
              \forall n \geqslant n_{0}, \lvert x_{n} \rvert \leqslant \lvert \mathrm{Re}(u_{n}) \rvert \leqslant \lvert u_{n} \rvert \\
              \forall n\geqslant n_{0}, \lvert  x_{n} \rvert \geqslant 0 \text{ et } \lvert u_{n} \rvert \geqslant 0                  \\
              \sum_{n\geqslant n_{0}}u_{n} \text{ ACV } \implies \sum_{n\geqslant n_{0}}\lvert u_{n} \rvert \text{ CV }
            \end{array}\right\}\implies \sum_{n\geqslant n_{0}}\lvert x_{n} \rvert \text{ converge}
          $$
          Donc d'après le cas réel, $\sum_{n\geqslant n_{0}} x_{n}$ converge
          On montre de même que $\sum_{n\geqslant n_{0}}y_{n}$ converge
          Donc, par structure vectorielle, $\sum_{n\geqslant n_{0}}(x_{n}+ iy_{n})$ converge.
          Donc $u_{n}$ est le terme général d'une série convergente.
        \end{itemize}
        
\end{question_kholle}

\begin{question_kholle}
  {Décomposition d'une permutation en produit de cycles à supports disjoints puis en produit de transposition et calcul de son ordre}
  Prenons pour illustrer la décomposition
  \begin{equation*}
    \sigma = \begin{pmatrix}
      1 & \mapsto & 6 \\
      2 & \mapsto & 4 \\
      3 & \mapsto & 3 \\
      4 & \mapsto & 2 \\
      5 & \mapsto & 1 \\
      6 & \mapsto & 5 \\
    \end{pmatrix}
    \in \mathcal{S}_6
  \end{equation*}

  Il faut réaliser un "graphe des images". Chaque sommet est un nombre de $\lient 1; 6 \rient$ et pointe vers son image.

  \begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance={15mm},
        thick,
        edge/.style = {draw, circle},
        arrow/.style = {thick,
            arrows={
                - Stealth[length=8pt,open,bend,width=3mm]
              }
          }
      ]

      \node (1) {1};
      % 1/sqrt(3) = 0.58
      \node (5) [below right=1cm and 0.58cm of 1] {5};
      \node (6) [below left=1cm and 0.58cm of 1] {6};
      \node (2) [right=2cm of 1] {2};
      \node (4) [below=1cm of 2] {4};
      \node (3) [right=1cm of 2] {3};

      \draw[arrow] (1) edge [bend right=30] (6);
      \draw[arrow] (6) edge [bend right=30] (5);
      \draw[arrow] (5) edge [bend right=30] (1);

      \draw[arrow] (2) edge [bend right=30] (4);
      \draw[arrow] (4) edge [bend right=30] (2);

      %			\draw[arrow] (3) edge [bend right=45] (3);
      \draw [arrow] (3.200) arc (90:400:3mm) (3);
    \end{tikzpicture}
  \end{figure}

  Nous pouvons voir que $\sigma = (1, 6, 5) \circ (2, 4)$.
  De plus, $(1, 6, 5) = (1, 6) \circ (6, 5) \circ (5, 1)$.
  Donc $\sigma = (1, 6) \circ (6, 5) \circ (5, 1) \circ (2, 4)$.

  L'ordre d'une permutation est définit par $p(\sigma) = \min \{ n \in \N^* \;|\; \sigma^n = Id \}$. $p(\sigma)$ est aussi le PPCM des ordres des permutations de sa décomposition en produit de cycles à supports disjoints. Ici, $p(\sigma) = 2 \vee 3 = 6$.
\end{question_kholle}
\pagebreak\section{Semaine 29}

\begin{question_kholle}[{Le noyau d'un morphisme de groupe étant toujours un sous-groupe du groupe de départ, le groupe alterné d'indice $n \in \N^{*}$ est le sous groupe de $(\mathcal{S}_{n}, \circ)$ obtenu en considérant le noyau du morphisme signature.
        $$
          \mathcal{A}_{n}= \ker \varepsilon
        $$
        $\mathcal{A}_{n}$ est de cardinal $\frac{n!}{2}$.
      }]{Définition et cardinal du sous-groupe alternée $\mathcal{A}_n$}


  Fixons $\tau = (1, 2)$
  Considérons
  $$\Phi \left|\begin{array}{ll} \mathcal{A}_{n} &\to \mathcal{S}_{n} \setminus \mathcal{A}_{n} \\ \sigma &\mapsto \sigma \circ \tau \end{array}\right.$$
  \begin{itemize}
    \item

          $\Phi$ est bien définie: soit $\sigma \in \mathcal{A}_{n}$ fixée quelconque. Par propriété de morphisme de la signature, $\varepsilon(\sigma \circ \tau) = \varepsilon(\sigma) \times \varepsilon(\tau) = 1 \times (-1) = -1$ donc $\sigma \circ \tau \not\in \mathcal{A}_{n}$ donc $\Phi(\sigma) \in \mathcal{S}_{n}\setminus \mathcal{A}_{n}$

    \item De plus, $\Phi$ est bijective en considérant
          $$\Psi\left|\begin{array}{ll} \mathcal{S}_{n}\setminus \mathcal{A}_{n} &\to \mathcal{A}_{n} \\ \sigma &\mapsto \sigma \circ  \tau \end{array}\right.$$
          $\Psi \circ \Phi = \mathrm{Id}_{\mathcal{A}_{n}}$ et $\Phi \circ \Psi = \mathrm{Id}_{\mathcal{S}_{n}\setminus \mathcal{A}_{n}}$
  \end{itemize}
  Ainsi,
  $$
    \lvert \mathcal{A}_{n} \rvert  = \lvert \mathcal{S}_{n}\setminus \mathcal{A}_{n} \rvert  = \lvert \mathcal{S}_{n} \rvert - \lvert \mathcal{A}_{n} \rvert
  $$
  D'où $\lvert \mathcal{A}_{n} \rvert = \frac{\lvert \mathcal{S}_{n} \rvert}{2} = \frac{n!}{2}$
\end{question_kholle}

\begin{question_kholle}{Caractérisation des bases par le déterminant}
  ~\\
  \begin{itemize}[label=$\star$]
    \item Supposons que la famille $\mathcal{B'} =(u_{1}, \dots, u_{n}) \in E^{n}$ est une base de $E$.
          $$\det_{\mathcal{B}}\mathcal{B'}\times \det_{\mathcal{B'}}\mathcal{B} = 1 \implies \det _{\mathcal{B}}\mathcal{B'} \neq 0$$

    \item Supposons qu'il existe une base $\mathcal{B}$ telle que $\det_{\mathcal{B}} \mathcal{B}' \neq 0$
          Si $\mathcal{B}'$ était liée, le déterminant serait nul, donc en contraposant, $\mathcal{B}'$ n'est pas liée, et est de cardinal $n$, c'est une base.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}[{
  Soit $E$ un $\K$-espace vectoriel de dimension finie et $F \in \mathcal{L}_{\K}(E)$
  $$\exists!\lambda \in \K : \forall \mathcal{B} \text{ base de }E, \forall (u_{1}, \dots, u_{n})\in E^{n}, \
  \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n}))=\lambda \times \det_{\mathcal{B}}(u_{1}, \dots, u_{n})$$
  On appelle ce $\lambda$ \underline{le} déterminant de l'endomorphisme $f$.
  }]{Définition du déterminant d'un endomorphisme}

  \begin{itemize}[label=$\lozenge$]
    \item \underline{Existence}

          Soit $\mathcal{B}_{0}= (e_{1}, \dots, e_{n})$ une base de $E$ fixée.
          L'application
          $$
            \varphi \left|\begin{array}{ll} E^{n} &\to \K \\ (u_{1}, \dots, u_{n}) &\mapsto \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) \end{array}\right.
          $$
          est
          \begin{itemize}
            \item Une forme n-linéaire :  soient $(u_{1}, \dots, u_{n}) \in E^{n}$ fixés quelconques $(u, v, \lambda) \in E^{2} \times \K$

                  \begin{align*}
                    \varphi (v+\lambda.w, u_{2}, \dots, u_{n}) & = \det_{\mathcal{B}_{0}}(f(v+\lambda.w), f(u_{2}), \dots, f(u_{n}))                                                                \\
                                                               & = \det_{\mathcal{B}_{0}}(f(v)+ \lambda.f(w), f(u_{2}), \dots, f(u_{n})) \text{ par linéarité de }f                                 \\
                                                               & = \det_{\mathcal{B}_{0}}(f(v), f(u_{2}), \dots, f(u_{n})) + \lambda \times \det_{\mathcal{B}_{0}}(f(w), f(u_{2}), \dots, f(u_{n})) \\
                                                               & \text{ par linéarité de } \det_{\mathcal{B}_{0}}                                                                                   \\
                                                               & = \varphi(v, u_{2}, \dots, u_{n})+ \lambda \times \varphi(w, u_{2}, \dots, u_{n})
                  \end{align*}

                  Par conséquent, $\varphi$ est linéaire en son premier argument.
                  On prouve de même que $\varphi$ est linéaire en ses $n-1$ autres arguments, ce qui montre sa $n$-linéarité.

            \item Alternée
                  Soient $(u_{1}, \dots, u_{n}) \in E^{n}$ tels qu'il existe $(i, j) \in [ \! [ 1, n ] \!]^{2}$ tels que $i \neq j$ et $u_{i} = u_{j}$ alors on a aussi $f(u_{i}) = f(u_{j})$, si bien que le caractère alterné de $\det_{\mathcal{B}_{0}}$
                  $$
                    \varphi(u_{1}, \dots, u_{n})=\det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) = 0
                  $$
                  Donc $\varphi \in \land_{\K}^{n} = \text{Vect}\{ \det_{\mathcal{B}_{0}} \}$
          \end{itemize}
          Donc
          $$
            \exists \lambda_{\mathcal{B}_{0}}\in \K: \varphi = \lambda_{\mathcal{B}_{0}}.\det_{\mathcal{B_{0}}}
          $$
          d'où,
          $$
            \forall (u_{1}, \dots, u_{n}) \in E^{n}, \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n}))= \lambda_{\mathcal{B}_{0}}\times \det_{\mathcal{B_{0}}}(u_{1}, \dots, u_{n})
          $$
          Soit $\mathcal{B}$ une base de $E$ fixée quelconque. Nous savons que
          $$
            \det_{\mathcal{B}} = \det_{\mathcal{B}} \mathcal{B}_{0} . \det_{\mathcal{B}_{0}}
          $$
          Donc en multipliant la relation précédente par $\det_{\mathcal{B}}\mathcal{B}_{0}$,
          $$
            \forall (u_{1}, \dots, u_{n}) \in E^{n}, \underbrace{ \det_{\mathcal{B}}\mathcal{B}_{0} \times \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) }_{ \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n})) }= \lambda_{\mathcal{B}_{0}} \times \underbrace{ \det_{\mathcal{B}}\mathcal{B}_{0} \times \det_{\mathcal{B_{0}}}(u_{1}, \dots, u_{n}) }_{ \det_{\mathcal{B}}(u_{1}, \dots, u_{n}) }
          $$
          Par conséquent, $\lambda_{\mathcal{B}_{0}}$ convient pour toute base $\mathcal{B}$.

    \item \underline{Unicité}
          Soit $\lambda \in \K$ tel que
          $$
            \forall \mathcal{B} \text{ base de }E, \forall (u_{1}, \dots, u_{n})\in E^{n}, \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n}))=\lambda \times \det_{\mathcal{B}}(u_{1}, \dots, u_{n})
          $$
          Particularisons pour $\mathcal{B}\leftarrow \mathcal{B}_{0}$ et $(u_{1}, \dots, u_{n})\leftarrow \mathcal{B}_{0}$
          $$
            \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n})) = \lambda \times \det_{\mathcal{B}_{0}}\mathcal{B}_{0} = \lambda \times 1
          $$
          Donc $\lambda = \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n}))$
          Or, en particularisant la relation définissant $\lambda_{\mathcal{B}_{0}}$ pour $(u_{1}, \dots, u_{n}) \leftarrow \mathcal{B}_{0}$
          $$
            \lambda_{\mathcal{B}_{0}} = \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n}))
          $$
          donc $\lambda = \lambda_{\mathcal{B}_{0}}$
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}
  [{\begin{propositions}
          \item $\forall (f, g) \in \mathcal{L}_{\K}(E)^{2}, \ \det (f \circ g) = \det f \times \det g$

          \item $\forall f \in \mathcal{L}_{K}(E), \ f \in \mathcal{GL}_{\K}(E) \iff \det f \neq 0$
        \end{propositions}
      }]{Le déterminant est un morphisme de $(\mathcal{L}_{\K}(E), \circ)$ dans $(\K, \times)$, application à la caractérisation des automorphismes}
  Fixons $\mathcal{B}=(e_{1}, \dots, e_{n})$ une base de $E$
  \begin{enumerate}
    \item Soient $(f, g) \in \mathcal{L}_{\K}(E)^{2}$ fixés quelconques.

          \begin{align*}
            \det (f \circ  g) & = \det_{\mathcal{B}}((f \circ g)(e_{1}), \dots, (f \circ  g)(e_{n}))                                                   \\
                              & = \det_{\mathcal{B}}(f(g(e_{1})), \dots, f(g(e_{n})))                                                                  \\
                              & = \det f \times \det_{\mathcal{B}}(g(e_{1}), \dots, g(e_{n})) \text{ par définition du déterminant d'un endomorphisme} \\
                              & = \det f \times \det g \times \det_{\mathcal{B}}(e_{1}, \dots, e_{n})                                                  \\
                              & = \det f \times \det g
          \end{align*}



    \item Soit $f \in \mathcal{L}_{\K}(E)$
          \begin{itemize}
            \item Supposons $f \in \mathcal{GL}_{\K}(E)$
                  Appliquons la relation de morphisme pour $g \leftarrow f^{-1}$
                  $$
                    \underbrace{ \det(f \circ  f^{-1}) }_{ = \det \mathrm{Id}_{E} } = \det f \times \det f^{-1}
                  $$
                  Or, $\det \mathrm{Id}_{E}= \det_{\mathcal{B}}(e_{1}, \dots, e_{n}) = 1$ si bien que $\det f \times \det f^{-1} = 1$ on en déduit que $\det f \neq 0$ et d'autre part que $\det (f^{-1}) = \frac{1}{\det f}$
            \item Supposons que $\det f \neq 0$
                  Par définition du déterminant d'un endomorphisme
                  $$
                    \det_{\mathcal{B}}(f(e_{1}), \dots, f(e_{n})) = \det f \times \det_{\mathcal{B}}(e_{1}, \dots, e_{n}) = \det f
                  $$
                  Donc $\det_{\mathcal{B}}(f(e_{1}), \dots, f(e_{n})) \neq 0$ si bien que $(f(e_{1}), \dots, f(e_{n}))$ est une base de $E$, donc $f$ envoie une base sur une base : c'est un automorphisme.
          \end{itemize}
  \end{enumerate}
\end{question_kholle}
\begin{question_kholle}[{Soit $A \in \mathcal{M}_{n}(\K)$.
        Alors
        \begin{equation}
          A \times (\mathrm{com} A)^{T}
          = (\mathrm{com}A)^{T}\times A
          = \det A \times I_{n}
        \end{equation}
      }]{Produit d'une matrice carrée par la transposée de sa comatrice.}

  \begin{itemize}[label=$\lozenge$]
    \item Montrons que $A \times (\mathrm{com}A)^{T}=\det A \times I_{n}$
          Soient $(i, j) \in [ \! [ 1, n ] \!]$ fixés quelconques

          \begin{align*}
            [A \times (\mathrm{com}A)^{T}]_{i,j} & = \sum_{k=1}^{n}A_{i,k}[(\mathrm{com}A)^{T}]_{k,j}    \\
                                                 & =\sum_{k=1}^{n}A_{i,k}(\mathrm{com}A)_{j,k}           \\
                                                 & = \sum _{k=1}^{n}A_{i,k}\times (-1)^{k+j}\Delta_{j,k}
          \end{align*}

          \begin{itemize}[label=$\star$]
            \item Supposons que $i = j$ nous obtenons
                  $$
                    [A\times(\mathrm{com}A)^{T}]_{i,i} = \sum_{k=1}^{n}A_{i,k}\times(-1)^{k+i}\Delta_{i,k} = \det A
                  $$
                  D'après la formule du développement du déterminant de $A$ selon la $i$-ième ligne.

            \item Supposons que $i \neq j$
                  La formule peut être interprétée comme le développement selon la $i$-ième ligne du déterminant de la matrice obtenue à partir de $A$ en remplaçant sa $j$-ième ligne par sa $i$-ième ligne:

                  \begin{align*}
                    \left[ A\times (\mathrm{com}A)^{T} \right] _{i,j} & = \sum_{k=1}^{n}A_{i,k}\times(-1)^{k+j}\Delta_{j,k} \\
                                                                      & =\left| \begin{array}{c}
                                                                                  L_{1}          \\
                                                                                  \hline
                                                                                  \vdots         \\
                                                                                  \hline
                                                                                  L_{i-1}        \\
                                                                                  \hline
                                                                                  L_{i}          \\
                                                                                  \hline L_{i+1} \\
                                                                                  \hline \vdots  \\
                                                                                  \hline L_{j-1} \\
                                                                                  \hline L_{i}   \\
                                                                                  \hline L_{i+1} \\
                                                                                  \hline \vdots  \\
                                                                                  \hline L_{n}
                                                                                \end{array} \right|                         \\
                                                                      & =0
                  \end{align*}

                  Car les lignes d'indice $i$ et $j$ sont identiques.
                  Ainsi, pour tout $(i, j) \in [ \! [ 1, n ] \!]^{2}, [A\times(\mathrm{com}A)^{T}]_{i,j}=\delta_{i,j}\times \det A$
                  Donc
                  $$
                    [A\times(\mathrm{com}A)^{T}]_{i,j}=\det A\times I_{n}
                  $$
          \end{itemize}
    \item On montre de même le produit dans l'autre sens.
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{  Le système linéaire $AX=B$ d'inconnue $X \in \mathcal{M}_{n,1}(\K)$ et de paramètre $B \in \mathcal{M}_{n,1}(\K)$ est dit "de Cramer" s'il admet une unique solution, à savoir si $A$ est une matrice inversible. Dans ce cas, la solution peut être exprimée explicitement par la formule $A^{-1}B$ qui donne la formule dite de Cramer:
        \begin{equation}
          \left( \frac{
            \bigg|
            B\mid C_{2}\mid\dots \mid C_{n}
            \bigg|
          }{\det A}, \dots, \frac{
            \bigg|
            C_{1} \mid
            \dots \mid
            C_{i-1}\mid
            B \mid
            C_{i+1}\mid
            \dots \mid
            C_{n}
            \bigg|
          }{\det A}, \dots, \frac{
            \bigg|
            C_{1}\mid C_{2}\mid\dots \mid B
            \bigg|
          }{\det A} \right)
        \end{equation}
        où $(C_{1}, \dots, C_{n}) \in \mathcal{M}_{n,1}(\K)^{n}$ sont les colonnes de $A$.
      }]{Formule de Cramer}

  Partons de l'expression de l'inverse avec la comatrice:
  $$X = A^{-1}B= \frac{1}{\det A}(\mathrm{com}A)^{T} B$$
  Soit $i \in [ \! [ 1, n ] \!]$.

  \begin{align*}
    X_{i, 1} & = \frac{1}{\det A}[(\mathrm{com}A)^{T}B]_{i,j}                                                        \\
             & = \frac{1}{\det A}\sum_{k=1}^{n}[(\mathrm{com}A)^{T}]_{i,k}B_{k,1}                                    \\
             & = \frac{1}{\det A}\sum_{k=1}^{n}(\mathrm{com}A)_{k,i}B_{k, 1}                                         \\
             & = \frac{1}{\det A}\sum_{k=1}^{n}(-1)^{k+i}\Delta_{k,i}B_{k, 1}                                        \\
             & \text{qui s'interprète comme le développement selon la $i$-ième colonne de la matrice}                \\
             & = \frac{1}{\det A}\Bigg|C_{1}\Bigg|\dots\Bigg|C_{i-1}\Bigg|B\Bigg|C_{i+1}\Bigg|\dots\Bigg|C_{n}\Bigg|
  \end{align*}
\end{question_kholle}

\begin{question_kholle}[
    \begin{equation}
      \forall n \in \N, \forall a \in \K^\N, \
      V(a_0, a_1, \dots, a_n)
      = \left| \begin{matrix}
        1         & 1         & 1         & \dots  & 1         \\
        a_{0}     & a_{1}     & a_{2}     & \dots  & a_{n}     \\
        a_{0}^{2} & a_{1}^{2} & a_{2}^{2} & \dots  & a_{n}^{2} \\
        \vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
        a_{0}^{n} & a_{1}^{n} & a_{2}^{n} & \dots  & a_{n}^{n}
      \end{matrix} \right|
      = \prod_{0 \leqslant i < j \leqslant n}(a_{j} - a_{i})
    \end{equation}
  ]{Calcul du déterminant de Vandermonde}
  Posons
  $$
    \mathcal{P}(n) : \forall (a_{0},\dots,a_{n}) \in \K^{n+1}, V(a_{0}, \dots, a_{n}) = \prod_{0 \leqslant i < j \leqslant n}(a_{j} - a_{i})
  $$
  \begin{itemize}[label=$\lozenge$]
    \item \underline{Initialisation} $n \leftarrow 2$
          Soient $(a_{0}, a_{1}) \in \K^{2}$
          $$
            \left| \begin{matrix}
              1     & 1     \\
              a_{0} & a_{1}
            \end{matrix}\right| = a_{1} - a_{0}
          $$

    \item \underline{Hérédité}, soit $n \in \N^{*}$ fixé quelconque tel que $\mathcal{P}(n)$ est vraie.
          Soient $(a_{0}, a_{1}, \dots, a_{n+1}) \in \K^{n+2}$ fixés quelconques.
          \begin{itemize}
            \item Supposons que les éléments de $\left\{ a_{0}, \dots, a_{n+1} \right\}$ ne sont pas tous deux à deux distincts.

                  Alors le déterminant à calculer possède deux colonnes identiques donc il est nul, et la formule avec laquelle il doit coïncider s'annule également, donc $\mathcal{P}(n+1)$ est vraie dans ce cas

            \item Supposons que les éléments de $\left\{ a_{0} ,\dots, a_{n+1} \right\}$ sont tous distincts.
                  Notons
                  $$
                    Q(X) = \left| \begin{matrix}
                      1           & 1           & 1           & \dots  & 1           & 1       \\
                      a_{0}       & a_{1}       & a_{2}       & \dots  & a_{n}       & X       \\
                      a_{0}^{2}   & a_{1}^{2}   & a_{2}^{2}   & \dots  & a_{n}^{2}   & X^{2}   \\
                      \vdots      & \vdots      & \vdots      & \ddots & \vdots      & \vdots  \\
                      a_{0}^{n}   & a_{1}^{n}   & a_{2}^{n}   & \dots  & a_{n}^{n}   & X^{n}   \\
                      a_{0}^{n+1} & a_{1}^{n+1} & a_{2}^{n+1} & \dots  & a_{n}^{n+1} & X^{n+1}
                    \end{matrix} \right|
                  $$
                  Sachant que le déterminant d'une matrice est une somme de produits de coefficients de la matrice, puisque tous les coefficients du déterminant $Q(X)$ sont des polynômes en $X$, $Q(X) \in \K[X]$ (car $\K[X]$ est un anneau et donc stable par produit).
                  De plus, en développant le déterminant $Q(X)$ selon sa dernière colonne, on observe d'une part que $\deg Q \leqslant n+1$ et d'autre part que le coefficient de $X^{n+1}$ est le cofacteur de $X^{n+1}$ qui est, d'après $\mathcal{P}(n)$
                  $$
                    \prod_{0\leqslant i<j \leqslant n}(a_{j}-a_{i})
                  $$
                  Et, comme tous les $a_{i}$ sont distincts, ce coefficient est non-nul, donc $\deg Q=n+1$

                  De plus, $Q(a_{0})=0, Q(a_{1})=0, \dots, Q(a_{n})=0$ car le déterminant présente dans chacun des calculs deux colonnes égales. Nous en déduisons que $Q$ admet au moins $(n+1)$ racines deux à deux distinctes, or son degré est exactement $n+1$ donc
                  - il n'y a aucune autre racine
                  - elles sont toutes simples

                  La forme factorisée de $Q$ est donc
                  $$
                    Q(X)=\underbrace{ \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) }_{ \text{coefficient dominant} } \times \underbrace{ \prod_{k=0}^{n}(X-a_{k})^{1} }_{ n+1 \text{ racines simples} }
                  $$
                  Donc
                  \begin{align*}
                    V(a_{0}, a_{1},\dots, a_{n+1}) & =Q(a_{n+1})                                                                                                \\
                                                   & =  \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) \times  \prod_{k=0}^{n}(a_{n+1}-a_{k}) \\
                                                   & = \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) \prod_{\substack{0\leqslant i<j         \\ j=n+1}}^{n}(a_{j}-a_{k}) \\
                                                   & = \prod_{0\leqslant i < j \leqslant n+1} (a_{j}-a_{i})
                  \end{align*}

                  Donc $\mathcal{P}(n+1)$ est vraie
          \end{itemize}
  \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 30}

\begin{question_kholle}[{
        \begin{equation}
          \forall (x, y) \in E^2, \
          \left| \braket{ x | y } \right| \leqslant \|x\|\|y\|
        \end{equation}
        Il y a égalité \ssi $x$ et $y$ sont liés.
      }]{Inégalité de Cauchy-Schwartz dans un espace préhilbertien réel, cas d'égalité}
  Soit $E$ un $\R$-espace vectoriel, et $\braket{ \cdot | \cdot  }$ un produit scalaire sur $E$.
  Soient $(x, y) \in E^{2}$
  \begin{enumerate}
    \item \begin{itemize}[label=$\star$]
            \item Si $y=0$, l'inégalité est une égalité et est évidente
            \item Sinon, posons
                  $$
                    P:\left|\begin{array}{ll} \R &\to \R \\ t &\mapsto \braket{ x+t.y | x+t.y } = t^{2} \|y\|^{2} + 2 t \braket{ x | y }  + \|x\|^{2} \end{array}\right.
                  $$
                  Puisque $\|y\|^{2} \neq 0$, P est un polynôme de degré $2$ à coefficientts réels et positif d'après le caractère positif du produit scalaire (on a donc $\forall t \in \R, P(t)\geqslant 0$)
                  Le discriminant de cette fonction polynômiale est $\Delta = 4 \braket{ x | y }^{2} - 4 \|x\|^{2}\|y\|^{2}$, qui est obligatoirement négatif ou nul puisque $P$ admet au mieux une racine double.
                  Donc $\braket{ x | y }^{2} - \|x\|^{2}\|y\|^{2} \leqslant 0$ donc en prenant la racine carrée $|\braket{ x | y }| \leqslant \|x\|\|y\|$.
          \end{itemize}
    \item \begin{itemize}[label=$\star$]
            \item Supposons que $(x, y)$ est liée, sans perte de généralité, supposons $y = \lambda.x$ alors
                  $$
                    \lvert \braket{ x | \lambda.x } \rvert  = \lvert  \lambda \rvert  \braket{ x | x }  = \lvert  \lambda \rvert  \|x\|^{2} = \| x \| \| \lambda.x\|
                  $$
                  Donc l'inégalité est une égalité.

            \item Réciproquement, supposons que $\lvert  \braket{ x | y } \rvert = \| x\| \| y\|$
                  \begin{itemize}
                    \item Si $y = 0$ alors $(x, y)$ est liée
                    \item Sinon, $\Delta = 4(\braket{ x | y }^{2} - \|x\|\|y\|) = 0$
                          $P$ est un polynôme de degré $2$ de discriminant nul : il admet une racine double $\lambda$
                          Ainsi
                          $$
                            P(\lambda) = 0 \implies \braket{ x+\lambda.y | x+\lambda.y } = 0
                          $$
                          Donc $x+\lambda .y = 0_{E}$ d'après le caractère défini du produit scalaire.
                  \end{itemize}
          \end{itemize}
  \end{enumerate}
\end{question_kholle}

\begin{question_kholle}[{
        L'application
        \begin{equation}
          \chi \left|\begin{array}{ll}
            E & \to E^{*}                                                      \\
            x & \mapsto \left(\begin{array}{ll} E & \to \R
             \\ y &\mapsto \braket{ x | y }\end{array}\right)\end{array}\right.
        \end{equation}
        est un isomorphisme d'espaces vectoriels. $\chi$ est appelé l'isomorphisme canonique entre un espace vectoriel euclidien et son espace dual.
      }]{Isomorphisme entre un espace euclidien et l'espace de ses formes linéaires (Théorème de représentation de Riesz)}
  \begin{itemize}[label=$\star$]
    \item $\chi$ est bien définie car, $\forall x \in E$, par linéarité du produit scalaire en sa seconde variable, $\chi(x): \left|\begin{array}{ll} E &\to \R \\ y &\mapsto \braket{ x | y }  \end{array}\right.$ est une forme linaire sur $E$.


    \item Soient $(x, x') \in E^{2}$ et $\lambda \in \R$ fixés quelconques


          \begin{align*}
            \forall y \in E,  \chi(x + \lambda .x')(y) & = \braket{ x+\lambda.x' | y }                        \\
                                                       & = \braket{ x | y } + \lambda \times\braket{ x' | y } \\
                                                       & = \chi(x)(y) + \lambda \times \chi(x')(y)            \\
                                                       & = (\chi(x) + \lambda . \chi(x') )(y)
          \end{align*}

          Donc $\chi(x+\lambda x') = \chi(x) + \lambda.\chi(x')$, donc $\chi$ est linéaire.

    \item Soit $x \in \ker \chi$ fixé quelconque.
          Alors $\chi (x) = 0_{E^{*}}$
          $$\forall y \in E, \braket{ x | y } = 0$$
          Donc $x \in E^{\perp} = \{ 0_{E} \}$ donc $x = 0_{E}$
          Donc $\chi$ est injective, or $E$ et $E^{*}$ sont de même dimension, donc $\chi$ est bijective.
          Donc $\chi$ est un isomorphisme.
  \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Si $F$ est un sous-espace vectoriel de dimension finie d'un espace préhilbertien réel, $F^{\perp}$ est son supplémentaire orthogonal}


  Soient $(E, \braket{ \cdot | \cdot })$ un espace préhilbertien réel, et $F$ un sous espace vectoriel de dimension finie.
  Alors $F$ et $F^{\perp}$ sont supplémentaires orthogonaux, i.e.
  \begin{equation}
    E = F  \overset{\perp}{\oplus} F^{\perp}
  \end{equation}



  En notant $r = \dim F$, fixons une base orthonormale $(e_{1}, \dots, e_{r})$ de $F$, possible car $F$ est un espace euclidien (dimension finie et muni du produit scalaire induit par $E$).
  \begin{itemize}[label=$\lozenge$]
    \item \underline{Analyse}

          Soit $x \in E$ fixé quelconque, supposons que $\exists (x_{\sslash}, x_{\perp}) \in F \times F^{\perp} = x_{\sslash}+x_{\perp}$
          D'abord $$x_{\sslash} \in F \implies \exists (\lambda_{1}, \dots, \lambda_{r}) \in \R^{r}: x_{\sslash}= \sum_{i=1}^{r}\lambda_{i}.e_{i}$$
          Soit $j \in [ \! [ 1, r ] \!]$ fixé quelconque

          \begin{align*}
            \braket{ x | e_{j} } & =   \left\langle \sum_{i=1}^{r}\lambda_{i}.e_{i} + x_{\perp} \Bigg| e_{j}  \right\rangle                                                                                                                 \\
                                 & = \sum_{i=1}^{r}\lambda_{i}\times \underbrace{ \braket{ e_{i} | e_{j} } }_{ \delta_{ij} }  + \overbrace{ \braket{ \underbrace{ x_{\perp} }_{ \in F^{\perp} } | \underbrace{ e_{j} }_{ \in F } } }^{ =0 } \\
                                 & = \lambda_{j}
          \end{align*}

          Ainsi, $$\left\{ \begin{array}{ll}
              x_{\sslash} & = \sum_{i=1}^{r}\lambda_{i}.e_{i}= \sum_{i=1}^{r}\braket{ x | e_{i} } . e_{i} \\
              x_{\perp}   & = x - x_{\sslash}
            \end{array}\right.$$

    \item \underline{Synthèse}

          Posons donc
          $$\left\{ \begin{array}{ll}
              x_{\sslash} & = \sum_{i=1}^{r} \braket{ x | e_{i} } .e_{i} \\
              x_{\perp}   & = x - x_{\sslash}
            \end{array}\right. $$
          \begin{itemize}[label=$\star$]
            \item $(e_{1}, \dots, e_{r})$ est une base de $F$ donc $x_{\sslash} \in F$
            \item $x_{\sslash}+x_{\perp} = x_{\sslash}+ (x - x_{\sslash}) = x$
            \item Soit $j \in [ \! [ 1, r ] \!]$ fixé quelconque. Calculons $\braket{ x_{\perp} | e_{j} }$

                  \begin{align*}
                    \braket{ x_{\perp} | e_{j} } & = \braket{ x | e_{j} }  - \left\langle  \sum_{i=0}^{r}\braket{ x | e_{i} } .e_{i} \Bigg|  e_{j} \right\rangle                                                         \\
                                                 & = \left\langle x | e_{j} \right\rangle - \sum_{i=0}^{r} \left\langle x | e_{i} \right\rangle\underbrace{  \left\langle e_{i} | e_{j} \right\rangle  }_{ \delta_{ij} } \\
                                                 & = \left\langle x | e_{j} \right\rangle  - \left\langle x | e_{j} \right\rangle  = 0
                  \end{align*}


                  Donc $x_{\perp} \in \{e_{1}, \dots, e_{r}\}^{\perp}$
                  Donc $x_{\perp} \in \text{Vect}\{ e_{1}, \dots, e_{r} \}^{\perp} = F^{\perp}$

          \end{itemize}
  \end{itemize}
  Ainsi, $F$ et $F^{\perp}$ sont supplémentaires orthogonaux.

  De plus
  $$	\forall x \in E, x = \underbrace{ \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i} }_{ \in F } + \underbrace{x - \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i}  }_{ \in F^{\perp} }$$
  Donc $$p_{F}^{\perp}(x) = \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i} $$

\end{question_kholle}

\begin{question_kholle}[{On utilisera le produit scalaire $$
          \left\langle P | Q \right\rangle  = \int_{0}^{1} P(u)Q(u) \, \mathrm du
        $$ }]{Orthonormalisation de la base canonique de $\R_2[X]$}
  Partons de la base canonique de $\R_{2}[X]$.
  \begin{itemize}[label=$\star$]
    \item $P_{1} =X^{0}$ est un vecteur unitaire avec ce produit scalaire

    \item Calcul du second vecteur
          $$
            P_{2}' = X - \left\langle X | 1 \right\rangle.1 = X - \left( \int_{0}^{1} u \, \mathrm du \right) .1 = X-\frac{1}{2}
          $$

          $$
            P_{2} = \frac{P_{2}'}{\|P_{2}'\|} = \frac{P_{2}'}{\sqrt{ \left\langle P_{2}' | P_{2}' \right\rangle  }}= \frac{P_{2}'}{\sqrt{ \int_{0}^{1} \left( u-\frac{1}{2} \right)^{2} \, \mathrm du }}
            = \frac{P_{2}'}{\sqrt{ \frac{1}{12} }}= \sqrt{ 12 }P_{2}'
          $$
          Ce qui donne
          $$
            P_{2}' = 2\sqrt{ 3 }X - \sqrt{ 3 }
          $$

    \item Enfin,

          \begin{align*}
            P_{3}' & = X^{2} - \left\langle X^{2} | 2\sqrt{ 3 }X-\sqrt{ 3 } \right\rangle.(2\sqrt{ 3 }X-\sqrt{ 3 })  - \left\langle X^{2} | 1 \right\rangle .1                            \\
                   & = X^{2} - \left( \int_{0}^{1} 2\sqrt{ 3 }u^{3} - \sqrt{ 3 }u^{2} \, \mathrm du  \right).(2\sqrt{ 3 }X-\sqrt{ 3 })-\left( \int_{0}^{1} u^{2} \, \mathrm du  \right).1 \\
                   & = X^{2} - \frac{\sqrt{ 3 }}{6}(2\sqrt{ 3 }X - \sqrt{ 3 }) - \frac{1}{3}                                                                                              \\ \\
                   & =X^{2} -X + \frac{1}{6}
          \end{align*}


          $$
            P_{3} = \frac{P'_{3}}{\|P_{3}'\|}= \frac{P'_{3}}{\sqrt{ \left\langle P_{3} | P_{3} \right\rangle  }}= \frac{P'_{3}}{\sqrt{ \int_{0}^{1} \left( u^{2}-u +\frac{1}{6} \right)^{2} \, \mathrm du }}
            = \frac{P_{3}'}{\sqrt{ \frac{1}{180} }} =6\sqrt{ 5 }P'_{3} = 6\sqrt{ 5 }\left( X^{2}-X + \frac{1}{6} \right)
          $$

          Donc une base orthonormée de $\R_{2}[X]$ muni de ce produit scalaire est
          $$
            \left( 1, \ 2\sqrt{ 3 }X-\sqrt{ 3 }, \ 6\sqrt{ 5 }\left( X^{2}-X+\frac{1}{6} \right) \right)
          $$
  \end{itemize}
  \begin{figure}[H]
    \centering
    \tdplotsetmaincoords{80}{110} % Set the 3D view angles
    \begin{tikzpicture}[tdplot_main_coords, scale=4]

        % Pas à l'échelle pour que ça soit plus visible.
        \coordinate (A) at (2, 0, 0);
        \coordinate (B) at (1.5, 0.8660, 0);
        \coordinate (C) at (1, 0.8660, 0.55);
        \coordinate (D) at (1, 0.8660, 0);

        \draw[thick,->,blue] (0,0,0) -- (A) node[anchor=south east]{$X^0 = P_1' = P_1$};
        \draw[thick,->,red] (0,0,0) -- (B) node[anchor=north west]{$X$};
        \draw[thick,->,green!60!black] (0,0,0) -- (C) node[anchor=south west]{$X^2$};

        \draw[thick,->,black] (0,0,0) -- (0, 0.8660, 0) node[anchor=south west]{$P_2'$};
        \draw[thick,->,black] (0,0,0) -- (0, 2, 0) node[anchor=north east]{$P_2$};
        
        \draw[thick,->,black] (0,0,0) -- (0, 0, 0.55) node[anchor=north east]{$P_3'$};
        \draw[thick,->,black] (0,0,0) -- (0, 0, 2) node[anchor=north east]{$P_3$};


        \draw[dotted] (B) -- (1.5, 0, 0);
        \draw[dotted] (B) -- (0, 0.8660, 0);

        \draw[<-, black] (1.5, 0.9, 0) -- (0.2, 0.9, 0) node[pos=0.5, anchor=north west]{$\braket{X \mid X^0}.X^0$};

        \draw[->, black] (0, 0, 0.6) -- (1, 0.8660, 0.6) node[pos=0.5, anchor=south west]{$\braket{X^2 \mid X^0}.X^0 + \braket{X^2 \mid X}.X$};

        \draw[dotted] (C) -- (1, 0.8660, 0);
        \draw[dotted] (C) -- (D);
        \draw[dotted] (C) -- (0, 0, 0.55);
        \draw[dotted] (0, 0, 0) -- (D);
        \draw[dotted] (D) -- (1, 0, 0);
        \draw[dotted] (D) -- (1, 0, 0);
    \end{tikzpicture}
    \caption{Orthonormalisation de $(1, X, X^2)$ pour ce produit. Ces vecteurs ne sont ni orthogonaux, ni unitaires pour ce produit scalaire, mais engendrent bien $\R_2[X]$. En soustrayant leurs composantes respectives, on crée une base orthogonale que l'on peut ensuite normer.}
  \end{figure}
\end{question_kholle}

\begin{question_kholle}[{Soit $(E, \left\langle \cdot | \cdot \right\rangle)$ un espace préhilbertien Réel.
        Soient $F$ un sous espace vectoriel de dimension finie de $E$, et $x \in E$.

        L'ensemble $\{ \|x-z\| \mid z \in F \}$ admet une borne inférieure appelée distance de $x$ à $F$ et notée $d(x, F)$, qui est un plus petit élément, atteinte uniquement pour pour $z = p_{F}^{\perp}(x)$
      }]{Distance d'un vecteur à un sous-espace vectoriel de dimension finie}

  $\{ \|x - z\| \mid z \in F \}$ est une partie de $\R$, non vide car elle contient $\|x\|$ pour $z \leftarrow 0_{F}$ d'éléments positifs ou nuls. Elle admet donc une borne inférieure

  $E$ est un espace euclidien, donc $E = F \overset{\perp}{\oplus} F^{\perp}$ donc $x$ se décompose selon ces supplémentaires orthogonaux
  $$x = \underbrace{ p_{F}^{\perp}(x) }_{ \in F } + \underbrace{ x - p_{F}^{\perp}(x) }_{ \in F^{\perp} }$$
  si bien que, pour tout $z \in F$

  \begin{align*}
    \|x - z\|^{2} & =  \|p_{F}^{\perp}(x) - z + x - p_{F}^{\perp}(x)\|^{2}                                                  \\
                  & = \|p_{F}^{\perp}(x) - z \|^{2} + \|x - p_{F}^{\perp}(x)\|^{2} \text{ d'après le théorème de Pythagore} \\
                  & \geqslant \|x - p_{F}^{\perp}(x)\|^{2}
  \end{align*}

  En prenant la racine carrée,
  $$\forall z \in F, \| x  - z\| \geqslant \|x - p_{F}^{\perp }(x)\|$$
  D'où $\| x - p_{F}^{\perp}(x)\|$ minore $\{ \| x - z\| \mid z \in F \}$ et donc sa borne inférieure.

  Or, en remonant le calcul précédent, il y a égalité pour $z = p_{F}^{\perp}(x)$ si bien que la borne inférieure est un plus petit élément, et vaut $d(x, F) = \|x-p_{F}^{\perp}(x)\|$

  De plus, si $z' \in F$ atteint ce plus petit élément on a

  \begin{align*}
    \| x- z'\|^{2}                & = \|p_{F}^{\perp}(x) - z' + x - p_{F}^{\perp}(x)\|^{2}          \\
    \| x - p_{F}^{\perp}(x)\|^{2} & = \|p_{F}^{\perp}(x) - z' \|^{2} + \|x - p_{F}^{\perp}(x)\|^{2} \\
    0                             & = \|p_{F}^{\perp}(x) - z' \|^{2}
  \end{align*}

  Si bien que $p_{F}^{\perp}(x) - z' = 0_{E}$ d'après le caractère défini du produit scalaire.
  Donc le plus petit élément $d(x, F) = \min\{ \|x-z\| \mid z \in F \}$ est uniquement atteint pour $z = p_{F}^{\perp}(x)$.
\end{question_kholle}

\begin{question_kholle}[{
  Soit $(E, \left\langle \cdot | \cdot \right\rangle)$ un espace vectoriel euclidien, $\mathcal{B}=(e_{1}, \dots, e_{n})$ une base orthonormée de $E$.
  Soit $u = \sum_{i=1}^{n}u_{i}.e_{i}$ un vecteur de $E$.
  Soient $(a_{1}, \dots, a_{n}) \in\R^{n} \setminus \{ 0_{\R^{n}} \}$, $\alpha \in \R$ et $H_{\alpha}$ l'hyperplan affine d'équation
  $$
    \sum_{i=1}^{n}a_{i}x_{i}=\alpha
  $$
  }]{Distance à un sous-espace affine}
  Posons $a = \sum_{i=1}^{n}a_{i}.e_{i}$ $H_{0}$ est un hyperplan vectoriel et, $H_{0} = a^{\perp}$ et $H_{0}^{\perp}= \text{Vect}\{ a \}$
  Introduisons $h_{\alpha} \in a^{\perp}$ tel que $H_{\alpha}= h_{\alpha}+H_{0}$ et souvenons nous que $h_{\alpha}=\frac{\alpha}{\| a\|^{2}}.a$

  Observons que l'égalité $H_{\alpha}=h_{\alpha}+H_{0}$ donne
  $$
    \{ \|u-z\| \mid z \in H_{\alpha} \} = \{ \|u-(h_{\alpha}+z')\| \mid z' \in H_{0} \}= \{ \|(u - h_{\alpha})-z'\| \mid z' \in H_{0}\}
  $$

  or, d'après la caractérisation de la distance à un sous-espace quelconque, on a
  \begin{itemize}[label=$\star$]
    \item L'ensemble $\{ \|(u-h_{\alpha}) -z'\| \mid z' \in H_{0} \}$ admet une borne inférieure donc $\{ \| u - z\| \mid z \in H_{\alpha} \}$ aussi qui vaut $\mathrm{d}(u-h_{\alpha}, H_{0})$, ce qui prouve que $\mathrm{d}(u, H_{\alpha})$ est bien définie

    \item $\inf \{  \| (u - h_{\alpha}) - z'\| \mid z' \in H_{0} \}$ est un plus petit élément atteint pour l'unique valeur $z' = p_{H_{0}}^{\perp}(u-h_{\alpha})=p_{H_{0}}^{\perp}(u)$ car $h_{\alpha} \in H_{0}^{\perp}= \ker p_{H_{0}}^{\perp}$, donc $\mathrm{d(u, H_{\alpha})}=\inf \{  \|u-z\| \mid z \in H_{\alpha} \}$ est un plus petit élément atteint pour l'unique valeur $z = h_{\alpha}+p_{H_{0}}^{\perp}(u-h_{\alpha})= h_{\alpha}+ p_{H_{0}}^{\perp}(u)$
          $$
            \mathrm{d}(u, H_{\alpha})= \| u - h_{\alpha}-p_{H_{0}}^{\perp}(u)\|
          $$
  \end{itemize}
  Or $u - p_{H_{0}}^{\perp}(u) = (\mathrm{Id}-p_{H_{0}}^{\perp})(u) = p_{H_{0}^{\perp}}^{\perp}(u) = \left\langle u | \frac{a}{\|a\|} \right\rangle. \frac{a}{\|a\|}$ car $H_{0}^{\perp} = \text{Vect}\{ a \}$ d'où, sachant aussi que $h_{\alpha}= \frac{\alpha}{\|a\|^{2}}.a$
  $$
    \mathrm{d}(u, H_{\alpha})= \|p_{H_{0}^{\perp}}^{\perp}(u)-h_{\alpha}\|= \left\| \left\langle a | \frac{a}{\|a\|} \right\rangle. \frac{a}{\|a\|}- \frac{\alpha}{\|a\|^{2}}.a \right\|
  $$
\end{question_kholle}

\begin{question_kholle}
  {Dénombrement des surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$ et dans $\lient 1; 3 \rient$}

  Soit $n \in \N^*$.

  Il y a ${\left| \lient 1; 2 \rient \right|} ^ {\left| \lient 1; n \rient \right|} = 2^n$ applications de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$.
  Seules les applications constantes $\widetilde{1}$ et $\widetilde{2}$ ne sont pas surjectives.
  Il y a donc $2^n - 2$ surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$.

  Il y a ${\left| \lient 1; 3 \rient \right|} ^ {\left| \lient 1; n \rient \right|} = 3^n$ applications de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$. Les applications non surjectives sont celles dont l'image n'est pas $\lient 1; 3 \rient$. C'est-à-dire, celles dont l'image est de cardinal 1 (les fonctions constantes $\widetilde{1}$, $\widetilde{2}$ et $\widetilde{3}$) et celles dont l'image est de cardinal 2. Ces dernières sont les surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$, $\{ 1; 3 \}$ et $\{ 2; 3 \}$. Comme ces trois ensembles ont la même taille, il y a $3 \times (2^n - 2)$ (voir résultat précédent) applications de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$ dont l'image est de cardinal 2. Ainsi, le nombre de surjections de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$ est $3^n - 3 - 3(2^n -2) = 3^n - 3 \times 2^n + 3$.
\end{question_kholle}

\begin{question_kholle}
  [Soient $E, F$ deux ensembles finis non vides et $f : E \rightarrow F$ telle que tout élément de $F$ possède le même nombre $k \in \N^*$ d'antécédents par $f$.
    Alors $\left|F\right| = \frac{\left|E\right|}{k}$
    \begin{quotation}
      \textquotedblleft Pour compter les moutons, il faut compter les pattes puis diviser par quatre. \textquotedblright
    \end{quotation}]
  {Lemme des bergers}

  Considérons la relation binaire définie sur $E$ par :
  \begin{equation*}
    \forall (x, y) \in E^2, x \sim y
    \iff f(x) = f(y)
  \end{equation*}
  Elle est réflexive, transitive et symétrique donc c'est bien une relation d'équivalence.
  Donc les classes d'équivalence réalise une partition de $E$.
  Nous avons $\displaystyle E = \bigsqcup_{C \in \nicefrac{E}{\sim}} C$ donc, en passant aux cardinaux, $\displaystyle \left|E\right| = \sum_{C \in \nicefrac{E}{\sim}} \left|C\right|$.

  Soit $x \in E$ \fq. Alors $\bar{x} = \left\{ y \in E \;|\; f(x) = f(y) \right\} = f^{-1}(f(\{x\}))$. Par hypothèse, tous les éléments de $F$ ont le même nombre $k$ d'antécédents, or $f({x})$ est un singleton d'élément de $F$ donc $\left|\bar{x}\right| = k$.
  Ainsi $\forall C \in \nicefrac{E}{\sim}, \left|C\right| = k$.

  Posons $\varphi \left|\begin{array}{ccc}
      \nicefrac{E}{\sim} & \mapsto     & F                        \\
      C                  & \rightarrow & f(x) \text{ où } x \in C
    \end{array}\right.$.
  $\varphi$ est bien défini car si $(x, y) \in E$ vérifie $\bar{x} = \bar{y}$ alors $f(x) = f(y)$ donc l'image par $\varphi$ ne dépend pas du représentant de classe choisi.
  $\varphi$ est surjective car soit $z \in F$, $f$ est surjective donc $\exists x_z \in E : f(x_z) = z$ et alors $\varphi(\bar{x_z}) = f(x_z) = z$.
  $\varphi$ est injective car soient $(C, C') \in \left( \nicefrac{E}{\sim} \right)^2, \varphi(C) = \varphi(C')$ alors $\exists (x, x') \in C \times C' : x \sim x'$, comme deux classes d'équivalence sont confondues ou disjointes, $C = C'$.
  Ainsi $\varphi$ est une bijection donc $\left|F\right| = \left|\nicefrac{E}{\sim}\right|$.

  Ainsi $\displaystyle \left|E\right| = \sum_{C \in \nicefrac{E}{\sim}} \left|C\right| = \sum_{C \in \nicefrac{E}{\sim}} k = \left| \nicefrac{E}{\sim} \right| k = \left|F\right| k$.
\end{question_kholle}
\pagebreak\section{Semaine 31}

Pour cette semaine, $E$ est un ensemble fini de cardianl $n \in \N^*$ et $(\Omega, \proba)$ désigne un espace probabilisé fini.

\begin{question_kholle}
  [Soit $p \in \N^*$. Un $p$-partage de $E$ est un $p$-liste $(A_1, \ldots, A_p) \in \mathcal{P}(E)^p$ de parties de $E$ (éventuellement vide), deux à deux disjointes qui recouvrent $E$ c'est-à-dire \tq+* t:
    \begin{equation}
      \forall (i, j) \in \lient 1 ; p \rient,
      i \neq j \implies A_i \cap A_j = \emptyset
      \qquad \text{et} \qquad
      \bigcup_{i=1}^{p} A_i = E
    \end{equation}

    Soient $(n_1, \ldots n_p) \in \N^p$ \tqs $n = n_1 + \ldots + n_p$ est un $p$-partage de $E$ \tq
    \begin{equation*}
      \forall (i, j) \in \lient 1 ; p \rient, \
      \left|A_i\right| = n_i
    \end{equation*}
    Le nombre de $p$-partage de type $(n_1, \ldots, n_p)$ est :
    \begin{equation}
      \frac{n!}{\displaystyle \prod_{i=1}^{p} n_i !}
    \end{equation}
  ]
  {$p$-partage d'un ensemble $E$ et leur dénombrement}

  Considérons les $p$-partages de type $(n_1, \ldots, n_p)$ et appliquons le principe des choix successifs :
  \begin{equation*}
    \left(
    \underbrace{A_1}_{\binom{n}{n_1} \text{ choix}},
    \underbrace{A_2}_{\binom{n}{n_2} \text{ choix}},
    \underbrace{A_3}_{\binom{n}{n_3} \text{ choix}},
    \ldots,
    \underbrace{A_p}_{\binom{n}{n_p} \text{ choix}}
    \right)
  \end{equation*}
  donc il y a
  \begin{equation*}
    \frac{ n! }{n_1! \cancel{(n-n_1)!} }
    \frac{ \bcancel{(n-n_1)!} }{n_2! \cancel{(n-n_1-n_2)!} }
    \frac{ \bcancel{(n-n_1-n_2)!} }{n_2! \cancel{(n-n_1-n_2-n_3)!} }
    \ldots
    \frac{ \bcancel{(n-(n_1+\ldots+n_{p-1})!} }{n_p! \underbrace{(n_1+\ldots+n_p)!}_{=0!} }
  \end{equation*}
  Donc, au total, il y a $\frac{n!}{n_1! n_2! \ldots n_p!}$ $p$-partages.
\end{question_kholle}

\begin{question_kholle}
  [Soit $B$ un évènement de probabilité non nulle.
    L'application $\proba_B$
    {\begin{equation}
          \proba_B \left| \begin{array}{ccc}
            \mathcal{P}(\Omega) & \mapsto     & [0;1]                                              \\
            A                   & \rightarrow & \displaystyle \frac{\proba( A \cap B )}{\proba(B)}
          \end{array} \right.
        \end{equation}}
    est une probabilité sur sur $\Omega$. ]
  {Une probabilité conditionnelle est une probabilité}

  ~\\
  \begin{liste}
    \item Soit $A \in \mathcal{P}(\Omega)$ \fq. \\
    On a $\emptyset \subset A \cap B  \subset B$ donc par croissance de la probabilité, $0 = \proba(\emptyset) \leqslant \proba(A \cap B) \leqslant \proba(B)$.
    En divisant par $\proba(B) \neq 0$, $0 \leqslant \proba_B(A) \leqslant 1$. Donc $\proba_B$ est \textit{bien définie}.
    \item $\proba_B(\Omega)
      = \frac{\proba(\Omega \cup B)}{\proba(B)}
      = \frac{\proba(B)}{\proba(B)}
      = 1$
    \item Soient $(A, A') \in \mathcal{P}(\Omega)^2$ \fq* \tq* $A$ et $A'$ sont incompatibles.
    \begin{equation}
      \begin{aligned}
        \proba_B(A \sqcup A')
         & = \frac{ \proba(B \cap (A \sqcup A') ) }{ \proba(B) }                                                                                  \\
         & = \frac{ \proba( (B \cap A) \sqcup (B \cap A') ) }{ \proba(B) } \text{ car } (B \cap A) \cap (B \cap A') \subset A \cap A' = \emptyset \\
         & = \frac{ \proba(B \cap A) + \proba(B \cap A') }{ \proba(B) }                                                                           \\
         & = \proba_B(A) + \proba_B(A')
      \end{aligned}
    \end{equation}
  \end{liste}
  Ainsi, $\proba_B$ est bien une probabilité sur $\Omega$.
\end{question_kholle}

\begin{question_kholle}{Si $A$ et $B$ sont des événements indépendants, alors $A$ et $\overline{B}$ aussi}
  Supposons donc que $0 \leqslant \proba(A) \leqslant 1$ et $0 \leqslant \proba(B) \leqslant 1$.
  D'une part, $\{ B, \bar{B} \}$ constitue un système complet donc

  \begin{align*}
    \proba(A)                                 & = \proba(A \cap B) + \proba(A \cap \bar{B})    \\
    \iff \proba(A)                            & = \proba(A) \proba(B) + \proba(A \cap \bar{B}) \\
    \iff \proba(A) - \proba(A) \proba(B)      & = \proba(A \cap \bar{B})                       \\
    \iff \proba(A)(1-\proba(B))               & = \proba(A \cap \bar{B})                       \\
    \iff \proba(A) \proba\left(\bar{B}\right) & = \proba(A \cap \bar{B})
  \end{align*}

  Donc $A$ et $\bar B$ sont indépendants
\end{question_kholle}

\begin{question_kholle}[
    \begin{equation}
      \forall n \in \N^*, \
      \forall A \in \mathcal{P}(\Omega)^n, \
      \proba\left( \bigcap_{i=1}^{k}A_{i} \right) =
      \proba(A_{1}) \proba_{A_{1}}(A_{2}) \proba_{A_{1} \cap A_{2}}(A_{3}) \dots \proba_{A_{1} \cap \dots \cap A_{n-1}}(A_{n})
    \end{equation}
  ]{Formule des probabilités composées}
  Procédons par récurrence.
  Soient $(A_{1}, \dots, A_{n})$, $n$ événements tels que $\proba\left( \bigcap_{i=1}^{n} A_{i} \right) \neq 0$.
  Pour tout $k \in \lient 2, n \rient$, posons
  $$\mathcal{H}_{k} : " \proba\left( \bigcap_{i=1}^{k}A_{i} \right) = \proba(A_{1})\proba_{A_{1}}(A_{2})\proba_{A_{1} \cap A_{2}}(A_{3})\proba_{A_{1} \cap A_{2} \cap A_{3}}(A_{4})\dots \proba_{A_{1} \cap\dots \cap A_{k-1}}(A_{k})"$$

  \begin{itemize}[label=$\star$]

    \item Initialisation, $k \leftarrow 2$
          d'une part, $\bigcap_{i=1}^{n}A_{i} \subset A_{1}$, donc par croissance de $\proba$, $$0<\proba\left( \bigcap_{i=1}^{n}A_{i} \right) \leqslant \proba(A_{1})$$
          Si bien que $\proba(A_{1}) \neq 0$ donc la probabilité conditionnelle $\proba_{A_{1}}$ a un sens.
          D'où, par définition d'une probabilité conditionnelle:
          $$\proba(A_{1} \cap A_{2}) = \proba(A_{1}) \proba_{A_{1}}(A_{2})$$
          Donc $\mathcal{H}_{2}$ est vérifiée.

    \item Hérédité, Soit $k \in [ \! [ 2, n - 1] \!]$ fixé quelconque tel que $\mathcal{H}_{k}$ est vérifiée.

          D'abord, remarquons que $\bigcap_{i=1}^{n}A_{i} \subset \bigcap_{i=1}^{k}A_{i}$ donc par croissance de $\proba$,
          $$0 < \proba\left( \bigcap_{i=1}^{n}A_{i} \right)\leqslant \proba\left( \bigcap_{i=1}^{k}A_{i} \right)$$
          Si bien que $\proba\left( \bigcap_{i=1}^{k} A_{i} \right) \neq 0$ donc la probabilité conditionnelle $\proba_{A_{1}\cap\dots \cap A_{k}}$  a un sens.


          \begin{align*}
            \proba\left( \bigcap_{i=1}^{k+1}A_{i} \right)
             & = \proba\left( \left( \bigcap_{i=1}^{k}A_{i} \right) \cap A_{k+1} \right)                                                                                            \\
             & = \proba\left( \bigcap_{i=1}^{k}A_{i} \right)\proba_{\bigcap _{i=1}^{k}A_{i}}(A_{k+1})                                                                               \\
             & = \proba(A_{1})\proba_{A_{1}}(A_{2})\proba_{A_{1}\cap A_{2}}(A_{3})\dots \proba_{A_{1} \cap \dots \cap A_{k-1}}(A_{k}) \proba_{A_{1} \cap \dots \cap A_{k}}(A_{k+1})
          \end{align*}
          Donc $\mathcal{H}_{k+1}$ est aussi vérifiée
  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Formule des probabilités totales et formule de Bayes}

  \underline{Formule des probabilités totales}

  Soit $(A_{1}, \dots A_{n})$ un système complet d'événements.
  Comme ils sont incompatibles
  $$\proba\left( \bigsqcup_{k=1}^{n} A_{k} \right)= \sum_{k=1}^{n}\proba(A_{k})$$

  Le système est de plus complet donc $\bigsqcup _{k=1}^{n} A_{k} = \Omega$. Donc $\sum_{k=1}^{n}\proba(A_{k}) = 1$.

  $(A_{1}, \dots , A_{n})$ sont aussi deux à deux incompatibles, donc $(B \cap A_{1}, \dots B \cap A_{n})$ aussi.
  De plus $B = B \cap \Omega = B \cap \left( \bigsqcup_{k=1}^{n}A_{k} \right) = \bigsqcup_{k=1}^{n}(B \cap A_{k})$.
  Donc
  $$\proba(B)= \proba\left( \bigsqcup_{k=1}^{n}(B \cap A_{k}) \right) = \sum_{k=1}^{n}\proba(B \cap A_{k})$$
  De plus, en passant aux probabilités conditionnelles $\left(\proba_{A_{i}}\right)_{1\leqslant i\leqslant n}$ on a

  \begin{equation}
    \proba(B) = \sum_{k=1}^{n}\proba(A_{k})\proba_{A_{k}}(B)
  \end{equation}

  \underline{Formule de Bayes}

  Soient $A$ et $B$ deux événements de probabilité non nulle, on a alors :
  $$\proba(A)\proba_{A}(B) = \proba(A \cap B) = \proba(B)\proba_{B}(A)$$
  donc

  \begin{equation}
    \proba_{A}(B) = \frac{\proba(B)\proba_{B}(A)}{\proba(A)}
  \end{equation}
\end{question_kholle}

\begin{question_kholle}[{
        Soit $X$ une variable alétoire sur $\Omega$ et $g$ une fonction définie sur $X(\Omega)$.
        La loi de probabilité $Y = g(X)$ est donnée par $Y(\Omega) = g(X(\Omega))$ et
        \begin{equation}
          \forall y \in Y(\Omega), \proba_{Y}(\{ y \})= \proba(Y=y) = \sum_{x \in g^{-1}(\{ y \})}\proba(X=x)= \sum_{\substack{x \in X(\Omega)\\ g(x)=y}}\proba(X=x)
        \end{equation}
      }]{Loi d'une fonction de $X$}

  Utilisons le système complet $(X = x)_{x \in X(\Omega)}$ associé à la variable aléatoire $X$ et la formule des probabilités totales

  \begin{align*}
    \proba_{Y}(\{ y \}) = \proba(Y=y) & = \sum_{x \in X(\Omega)}\proba((Y=y) \cap (X=x)) \\
                                      & = \sum_{\substack{x \in X(\Omega)                \\ g(x)=y}}\proba((g(X)=y) \cap (X=x)) + \sum_{\substack{x \in X(\Omega)\\ g(x)\neq y}}\proba((g(X)=y) \cap (X=x))
  \end{align*}


  Remarquons ainsi que
  \begin{itemize}[label=$\star$]
    \item Si $g(x) = y$
          $$
            \omega \in(X=x) \implies X(\omega)=x \implies g(X(\omega)) = g(x) \implies \omega \in (g(X) = y)
          $$
          D'où $(X = x) \subset (g(X)=y)$ donc $(g(X) = y) \cap(X=x)=(X=x)$

    \item Sinon, si $g(x) \neq y$
          $$
            \omega \in (X=x) \implies X(\omega) = x \implies g(X(\omega)) = g(x) \neq y \implies \omega \not\in (g(X) = y)
          $$
          Dans ce cas, $(g(X) = y) \cap (X = x) = \emptyset$

  \end{itemize}
  Ainsi,

  \begin{align*}
    \proba_{y}(\{ y \}) & = \sum_{\substack{x \in X(\Omega)         \\ g(x)=y}}\proba(\underbrace{ (g(X)=y) \cap (X=x) }_{ = (X=x) }) + \underbrace{ \sum_{\substack{x \in X(\Omega)\\ g(x)\neq y}}\proba((g(X)=y) \cap (X=x)) }_{ =0 }\\
                        & = \sum_{\substack{x \in X(\Omega)         \\ g(x)=y}}\proba(X=x) \\
                        & = \sum_{x \in g^{-1}(\{ y \})}\proba(X=x)
  \end{align*}


\end{question_kholle}
\begin{question_kholle}{Si $X \geqslant 0$ presque sûrement, $\esp(X) = 0 \iff X = 0$ presque sûrement}
  Soit $X\geqslant 0$ presque sûrement
  \begin{itemize}
    \item Supposons que $\esp(X) = 0$
          Par hypothèse, l'évènement $(X<0)$ est négligeable donc

          \begin{align*}
            \esp (X) & = \sum_{\omega \in \Omega}X(\omega)\proba(\{ \omega \})                                                                                                                                                                   \\
                     & = \sum_{\omega \in (X = 0)} \underbrace{ X(\omega) }_{ = 0 } \proba(\{ \omega  \}) + \sum_{\omega \in (X<0)}X(\omega) \underbrace{ \proba(\{ \omega \}) }_{ =0 } + \sum_{\omega \in (X >0)} X(\omega)\proba(\{ \omega \}) \\
                     & = \sum_{\omega \in (X >0)} X(\omega)\proba(\{ \omega \})
          \end{align*}

          Soit $\omega_{0} \in (X>0)$ fixé quelconque
          La nullité de l'espérance donne
          $$0 \leqslant X(\omega_{0})\proba(\{ \omega_{0} \}) \leqslant \sum_{\omega \in(X>0)}X(\omega) P(\{ \omega \})= \esp(X) = 0$$
          donc $X(\omega_{0})\proba(\{ \omega_{0} \}) = 0$, or $X(\omega_{0})>0$ donc $\proba(\{ \omega_{0} \})=0$
          donc
          $$\proba(X>0) = \sum_{\omega_{0} \in (X>0)} \proba(\{ \omega_{0} \}) = 0$$
          Donc  $(X>0)$ est négligeable, mais $(X<0)$ est négligeable aussi, donc
          $$0\leqslant \proba((X>0) \cup (X<0))\leqslant \proba(X>0) + \proba(X<0)=0$$
          Ainsi l'évènement contraire de $(X>0) \cup (X<0)$, qui est $(X=0)$ est certain.

    \item Supposons $X=0$ presque sûrement.

          \begin{align*}
            \esp(X) & = \sum_{\omega \in \Omega}X(\omega)\proba(\{ \omega \})                                                                                                              \\
                    & = \sum_{\omega \in (X = 0)} \underbrace{ X(\omega) }_{ =0 } \proba(\{ \omega \}) + \sum_{\omega \in (X \neq 0)} X(\omega) \underbrace{ \proba(\{ \omega \}) }_{ =0 } \\
                    & = 0
          \end{align*}

  \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Calcul de l'espérance et la variance d'une variable aléatoire suivant une loi binomiale}
  Soit $n \in \mathbb{N}^*$ et $p \in [0, 1]$.
  Supposons que $X \hookrightarrow \mathcal{B}(n, p)$.

  \begin{align*}
    \esp(X) & = \sum_{\omega \in X(\Omega)}\omega \proba(X = \omega)                   \\
            & = \sum_{k=0}^{n}k\proba(X=k)                                             \\
            & = \sum_{k=0}^{n}k \binom{n}{k} p^{k}(1-p)^{n-k}                          \\
            & = \sum_{k=0}^{n}k \frac{n!}{k! (n-k)!} p^{k}(1-p)^{n-k}                  \\
            & = n \sum_{k=1}^{n} \frac{(n-1)!}{(k-1)! ((n-1)-(k-1))!} p^{k}(1-p)^{n-k} \\
            & = n \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k}(1-p)^{n-k}                     \\ \\
            & = n \sum_{j=0}^{n-1} \binom{n-1}{j} p^{j+1}(1-p)^{n-1-j}                 \\
            & = n p \sum_{j=0}^{n-1} \binom{n-1}{j} p^{j}(1-p)^{n-1-j}                 \\ \\
            & = np (p + (1-p))^{n-1} = np
  \end{align*}
  Pour la variance, calculons d'abord $\esp(X^{2})$

  \begin{align*}
    \esp(X^{2}) & = \sum _{k=0}^{n}k^{2} \proba(X=k^{2})                                                                                                                                                                         \\
                & =  \sum_{k=1}^{n} k \underbrace{ k \binom{n}{k} }_{ n \binom{n-1}{k-1} }p^{k}(1-p)^{n-k}                                                                                                                       \\
                & = n \sum_{k=1}^{n} \underbrace{ k }_{ (k-1)+1 } \binom{n-1}{k-1} p^{k}(1-p)^{n-k}                                                                                                                              \\
                & = n \sum_{k=1}^{n}(k-1)\binom{n-1}{k-1}p^{k}(1-p)^{n-k}+ n \sum_{k=1}^{n}\binom{n-1}{k-1}p^{k}(1-p)^{n-k}                                                                                                      \\
                & = n \sum_{k=2}^{n}\underbrace{ (k-1)\binom{n-1}{k-1} }_{ (n-1)\binom{n-2}{k-2} } p^{k}(1-p)^{n-k} + n \sum_{k=1}^{n}\binom{n-1}{k-1}p^{k}(1-p)^{n-k}                                                           \\
                & = n(n-1) \underbrace{ \sum_{i=0}^{n-2}\binom{n-2}{i}p^{i+2}(1-p)^{(n-2)-i}  }_{ \text{ en posant } i = k-2 }+ n \underbrace{ \sum_{i=0}^{n-1}\binom{n-1}{i}p^{i+1}(i-p)^{(n-1)-i} }_{ \text{en posant }i=k-1 } \\
                & = n(n-1)p^{2}(p+(1-p))^{n-2} + np(p+(1-p))^{n-1}                                                                                                                                                               \\
                & = n(n-1)p^{2}+np                                                                                                                                                                                               \\
                & = np((n-1)p+1)
  \end{align*}

  D'où,
  $$
    \variance(X) = \esp(X^{2}) - \esp(X)^{2}=np((n-1)p+1)- n^{2} p ^{2}=np(1-p)
  $$

  \noindent \textbf{Calcul alternatif de $\mathbf{E^2}$} En utilisant la formule de transfert pour $f \leftarrow \left( \begin{matrix}
        X(\Omega) = \lient 0; n \rient & \to     & \lient 0; n(n-1) \rient \\
        x                              & \mapsto & x(x-1)
      \end{matrix} \right)$


  \begin{align*}
    \esp(X(X - 1)) & = \sum_{k=0}^{n}k(k-1)\proba(X=k)                               \\
                   & = \sum_{k=0}^{n}k(k-1)\binom{n}{k}p^{k}(1-p)^{n-k}              \\
                   & = n(n-1) \sum_{k=2}^{n}\binom{n-2}{k-2}p^{k}(1-p)^{n-k}         \\
                   & = n(n-1)p^{2} \sum_{j=0}^{n-2} \binom{n-2}{j}p^{j}(1-p)^{n-2-j} \\
                   & = n(n-1) p^{2} (p + (1-p))^{n-2}                                \\
                   & = n(n-1)p^{2}
  \end{align*}

  Donc en remarquant que $$\esp(X^{2}) = \esp(X(X-1)+X) = \esp(X(X-1))+\esp(X)= n(n-1)p^{2} + np$$
  Donc
  $$\variance(X) = \esp(X^{2})- \esp(X)^{2} = n(n-1)p^{2} + np - n^{2}p^{2} = np(1-p)$$


\end{question_kholle}
\end{document}
