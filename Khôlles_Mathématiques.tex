% Ceci est un fichier généré automatiquement.
% Ne pas le modifier directement. Exécuter $ make pour le générer.
% Il rassemble les questions de khôlles de toutes les semaines.

\documentclass{article}

\usepackage{kholles-bis}

\begin{document}
\maketitle

\tableofcontents
\pagebreak\section{Semaine 6}


\begin{question_kholle}{Montrer que si $f$ est impaire et bijective, alors $f^{-1}$ est aussi impaire. Donnez un/des exemples.} 
	Soit $f: I \to F$, avec $I,F$ deux parties non-vides de $\mathbb{R}$, une telle fonction et notons $f^{-1}$ sa bijection réciproque. Si $f$ est impaire sur $I$, alors pour tout $x\in I$, $-x\in I$, ainsi $I$ est centré en $0$ et on a : 
	
	\begin{equation*}
	    \forall x \in I, \ f(-x) = -f(x).
	\end{equation*}
	
	Ainsi, prenons $y\in F$, alors $-y \in F$ par imparité et bijectivité de $f$. On a donc : 
	
	\begin{eqnarray*}
		f^{-1}(-y) & = & f^{-1}(-f(f^{-1}(y))) \\
			& = & f^{-1}(f(-f^{-1}(y))) \\
		    & = & -f^{-1}(y).
	\end{eqnarray*}
	
	\
	
	D'où l'imparité de $f^{-1}$.
	
	\
	
	Pour ce qui est de l'exemple, prenons notre fonction bijective impaire préférée, la fonction $\textstyle \sin |_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right] }^{[-1,1]}$ que l'on notera $\widetilde{\sin}$. Sa bijection réciproque est bien entendu $\textstyle \arcsin : [-1,1] \to \left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]$.
	
	De la même manière que dans la démonstration du cas général, prenons $y\in [-1, 1]$, comme $[-1,1]$ est centré en $0$, $-y\in [-1,1]$, on a dès lors : 
	
	\begin{eqnarray*}
		\arcsin(-y) & = & \arcsin(-\widetilde{\sin}(\arcsin(y))) \\
			& = & \arcsin(\widetilde{\sin}(-\arcsin(y))) \\
		    & = & -\arcsin(y).
	\end{eqnarray*}
\end{question_kholle}

\begin{question_kholle}{Limite (et preuve) lorsque $x$ tend vers $+\infty$ de $\frac{(\ln x)^{\alpha}}{x^{\beta}}$ pour $\alpha ,\beta \in \left( \mathbb{R}_+^*\right) ^2$.} 

	Premièrement, posons : 
	
	\begin{equation*}
	    \forall  (x,\alpha,\beta)\in [1,+\infty[ \times \left( \mathbb{R}_+^*\right) ^2, \quad  f_{\alpha,\beta}(x)=\frac{(\ln x)^{\alpha}}{x^{\beta}}.
	\end{equation*}
	
	Deuxièmement, montrons que : 
	\[ \frac{\ln (x)}{x^2}\xrightarrow[n \to +\infty ]{} 0. \]
	
	Soit $x \in [1,+\infty[ \ = \mathcal{A}$. Nous savons que la fonction $\ln$ est concave sur $\mathbb{R}_+^*$, donc en particulier sur $\mathcal{A}$. Ainsi, $\ln$ est en dessous de toutes ses tangentes, d'où : 
	\[ 
	\forall x \in \mathcal{A}, \quad 0 \; \leq \; \ln (x) \; \leq \; x-1.
	\]
	
	\newpage
	
	Illustration de l'inégalité : 
	
	\
	
	\begin{center}
		\definecolor{ttttff}{rgb}{0.2,0.2,1}
		\definecolor{ffttww}{rgb}{1,0.2,0.4}
		\definecolor{cqcqcq}{rgb}{0.75,0.75,0.75}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
			\draw [color=cqcqcq,dash pattern=on 1pt off 1pt, xstep=1.0cm,ystep=1.0cm] (-1,-2) grid (3,2);
			\draw[->,color=black] (-1,0) -- (3,0);
			\foreach \x in {-1,1,2}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0,-2) -- (0,2);
			\foreach \y in {-2,-1,1}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1,-2) rectangle (3,2);
			\draw[color=ffttww, smooth,samples=100,domain=3.244085366007135E-3:3.0] plot(\x,{ln(\x)});
			\draw[color=ttttff, smooth,samples=100,domain=-1.0:3.0] plot(\x,{\x-1});
			\draw (12.46,5.36) node[anchor=north west] {y = x-1};
			\draw (9.62,5.22) node[anchor=north west] {y = ln(x)};
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 1.} $\ln$ en rouge et la première bissectrice en bleu. 
	\end{center}
	
	\
	
	On peut alors diviser par $x^2$ (car $x \neq 0$): 
	
	\
	
	\[
		\forall x \in \mathcal{A},\quad 0 \; \leq \; \underset{f_{1,2}(x)}{\underbrace{\frac{\ln (x)}{x^2}}} \; \leq \; \underset{\xrightarrow[x\to+\infty]{} \ 0}{\underbrace{\frac{1}{x}}} - \underset{\xrightarrow[x\to+\infty]{} \ 0}{\underbrace{\frac{1}{x^2}}}.
	\]
	
	\
	
	Donc par théorème d'encadrement $f_{1,2}(x)\xrightarrow[x\to+\infty]{} 0$.
	
	\
	
	Dernièrement, le cas général. Soit $x\in \mathcal{A}$ et soient $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2$. On fait une preuve directe. 
	
	\begin{eqnarray*}
	    \frac{(\ln (x))^\alpha}{x^\beta} & = & \left( \frac{\ln (x)}{x^{\frac{\beta}{\alpha}}} \right)^\alpha \\[1.5ex]
	    & = & \underset{\underset{\text{par produit}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{\underset{c^{\underline{te}} \ \text{(définie!)}}{\underbrace{\left( \frac{2\alpha}{\beta} \right)^\alpha}} \cdot \underset{\underset{\text{par composition des limites}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{\left[ \underset{\underset{\text{d'après le dernier point}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{ \frac{\ln \left( x^{\frac{\beta}{2 \alpha}} \right) }{\left( x^{\frac{\beta}{2\alpha}} \right)^2} }}\right]^\alpha}}}}.
	\end{eqnarray*}

\end{question_kholle}

\

%-------------------------------------------------------------

\begin{question_kholle}{Limite en $0$ de $\frac{1-\cos (x)}{x^2}$ et limite en $+\infty$ suivant $n$ de $\frac{\left(q^n \right)^\alpha}{(n!)^\beta}$ pour $q\in \mathbb{R}$ et $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2.$} 

	\
	
	Montrons que $\frac{1-\cos (x)}{x^2} \xrightarrow[x\to 0]{} \frac{1}{2}$.
	
	\
	
	On fait toujours une preuve directe. 
	\begin{eqnarray*}
	    \lim_{x\to0} \ \frac{1-\cos (x)}{x^2} & = & \lim_{x\to0} \ \frac{1-\cos \left( \frac{2x}{2}\right) }{x^2} \\[1ex]
	    & = & \lim_{x\to0} \ \frac{1-\left( 1-2\sin ^2 \left( \frac{x}{2}\right) \right) }{x^2} \\[1ex]
	    & = &  \lim_{x\to0} \ \frac{2\sin ^2 \left( \frac{x}{2}\right) }{4 \left( \frac{x}{2}\right) ^2} \\[1ex]
	    & = & \lim_{x\to0} \ \underset{\underset{\text{par produit}}{\xrightarrow[x\to 0]{} \ \frac{1}{2}}}{\underbrace{\underset{c^{\underline{te}}}{\underbrace{\frac{1}{2}}} \cdot \underset{\underset{\text{par composition}}{\xrightarrow[x\to 0]{} \ 1}}{\underbrace{\left[\underset{\underset{\text{limite usuelle}}{\xrightarrow[x\to 0]{} \ 1}}{\underbrace{\frac{\sin \left( \frac{x}{2}\right) }{\left( \frac{x}{2}\right)} }} \right] ^2}}}} \\[1ex]
	    & = & \frac{1}{2}
	\end{eqnarray*}
	
	\
	
	
	
	\
	
	Trouvons la limite, sous réserve d'existence, de $\frac{\left(q^n \right)^\alpha}{(n!)^\beta}$ pour $q\in \mathbb{R}$ et $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2$ suivant $n$ en $+\infty$.
	
	\
	
	Remarquons que si $q\leq0$, il est \textbf{\textit{nécessaire}} d'avoir $\alpha\in\mathbb{Z}^*$ sinon l'expression n'a tout simplement \textbf{\textit{aucun sens}}. De fait, on supposera $q>0$ tout le long, les cas $q<0$ se font naturellement (convergence pour $q\in \mathbb{R_-}$).
	
	\
	
	Soit donc $0<q<1$, ce cas est immédiat, $\left( \left(q^n \right)^\alpha\right)_{n\in\mathbb{N}}=\left( \left(q^\alpha \right)^n\right)_{n\in\mathbb{N}}$, donc il s'agit de la suite géométrique de raison $q^\alpha \in ]0,1[$ et de premier terme $q^{\min_{I}(n)\alpha}$ ($\min_{I}(n)$, avec $I$ une partie non vide de $\mathbb{N}$, car la suite ne démarre pas forcément à $0$), donc elle converge vers $0$.
	
	\
	
	Si $q\geq 1$, on montre le cas trivial $\alpha = \beta =1$ : 
	\[
	\forall n\in [\![ \lfloor q \rfloor +1,+\infty [\![, \quad 0 \leq \frac{q^n}{n!} = \underset{=\ \lambda \text{ (une constante)}}{\underbrace{\frac{q}{1} \times \frac{q}{2} \times \dots \times \frac{q}{\lfloor q \rfloor} }}\times \underset{\leq 1}{\underbrace{\frac{q}{\lfloor q\rfloor +1}}} \times \dots \times \underset{\leq1}{\underbrace{\frac{q}{n-1}}} \times \frac{q}{n} \leq \underset{\xrightarrow[n\to +\infty]{} \ 0}{\underbrace{\frac{\lambda q}{n}}}
	\]
	
	\
	
	Par théorème d'existence de limite par encadrement, $\left( \frac{q^n}{n!} \right)_{n\in \mathbb{N}}$ converge et sa limite est $0$.
	
	\
	
	Soient $(\alpha,\beta)\in \mathbb{R}^*_+$, montrons le cas général pour $q\geq 1$.
	
	\[
	\forall n \in \mathbb{N}, \quad \frac{(q^n)^\alpha}{(n!)\beta} = \left( \frac{\left(q^{\frac{\alpha}{\beta}}\right)^n}{n!} \right)^\beta = \underset{\underset{\text{par composition des limites }(\beta>0)}{\xrightarrow[n\to +\infty]{} 0}}{\underbrace{\left( \underset{\underset{\text{c'est le cas trivial}}{\xrightarrow[n\to +\infty]{} 0}}{\underbrace{\frac{\left(q^{\frac{\alpha}{\beta}}\right)^n}{n!}}} \right)^\beta}}
	\]
	
\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arcsin$.}
	Premièrement, ladite fonction est la bijection réciproque de la fonction $\widetilde{\sin}$ (voir \textbf{1}.). D'où : 
	\begin{equation*}
		\arcsin = \left\{  
		\begin{array}{c c c}
		[-1,1] & \to & [-\frac{\pi}{2} , \frac{\pi}{2}] \\ [1ex]
		x & \mapsto & \left( \widetilde{\sin} \right)^{-1}(x)
		\end{array} 
		\right.
	\end{equation*}
	
	\
	
	Ainsi, pour $x\in [-1,1]$, $\arcsin (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left[-\frac{\pi}{2} , \frac{\pi}{2}\right]$, $\sin(\theta) = x$. 
	
	\
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\sin}$ : 
	
	\begin{enumerate}
	    \item $\arcsin$ est impaire.
	    \item $\arcsin$ est strictement croissante sur $[-1,1]$.
	    \item $\arcsin \in \mathcal{C}^0\left([-1,1],[-\frac{\pi}{2} , \frac{\pi}{2}] \right)$.
	    \item $\arcsin \in \mathcal{D}^1\left(]-1,1[,\left]-\frac{\pi}{2} , \frac{\pi}{2}\right[ \right)$.
	    \item $\arcsin'(x) = \frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
	    \item $\arcsin$ admet deux demi-tangentes verticales en $-1$ et $1$.
	\end{enumerate}
	
	\
	
	Graphe de $\arcsin$ : 
	\begin{center}
		\definecolor{ffttww}{rgb}{1,0.2,0.4}
		\definecolor{ttzzff}{rgb}{0.2,0.6,1}
		\definecolor{zzffzz}{rgb}{0.6,1,0.6}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.910828025477707cm,y=1.910828025477707cm]
			\draw[->,color=black] (-1.57,0) -- (1.57,0);
			\foreach \x in {-1.5,-1,-0.5,0.5,1,1.5}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0,-1.57) -- (0,1.57);
			\foreach \y in {-1.5,-1,-0.5,0.5,1,1.5}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1.57,-1.57) rectangle (1.57,1.57);
			\draw[color=zzffzz] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.57:1.57]; plot sin(x)};
			\draw[color=ttzzff] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-1.57:1.57]; plot asin(x)};
			\draw[color=ffttww] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-1.57:1.57]; plot x};
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(--1.57-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (1,-1.57) -- (1,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (1.57,-1.57) -- (1.57,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(--1-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (-1.57,-1.57) -- (-1.57,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (-1,-1.57) -- (-1,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(-1-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(-1.57-0*\x)/1});
			\draw [->] (-1.57,-1) -- (-1.18,-1);
			\draw [->] (-1,-1.57) -- (-1,-1.16);
			\draw [->] (1,1.57) -- (1,1.16);
			\draw [->] (1.57,1) -- (1.12,1);
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 2.} $\arcsin$ en bleu, $\widetilde{\sin}$ en vert et la première bissectrice en rouge.
	\end{center}
	
	\
	
	On a aussi, grâce au taux d'accroissement en 0 d'$\arcsin$ : 
	\[
		\lim_{x\to0} \frac{\arcsin(x)}{x} \ = \ 1.
	\]
	
	\
	
	Puis finalement (visible sur le graphe) : 
	\[
		\forall x \in [0,1], \quad \arcsin(x) \geq x.
	\]
\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arccos$.} 

	Premièrement, ladite fonction est la bijection réciproque de la fonction $\cos |_{[0,\pi]}^{[-1,1]} := \widetilde{\cos}$. D'où : 
	\begin{equation*}
		\arccos = \left\{  
		\begin{array}{c c c}
		[-1,1] & \to & [0 , \pi] \\ [1ex]
		x & \mapsto & \left( \widetilde{\cos} \right)^{-1}(x)
		
		\end{array} 
		\right.
	\end{equation*}
	
	\
	
	Ainsi, pour $x\in [-1,1]$, $\arccos (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle [0 ,\pi]$, $\cos(\theta) = x$.
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\cos}$ : 
	
	\begin{enumerate}
	    \item $\arccos$ est strictement décroissante sur $[-1,1]$.
	    \item $\arccos \in \mathcal{C}^0\left([-1,1],[0 , \pi] \right)$.
	    \item $\arccos \in \mathcal{D}^1\left(]-1,1[,]0 ,\pi [ \right)$.
	    \item $\arccos'(x) = -\frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
	    \item $\arccos$ admet deux demi-tangentes verticales en $-1$ et $1$.
	\end{enumerate}
	
	\
	
	Graphe de $\arccos$ : 
	\begin{center}
		\definecolor{cczzff}{rgb}{0.8,0.6,1.0}
		\definecolor{qqffqq}{rgb}{0.0,1.0,0.0}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{xfqqff}{rgb}{0.4980392156862745,0.0,1.0}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.3636363636363635cm,y=1.3636363636363635cm]
			\draw[->,color=black] (-1.2,0.0) -- (3.2,0.0);
			\foreach \x in {-1.0,-0.5,0.5,1.0,1.5,2.0,2.5,3.0}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0.0,-1.2) -- (0.0,3.2);
			\foreach \y in {-1.0,-0.5,0.5,1.0,1.5,2.0,2.5,3.0}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1.2,-1.2) rectangle (3.2,3.2);
			\draw[color=xfqqff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [0:3.14]; plot cos(x)};
			\draw[color=ffqqqq] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-1.2:3.2]; plot x};
			\draw[color=qqffqq] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-1:1]; plot acos(x)};
			\draw[dotted,color=cczzff] plot[raw gnuplot, id=func3] function{set samples 100; set xrange [-1.2:3.2]; plot 3.1415926535/2.0-x};
			\draw [dotted] (-1.0,0.0)-- (-1.0,3.141592653589793);
			\draw [dotted] (-1.0,3.141592653589793)-- (0.0,3.141592653589793);
			\draw [dotted] (0.0,1.0)-- (1.0,1.0);
			\draw [dotted] (1.0,0.0)-- (1.0,1.0);
			\draw [dotted] (0.0,-1.0)-- (3.141592653589793,-1.0);
			\draw [dotted] (3.141592653589793,0.0)-- (3.141592653589793,-1.0);
			\draw [->] (-1.0,3.141592653589793) -- (-1.0,2.6982051899765422);
			\draw [->] (0.0,1.0) -- (0.41398424427733616,1.0);
			\draw [->] (1.0,0.0) -- (1.0,0.41498464079523883);
			\draw [->] (3.141592653589793,-1.0) -- (2.697204793458636,-1.0);
		\end{tikzpicture}
	
		\
	
		\textbf{Figure 3.} $\arccos$ en vert, $\widetilde{\cos}$ en violet, la première bissectrice en rouge et $y = \frac{\pi}{2} - x$ en rose.
	\end{center}

\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arctan$.} 

	\
	
	Premièrement, ladite fonction est la bijection réciproque de la fonction $\tan |_{\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ }:=\widetilde{\tan}$. D'où : 
	\begin{center}
	
	$\arctan = \left\{  
	\begin{array}{c c c}
	\mathbb{R} & \to & \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \\ [1ex]
	x & \mapsto & \left( \widetilde{\tan} \right)^{-1}(x)
	\end{array} 
	\right.
	$
	\end{center}
	
	\
	
	Ainsi, pour $x\in \mathbb{R}$, $\arctan (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[$, $\tan(\theta) = x$. 
	
	\
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\tan}$ : 
	
	\begin{enumerate}
	    \item $\arctan$ est impaire.
	    \item $\arctan \in \mathcal{C}^0\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
	    \item $\arctan \in \mathcal{D}^1\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
	    \item $\arctan'(x) = \frac{1}{1+x^2}$ pour tout $x\in\mathbb{R}$.
	\end{enumerate}
	
	\newpage
	
	Graphe de $\arctan$ : 
	\begin{center}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{qqffqq}{rgb}{0.0,1.0,0.0}
		\definecolor{qqffff}{rgb}{0.0,1.0,1.0}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
			\draw[->,color=black] (-3.0,0.0) -- (3.0,0.0);
			\foreach \x in {-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.5,1.0,1.5,2.0,2.5}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0.0,-3.0) -- (0.0,3.0);
			\foreach \y in {-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.5,1.0,1.5,2.0,2.5}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-3.0,-3.0) rectangle (3.0,3.0);
			\draw[color=qqffff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.56:1.56]; plot tan(x)};
			\draw[color=qqffqq] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-3:3]; plot atan(x)};
			\draw[color=ffqqqq] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-3:3]; plot x};
			\draw [domain=-3.0:3.0] plot(\x,{(--1.5707963267948966-0.0*\x)/1.0});
			\draw (1.5707963267948966,-3.0) -- (1.5707963267948966,3.0);
			\draw [domain=-3.0:3.0] plot(\x,{(-1.5707963267948966-0.0*\x)/1.0});
			\draw (-1.5707963267948966,-3.0) -- (-1.5707963267948966,3.0);
			\draw [dotted] (0.7853981633974483,1.0)-- (0.7853981633974483,0.0);
			\draw [dotted] (0.0,0.7853981633974483)-- (1.0,0.7853981633974483);
			\draw [dotted] (0.0,1.0)-- (1.0,1.0);
			\draw [dotted] (1.0,0.0)-- (1.0,1.0);
			\draw [dotted] (-1.0,0.0)-- (-1.0,-1.0);
			\draw [dotted] (0.0,-1.0)-- (-1.0,-1.0);
			\draw [dotted] (-0.7853981633974483,0.0)-- (-0.7853981633974483,-1.0);
			\draw [dotted] (0.0,-0.7853981633974483)-- (-1.0,-0.7853981633974483);
			\begin{scriptsize}
				\draw[color=qqffff] (-3.1177254400173355,-0.014744648606977613) node {$f$};
				\draw[color=qqffqq] (-3.1177254400173355,-1.3072326284088318) node {};
				\draw[color=ffqqqq] (-1.8026940652189338,-1.8332451783281911) node {};
				\draw[color=black] (-3.1177254400173355,1.5106917461591645) node {$a$};
				\draw[color=black] (1.4660982092799508,2.322253966034747) node {};
				\draw[color=black] (-3.1177254400173355,-1.4500074633869435) node {$c$};
				\draw[color=black] (-1.4946010002661654,2.322253966034747) node {};
			\end{scriptsize}
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 4.} $\arctan$ en vert, $\widetilde{\tan}$ en bleu, la première bissectrice en rouge, et les fonctions $y = \pm \frac{\pi}{2}$ et $x = \pm \frac{\pi}{2}$ en noir.
	\end{center}
	
	\
	
	On a aussi (visible sur le graphe) : 
	\[
		\forall x \in \mathbb{R}_+, \quad \arctan(x) \leq x.
	\]
	
	Et enfin : 
	\[
		\forall x \in \mathbb{R}^*, \quad \arctan(x) + \arctan \left( \frac{1}{x} \right) = 
		\left\{ \begin{array}{cl}
		\frac{\pi}{2} & \text{si } x \ > \ 0 \\
		-\frac{\pi}{2} & \text{si } x \ < \ 0.
		\end{array} \right.
	\]

\end{question_kholle}

%-------------------------------------------------------------
\begin{question_kholle}{$2$ preuves de $\arcsin(x) + \arccos(x) =\frac{\pi}{2}$ sur $[-1,1]$, dont une basée sur une interprétation géométrique du cercle trigonométrique.} 

	L'interprétation géométrique sur $[0,1]$, celle sur $[-1,0]$ est laissée au lecteur car il s'agit du même principe modulo des détails : 
	
	\begin{center}
		\definecolor{eqbqff}{rgb}{0.8784313725490196,0.6901960784313725,1.0}
		\definecolor{xfqqff}{rgb}{0.4980392156862745,0.0,1.0}
		\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{qqffff}{rgb}{0.0,1.0,1.0}
		\definecolor{cqcqcq}{rgb}{0.7529411764705882,0.7529411764705882,0.7529411764705882}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=2.5cm,y=2.5cm]
			\clip(-1.2,-1.2) rectangle (1.2,1.2);
			\fill[fill=black,fill opacity=1.0] (0.08956860342780414,0.20707624336357297) -- (0.08145277478981197,0.18983010750783963) -- (0.0742401101509475,0.2039730262575521) -- cycle;
			\fill[fill=black,fill opacity=1.0] (0.3604343842207928,0.11272973544691409) -- (0.3441578360574464,0.12526320821437803) -- (0.33811585546631434,0.10562838538867093) -- cycle;
			\draw [color=cqcqcq] (0.0,0.0) circle (2.5cm);
			\draw [shift={(0.0,0.0)},line width=1.2000000000000002pt,color=qqffff]  plot[domain=0.0:3.141592653589793,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [shift={(3.061616997868383E-17,0.0)},line width=1.2000000000000002pt,color=ffqqqq]  plot[domain=-1.5707963267948966:1.5707963267948963,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [shift={(0.0,-0.0)},line width=1.2000000000000002pt,color=xfqqff]  plot[domain=0.0:1.5707963267948966,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [line width=1.2000000000000002pt,color=qqffff] (-1.0589461703485412,0.0)-- (-0.9410538296514588,0.0);
			\draw [line width=1.2000000000000002pt,color=qqffff] (1.0589461703485412,0.0)-- (0.9410538296514588,0.0);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,1.0589461703485412)-- (5.762292801540088E-17,0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (-1.945252556980033E-16,-1.0589461703485412)-- (-1.7286878404620264E-16,-0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,1.0589461703485412)-- (0.05398708109469719,1.0589461703485412);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (5.762292801540088E-17,0.9410538296514588)-- (0.05398708109469719,0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,-1.0589461703485412)-- (0.05398708109469719,-1.0589461703485412);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (5.762292801540088E-17,-0.9410538296514588)-- (0.05398708109469719,-0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=qqffff] (-1.0589461703485412,1.2968350379866888E-16)-- (-1.0589461703485412,0.05398708109469725);
			\draw [line width=1.2000000000000002pt,color=qqffff] (-0.9410538296514588,1.1524585603080177E-16)-- (-0.9410538296514588,0.05398708109469724);
			\draw [line width=1.2000000000000002pt,color=qqffff] (0.9410538296514588,1.1524585603080177E-16)-- (0.9410538296514588,0.05398708109469724);
			\draw [line width=1.2000000000000002pt,color=qqffff] (1.0589461703485412,1.2968350379866888E-16)-- (1.0589461703485412,0.05398708109469725);
			\draw [dash pattern=on 1pt off 1pt on 3pt off 4pt] (6.123233995736766E-17,1.0)-- (0.0,-1.0);
			\draw [dash pattern=on 1pt off 1pt on 3pt off 4pt] (-1.0,0.0)-- (1.0,-0.0);
			\draw[color=eqbqff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.0999999999999999:1.0999999999999999]; plot x};
			\draw (0.3420201433256688,0.9396926207859083)-- (0.0,-0.0);
			\draw (0.0,-0.0)-- (0.9396926207859084,0.3420201433256687);
			\draw [shift={(0.0,-0.0)}] plot[domain=0.0:0.3490658503988659,variable=\t]({1.0*0.36624511935574344*cos(\t r)+-0.0*0.36624511935574344*sin(\t r)},{0.0*0.36624511935574344*cos(\t r)+1.0*0.36624511935574344*sin(\t r)});
			\draw [shift={(0.0,-0.0)}] plot[domain=0.0:1.2217304763960306,variable=\t]({1.0*0.21706356072793254*cos(\t r)+-0.0*0.21706356072793254*sin(\t r)},{0.0*0.21706356072793254*cos(\t r)+1.0*0.21706356072793254*sin(\t r)});
			\draw (0.08956860342780414,0.20707624336357297)-- (0.08145277478981197,0.18983010750783963);
			\draw (0.08145277478981197,0.18983010750783963)-- (0.0742401101509475,0.2039730262575521);
			\draw (0.0742401101509475,0.2039730262575521)-- (0.08956860342780414,0.20707624336357297);
			\draw (0.3604343842207928,0.11272973544691409)-- (0.3441578360574464,0.12526320821437803);
			\draw (0.3441578360574464,0.12526320821437803)-- (0.33811585546631434,0.10562838538867093);
			\draw (0.33811585546631434,0.10562838538867093)-- (0.3604343842207928,0.11272973544691409);
			\draw [dash pattern=on 3pt off 3pt] (5.938595898438009E-17,0.9396926207859083)-- (0.3420201433256688,0.9396926207859083);
			\draw [dash pattern=on 3pt off 3pt] (4.108751682287631E-17,0.34202014332566877)-- (0.9396926207859084,0.3420201433256687);
			\draw [dash pattern=on 3pt off 3pt] (0.3420201433256688,0.9396926207859083)-- (0.3420201433256688,-0.0);
			\draw [dash pattern=on 3pt off 3pt] (0.9396926207859084,0.3420201433256687)-- (0.9396926207859084,-0.0);
			\draw (0.3106196735830377,0.20690669566269085) node[anchor=north west] {arccos(x)};
			\draw (0.06201603682796799,0.3536235960427321) node[anchor=north west] {arcsin(x)};
			\draw (-0.18658759992710172,0.9853213615679096) node[anchor=north west] {x};
			\draw (0.8241288249131816,0.0031332229126335774) node[anchor=north west] {x};
			\begin{scriptsize}
				\draw [fill=uuuuuu] (0.0,-0.0) circle (1.5pt);
				\draw[color=uuuuuu] (-0.011342413362052578,0.09279355092265877) node {$\Omega$};
				\draw [fill=uuuuuu] (5.938595898438009E-17,0.9396926207859083) circle (1.5pt);
				\draw [fill=uuuuuu] (0.9396926207859084,-0.0) circle (1.5pt);
			\end{scriptsize}
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 5.}
	\end{center}
	
	\
	
	Preuve formelle : 
	
	\
	
	Soit $x\in [-1,1]$. Posons  $\varphi \ = \ \arcsin(x) \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Ainsi : 
	
	\[
		\arcsin(x) + \arccos(x) \ = \ \varphi + \arccos(\sin(\varphi)) \ = \ \varphi + \arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right),
	\]
	
	\
	
	or $\varphi \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ donc
	$\frac{\pi}{2}- \varphi \in [0,\pi]$ d'où $\arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right) = \frac{\pi}{2}- \varphi$ si bien que : 
	\[
		\arcsin(x) + \arccos(x) \ = \  \varphi +\frac{\pi}{2} - \varphi \ = \ \frac{\pi}{2}.
	\]

\end{question_kholle}

\begin{question_kholle}{Présentation analytique rapide des fonctions \(\cosh\) et \(\sinh \).}
	~\smallbreak
	
	\begin{itemize}[label=$\bullet$]
		\item Domaine de définition et symétries.
		\newline
		$\sinh$ et $\cosh$ sont définies sur $\mathbb{R}$. 
		\newline
		De plus, 
		\newline
		$(i)$ $\forall x \in \mathbb{R}$, $-x\in \mathbb{R}$, 
		\newline
		$(ii)$ $\forall x \in \mathbb{R}$, 
		$
			\left\{ \begin{array}{c c c c c c c}
			\sinh (-x) & = & \frac{e^{-x} - e^{x}}{2} & = & - \frac{e^x - e^{-x}}{2} & = & -\sinh(x) \\
			\text{et} & & & & & & \\ 
			\cosh (-x) & = & \frac{e^{-x} + e^{-(-x)}}{2} & = &  \frac{e^x + e^{-x}}{2} & = & \cosh(x).
			\end{array} 
			\right.
		$
		\newline
		Donc $\sinh$ et $\cosh$ sont respectivement impaire et paire.
		\newline
		Nous les étudierons sur $\mathbb{R}_+$ et pour les obtenir les graphes $(\mathcal{C}_{\sinh} \text{ et } \mathcal{C}_{\cosh})$ de ces fonctions sur $\mathbb{R}$ à partir de ceux $(\mathcal{C}_{\sinh}^+ \text{ et } \mathcal{C}_{\cosh}^+)$ obtenus sur $\mathbb{R}_+$, nous le complèterons en traçant les images de ces graphes par la symétrie centrale $s$ de centre $O$ et par la réflexion $r$ d'axe $\left( O, \overrightarrow{\jmath} \right)$ : 
		\[
			\mathcal{C}_{\sinh} = \mathcal{C}_{\sinh}^+ \cup s \left( \mathcal{C}_{\sinh}^+ \right) \qquad \text{ et } \qquad \mathcal{C}_{\cosh} = \mathcal{C}_{\cosh}^+ \cup r \left( \mathcal{C}_{\cosh}^+ \right)
		\]
		
		\
		
		\item Variations : triviales.
		
		\
		
		\item Branches infinies en $+\infty$ et position relative de $\mathcal{C}_{\sinh}$ et $\mathcal{C}_{\cosh}$.
		
		\[
			\frac{\cosh(x)}{x} = \underset{\xrightarrow[x\to +\infty]{} \ +\infty}{\underbrace{\frac{e^{x}}{x}}} + \underset{\xrightarrow[x\to +\infty]{} \ 0}{\underbrace{\frac{e^{-x}}{x}}} \xrightarrow[x\to +\infty]{} \ +\infty
		\]
		Donc le graphe de $\cosh$ admet une branche parabolique de direction asymptotique $\left( O, \overrightarrow{\jmath}\right)$.
		\newline
		On a : 
		\[
			\forall x \in \mathbb{R}, \quad \cosh(x) - \sinh(x) = e^{-x} \xrightarrow[x\to +\infty]{} 0^+
		\]
		Donc les graphes des deux fonctions se rapprochent l'un de l'autre arbitrairement près lorsque $x \to +\infty$, et le graphe de $\cosh$ est au-dessus de celui de $\sinh$.
		
		\
		
		\item Tangente au graphe de $\sinh$ à l'origine et position relative.
	\end{itemize}

	Il s'agira d'étudier $g : x\in \mathbb{R}_+ \mapsto \sinh(x) -x$, de remarquer sa dérivabilité d'en étudier les variations puis de conclure, en précisant que cette étude révèle l'inflexion du graphe de $\sinh$ en 0.
\end{question_kholle}

\pagebreak\section{Semaine 7}

\begin{question_kholle}{Calcul de $\int_0^{2\pi}e^{imt}dt$ en fonction de $m \in \Z$. En Déduire qu'une fonction polynomiale nulle sur un cercle centré en l'origine a tous ses coefficients nuls.}
	Soit $m \in \Z$ fq. Calculons:
	$$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt}dt$$
	Si $m \neq 0$:
	\begin{align*}
		\frac{1}{2 \pi} \int_0^{2\pi}e^{imt}dt &= \frac{1}{2 \pi} \Big[ \frac{e^{mt}}{im} \Big]_0^{2\pi}\\
		&= \frac{1}{2 \pi} \Big( \frac{1}{im} - \frac{1}{im} \Big) = 0
	\end{align*}
	Si $m = 0$:
	$$
		\frac{1}{2 \pi} \int_0^{2\pi}e^{imt}dt = \frac{1}{2 \pi} \int_0^{2\pi}dt = \frac{2 \pi}{2 \pi} = 1
	$$
	\\
	Donc $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt}dt = 
	\begin{cases}
		1 \text{ si } m=0\\
		0 \text{ si } m \neq 0
	\end{cases}
	$$
	\\
	Soit $n\in \N$ fq
	
	Soient $(a_0, ..., a_n) \in \C^{n+1}$ les coefficients de $P(z) = \sum_{k=0}^n a_k z^k$, et $s\in \Z$, et $r \in \R_+^*$ fq. tels que P soit nulle lorsqu'elle est évaluée sur $\mathscr C(0,r)$
	\begin{align*}
		\frac{1}{2 \pi} \int_0^{2\pi} P(re^{it}) e^{-imt}dt &= \frac{1}{2 \pi} \int_0^{2\pi} \bigg (\sum_{k=0}^n a_k (re^{it})^k \bigg) e^{-imt}dt\\
		&= \sum_{k=0}^n a_k r^k \underbrace{\int_0^{2\pi} \frac{e^{it(k-s)}}{2 \pi} dt}_{I_k}
	\end{align*}
	On remarque que:
	\begin{itemize}
		\item Si $s \notin [[0, n]], \{k \in [[0, n]] \text{ }| \text{ } k = s\}$ = $\emptyset$, Donc $$\sum_{k \in [[0, n]]} a_k s^k I_k  = \sum_{\substack{k \in [[0, n]] \\ k = s}} a_k r^k  = 0$$
		\item Si $s \in [[0, n]], \{k \in [[0, n]] \text{ }| \text{ } k = s\}$ = ${s}$, Donc $$\sum_{k \in [[0, n]]} a_k s^k I_k = \sum_{\substack{k \in [[0, n]] \\ k = s}} a_k s^k = a_s r^s \label{1}$$
	\end{itemize}
	Or, puisque $P$ s'annule sur le cercle de rayon $r$ et de centre $0$,  $\mathscr C(0,r)$, ces sommes sont aussi nulles. On en déduit, en particularisant pour un $s \in [[0, n]]$ fixé quelconque que:
	$$
		\sum_{k \in [[0, n]]} a_k s^k I_k = a_sr^s = 0 \implies a_s = 0
	$$
	
	Donc $$
		(\exists r \in \R_+^* : \forall \theta \in \R, P(re^{i\theta})=0) \implies \forall s \in [[0, n]]
	$$
	\\
	Pour la preuve réciproque,  soit $n \in \N$ fq. Soient $(a_0,...,a_n) \in \{ 0 \} ^{n+1}$ les coefficients nuls de la fonction polynomiale $P \in \C[z]$ définie pour tout $z \in \C$.
	
	En remarquant que $\forall z \in \C , P(z) = 0$, puisque n'importe quel cercle centré en 0 est un sous ensemble de $\C$,  $\exists r \in \R_+^*: \forall z \in \mathscr C(0, r), P(z) = 0$.
\end{question_kholle}

\begin{question_kholle}{Preuve de la Linéarité de la dérivation d'une fonction complexe}
	Définissons les fonctions $f_r$ et $f_i$ comme les parties réelles et imaginaires de $f$.

	Soient $(f, g) \in \mathcal{F}(I, \C)^2$, $(\alpha, \beta) \in \C^2$ fixés quelconques.
	\begin{align*}
		f_r = \Re(f) &, f_i = \Im(f) &g_r = \Re(f) &, g_i = \Im(g)\\
		\alpha_r = \Re(\alpha) &, \alpha_i = \Im(f) &\beta_r = \Re(f) &, \beta_i = \Im(g)
	\end{align*}
	
	\begin{align*}
		\Re( \alpha f + \beta g) &= \Re((\alpha_r + i \alpha_i)(f_r + i f_i) + (\beta_r+ i\beta_i)(g_r+ i g_i)) \\
		&= \underbrace{\alpha_r f_r + \beta_r g_r - \alpha_i f_i - \beta_i g_i}_{\text{Combinaison linéaire de } \underbrace{(f_r, f_i, g_r, g_i) \in \mathcal D^1(I, \R)^4}_{car (f,g) \in D^1(I, \R)^2}}
	\end{align*}
	Donc, selon le théorème de stabilité par combinaison linéaire des fonctions à valeurs réelles, $\Re(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\Re(\alpha f + \beta g)\big)' = \alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i'$
	\\
	On montre de même que $\Im(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\alpha f + \beta g\big)' = \alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r'$
	
	Ainsi,
	\begin{align*}
		\big( \alpha f + \beta g \big)' &= (\alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i') + i (\alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r') \\
		&= \alpha_r(f_r' + if_i') + \beta_r(g_r' + ig_i') + \alpha_i \underbrace{(-f_i' + if_r')}_{i(f_r' + if_i')} + \beta_i \underbrace{( -g_i' + ig_r')}_{i(g_r' + ig_i')} \\
		&=\alpha f' + \beta g'
	\end{align*}
\end{question_kholle}

\begin{question_kholle}{Dérivée composée d'une fonction à valeurs complexes}
	Soient $f \in \mathcal D ^1(J, \C) $ et $h \in \mathcal D^1(I, J)$ (I et J sont deux intervalles réels) fixés quelconques. Notons $f_r$ et $f_i$ respectivement la partie réelle et imaginaire de $f$.
	
	\begin{align*}
		\left .
		\begin{array}{ll}
			h \in \mathcal D^1(I, J) \\
			f_r \in \mathcal D^1(J, \R) \text{, car } f \in \mathcal D^1(J, \C)
		\end{array}
		\right \}
		\implies f_r \circ h \in \mathcal D^1(I, \R)
	\end{align*}
	
	On montre de même que $f_i \circ h \in \mathcal D^1(I, \mathbb  R)$ donc $f \circ h \in \mathcal D^1(I, \C)$.
	
	De plus,
	
	\begin{align*}
		(f \circ h)' &= (f_r \circ h)' + i (f_i \circ h)' \\
		&= (f_r' \circ h ) \times h' + i((f_i' \circ h) \times h') \\
		&=(f_r' \circ h + if_i' \circ h) \times h' \\
		&= (f' \circ h) \times h'
	\end{align*}
\end{question_kholle}

\begin{question_kholle}{Caractérisation des fonctions dérivables de dérivée nulle sur un intervalle}
	Soit $f \in \mathcal D ^1 (I, \C)$ où $I$ est un intervalle réel;
	Posons $f_r = \Re (f)$ et $f_i = \Im(f)$.
	
	\begin{align*}
	\forall t \in I, f'(t) = 0 &\iff \forall t \in I, f_r'(t) + i f_i'(t) = 0 \\
	&\iff \begin{cases}
		\forall t \in I, f_r'(t) = 0 \\
		\forall t \in I, f_i'(t) = 0
	\end{cases} \\
	&\iff \begin{cases}
		\exists \lambda_r \in \R : \forall t \in I,  f_r(t) = \lambda_r \\
			\exists \lambda_i \in \R : \forall t \in I,  f_i(t) = \lambda_i
	\end{cases} \\
	&\iff \exists \lambda \in \C : \forall t \in I,  f(t) = \lambda
	\end{align*}
\end{question_kholle}

\pagebreak\section{Semaine 8}
	
	\flushleft
	
	\begin{question_kholle}{Preuve de l’expression des solutions réelles des EDL homogènes d’ordre 2 à coefficients constants réels dans le cas $\Delta < 0$ (en admettant la connaissance de l’expression des solutions à valeurs complexes des EDLH2 à coeff. constants).}
		Notons $\Sol_{H, \C}$ et $\Sol_{H, \R}$ les ensembles des solutions complexes et réelles de l'équation différentielle, puisque nous nous plaçons dans le cas $\Delta < 0$ et $\alpha \pm i \beta$ les deux racines complexes conjuguées.
		$$
		\Sol_{H, \C} = 
		\left\{
		\begin{array}{l}
	    \R \to \C  \\
	    t \mapsto \lambda e^{(\alpha + i \beta) t}  + \mu e^{(\alpha - i \beta)t}
    	\end{array}
		\middle\vert  (\lambda, \mu) \in \C ^2 \right\}	
		$$
		
		Montrons que $\forall f \in \Sol_{H ,\C}, \Re(f) \in  \Sol_{H ,\R}$\\
		Soit $f \in \Sol_{H ,\C}$ fq.
		$$f \in \mathcal D^2(\R, \C) \implies \Re(f) \in \mathcal D^2(\R, \R)$$
		Et, de plus, par morphisme additif de \Re
		$$
		a_2\Re(f)'' + a_1\Re(f)' + a_0\Re(f) = \Re( a_2 f'' + a_1 f' + a_0 f) = 0
		$$
		D'où, avec $f:t \mapsto e^{(\alpha + i \beta)t}$; $\Re(f(t)) = \Re(e^{(\alpha + i \beta)t}) = e^{\alpha t } \cos (\beta t)$. Qui appartient donc à $\Sol_{H, \R}$\\
		En suivant le même raisonnement pour $\Im(f)$, $(t \mapsto e^\alpha \sin(\beta t)) \in \Sol_{H, \R}$
		
		
		Ainsi, par combinaison linéaire (qui se base sur le principe de superposition),
		$$
		\left\{ 
		\begin{array}{l}
	    \R \to \R  \\
	    t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
	  	\end{array}
		\middle\vert  (\lambda, \mu) \in \R ^2 \right\}
		\subset \Sol_{H ,\R}
		$$
		
		Réciproquement, soit $ f \in \Sol_{H ,\R}$ fq. Puisque $\R \subset \C$,  $ f \in \Sol_{H ,\C}$.
		
		$$
		\exists (a, b) \in \C^2 : f \left| \begin{array}{l}
	    \R \to \C  \\
	    t \mapsto a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t}
		\end{array}\right.$$
	
		Or, puisque toutes les valeurs de $f$ sont réelles, en notant $(a_r, a_i, b_r, b_i)$ les parties réelles et imaginaires respectives de $a$ et $b$.
		\begin{align*}
			\forall t \in \R, f(t) &= \Re(f(t)) \\
					&= \Re(a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t})\\
					&= \Re((a_r + i a_i) e^{(\alpha + i \beta) t}  + (b_r + i b_i) e^{(\alpha - i \beta)t})\\
				    &= a_r \cos(\beta t)e^\alpha - a_i\sin(\beta t)e^\alpha + b_r \cos(\beta t)e^\alpha + b_i \sin(\beta t) e^\alpha \\
				    &= (a_r + b_r) \cos(\beta t) e^\alpha + (b_i - a_i) \sin(\beta t) e^\alpha
		\end{align*}
		Ainsi,
		$$f\in \left\{ 
		\begin{array}{l}
	    \R \to \R  \\
	    t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
	  \end{array}
		\middle\vert  (\lambda, \mu) \in \R ^2 \right\}
	$$
	Ce qui conclut la preuve par double inclusion.
	\end{question_kholle}
	
	\begin{question_kholle}[
		Considérons le problème de Cauchy suivant :
		$$\left\{ \begin{array}{l}
			a_{2}y''+a_{1}y'+a_{0}y = b \text{ sur } J  \\
			y(t_{0}) = \alpha_{0} \\
			y'(t_{0}) = \alpha_{1}
		\end{array} \right. \text{ où } (\alpha_{0}, \alpha_{1}) \in \mathbb{K}^{2}, t_{0} \in J, (a_{0}, a_{1}, a_{2}) \in \mathbb{K}^{2} \times \mathbb{K}^{*}, b \in \mathcal{F}(J, \mathbb{K})$$
		Si $b$ est continu sur $J$, alors ce problème de Cauchy admet une unique solution définie sur $J$.
		]
		{Existence et unicité d'une solution au problème de Cauchy pour les EDL d'ordre 2 à coefficients constants et second membre continu sur $I$ (cas complexe puis cas réel).}
	 
	\textbf{Cas 1. } $\mathbb{K} = \mathbb{C}$ \\
	Nous savons que sous l'hyphothèse de continuité de $b$ sur $J$, les solutions de (EDL2) définies sur $J$ constituent le plan affine $S$ :
	$$S = \left\{ \lambda f_{1} + \mu f_{2} + s | (\lambda, \mu) \in \mathbb{C}^{2} \right\}$$
	où $s$ est une solution particulière de (EDL2), $(f_{1}, f_{2})$ sont deux solutions de (EDLH2) qui engendrent $S_{h}$. On a : \\
	
	$$\begin{array}{ccl}
	    f : J \to \mathbb{C} \text{ est sol. du pb de Cauchy } 
	    &\iff &\left\{ \begin{array}{l}
	      f \text{ sol de (EDL2) sur } J    \\
	      f(t_{0}) = \alpha_{0}    \\
	      f'(t_{0}) = \alpha_{1}
	    \end{array}  \right. \\\\
	    &\iff &\left\{ \begin{array}{l}
	      f \in S    \\
	      f(t_{0}) = \alpha_{0}    \\
	      f'(t_{0}) = \alpha_{1}
	    \end{array}\right. \\\\
	    &\iff &\exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
	      f = \lambda f_{1} + \mu f_{2} + s \\
	      \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) + s(t_{0}) = \alpha_{0} \\
	      \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) + s'(t_{0}) = \alpha_{1} \\
	    \end{array} \right. \\\\
	    &\iff &\exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
	      f = \lambda f_{1} + \mu f_{2} + s \\
	      \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) = \alpha_{0} - s(t_{0}) \\
	      \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) = \alpha_{1} - s'(t_{0}) \\
	    \end{array} \right. \\\\
	\end{array} $$
	On en déduit donc que $(\lambda, \mu)$ doit être solution d'un système linéaire $(2,2)$. On a une unique solution si et seulement si les déterminant de ce système est nul. \\
	Explicitons alors le déterminant de ce système, que l'on notera $D$.
	$$D = \left| 
	\begin{array}{cc}
	f_{1}(t_{0}) &f_{2}(t_{0}) \\
	f'_{1}(t_{0}) &f'_{2}(t_{0}) \\
	\end{array}
	\right| = f_{1}(t_{0}) \cdot f'_{2}(t_{0}) - f_{2}(t_{0}) \cdot f'_{1}(t_{0}) $$
	Notons $\Delta$ le discriminant de l'équation caractéristique de (EDL2) ($a_{2}r^{2} + a_{1}r^{1} + a_{0} = 0$). On distingue alors deux cas selon la nullité ou non de $\Delta$. Traitons d'abord le cas $\Delta \neq 0$. On peut choisir : 
	$$ f_{1}(t_{0}) = e^{r_{1}t_{0}} \text{ et } f_{2}(t_{0}) = e^{r_{2}t_{0}}$$
	$$ f'_{1}(t_{0}) = r_{1}e^{r_{1}t_{0}} \text{ et } f'_{2}(t_{0}) = r_{2}e^{r_{2}t_{0}}$$
	Donc (en sachant que $\Delta \neq 0 \Rightarrow r_{1} \neq r_{2}$):
	$$ D = e^{r_{1}t_{0}} \cdot r_{2}e^{r_{2}t_{0}} - r_{1}e^{r_{1}t_{0}} \cdot e^{r_{2}t_{0}} = (r_{2} - r_{1}) \cdot e^{r_{1}t_{0} + r_{2}t_{0}} \neq 0$$
	
	Dans le deuxième cas, on a $\Delta = 0$ ; on peut alors prendre :
	$$ f_{1}(t_{0}) = e^{r_{0}t_{0}} \text{ et } f_{2}(t_{0}) = t_{0}e^{r_{0}t_{0}}$$
	Ainsi : 
	$$ D = e^{r_{0}t_{0}} \left(r_{0}t_{0}e^{r_{0}t_{0}} + e^{r_{0}t_{0}} \right) - r_{0}e^{r_{0}t_{0}} \times t_{0}e^{r_{0}t_{0}} = e^{2r_{0}t_{0}} \neq 0$$
	On remarque alors que, dans les deux cas, $D \neq 0$, donc le système $(2, 2)$ étudié admet une unique solution, donc il existe un unique couple $(\lambda, \mu)$ le vérifiant d'où l'unicité et existence d'une solution au problème de Cauchy. 
	\newline\newline
	
	\textbf{Cas 2. } $\mathbb{K} = \mathbb{R}$ \\
	$(a_{0}, a_{1}, a_{2}) \in \mathbb{R}^{2} \times \mathbb{R}^{*},(\alpha_{0}, \alpha_{1}) \in \mathbb{R}^{2}, b \in C^{0}(J, \mathbb{R})$ 
	\newline
	\textbf{Existence :} Puisque $\mathbb{R} \subset \mathbb{C}$, le problème de Cauchy admet, dans $\mathbb{R}$, une solution à valeurs complexes $g$. Posons $f = \Re(g)$ et montrons que $f$ est une solution réelle du problème de Cauchy. \\
	\begin{itemize}
	    \item[$\star$] $g \in \mathcal{D}^{2}(J, \mathbb{C}) \text{ donc } f \in \mathcal{D}^{2}(J, \mathbb{R})$
	    \item[$\star$] $g$ vérifie $a_{2}g'' + a_{1}g' + a_{0}g = b$ sur $J$ donc en prenant $\Re(\cdot)$ : 
	    $$\begin{array}{ccl}
	      \Re(a_{2}g'' + a_{1}g' + a_{0}g = b) = \Re(b)   
	      &\iff &a_{2}\Re(g'') + a_{1}\Re(g') + a_{0}\Re(g) = b  \\\\
	      &\iff & a_{2}f'' + a_{1}f' + a_{0}f = b \text{ sur } J
	    \end{array}$$
	    \item[$\star$] $f(t_{0}) = \Re(g(t_{0})) = \Re(\alpha_{0}) = \alpha_{0}$
	    \item[$\star$] $f'(t_{0}) = \Re(g(t_{0}))' = \Re(g'(t_{0})) = \Re(\alpha_{1}) = \alpha_{1}$
	\end{itemize}
	Donc $f$ est une solution réelle définie sur $J$ au problème de Cauchy. 
	\newline
	
	\textbf{Unicité : }Soient $f_{1}$ et $f_{2}$ deux fonctions à valeurs réelles solutions du problème de Cauchy ci-dessus fixées quelconques : puisque $\mathbb{R} \subset \mathbb{C}$, $f_{1}$ et $f_{2}$ sont des fonctions à valeurs dans $\mathbb{C}$ solutions du même problème de Cauchy; or il y a unicité de la solution au problème de Cauchy dans les fonctions à valeurs complexes, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{C})$, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{R})$.
	\end{question_kholle}
	
	\begin{question_kholle}[
		Soient $(a,b)\in \mathbb{C}^2$, $f$ et $g$ les  solutions, définies sur $\mathbb{R}$ à valeurs
		dans $\mathbb{C}$, des problèmes de Cauchy suivants :
		\[
		    \left\{ \begin{array}{cl}
		        y'' +ay'+by = 0 \\
		        y(3) = 1\\
		        y'(3) = 0
		        \end{array} \right.        
			\quad \text{et} \quad
		    \left\{ \begin{array}{cl}
		        y'' +ay'+by = 0 \\
		        y(3) = 0\\
		        y'(3) = 1
		        \end{array} \right.
		\]
		
		Comment s'exprime la solution définie sur $\mathbb{R}$ de $\left\{ \begin{array}{cl}
		    y'' +ay'+by = 0 \\
		    y(3) = \alpha \\
		    y'(3) = \beta
		    \end{array} \right. $ pour $(\alpha, \beta)\in \mathbb{R}^2$ fixés ? 
		
		Peut-on affirmer que le plan vectoriel des solutions définies sur $\mathbb{R}$ à valeurs dans 
		$\mathbb{C}$ de $y'' + ay' + by = 0$ est $\{ \lambda \cdot f + \mu \cdot g  | 
		(\lambda, \mu)\in \mathbb{C}^2\}$
		]
		{Les solutions d'une EDL$_2$ constituent un espace vectoriel.}
	
	    La solution s'exprime simplement comme combinaison linéaire de f et g, plus précisément, la 
	    combinaison linéaire en $\alpha$ et $\beta$. En effet, soient de tels scalaires, et soient $f$ et 
	    $g$ de telles solutions, on a : 
	    \[
	        (\alpha \cdot f + \beta \cdot g)'' + a (\alpha \cdot f + \beta \cdot g)' + b (\alpha \cdot f + 
	        \beta \cdot g) = 0 \text{, par définition des espaces vectoriels.}
	    \]
	    Et de même, $(\alpha \cdot f + \beta \cdot g)'(3) = \alpha \cdot f'(3) + \beta \cdot g'(3) = \alpha$,
	    et $(\alpha \cdot f + \beta \cdot g)''(3) = \alpha \cdot f''(3) + \beta \cdot g''(3) = \beta$.
	    \newline
	    Ce qui suffit par unicité des solutions ( de la donc) d'un problème de Cauchy dans le cadre du 
	    théorème du cours.
	    \newline
	    Pour ce qui est du plan vectoriel des solutions, noté $\Omega$, notons aussi $\Phi$ l'ensemble proposé.
	    L'inclusion $\Phi \subset \Omega$ est triviale par propriété de linéarité des espaces vectoriels.
	    Finalement, pour $\Omega \subset \Phi$, soit $\omega \in \Omega$, forcément, $\omega$ vérifie 
	    l'$EDL_2$, mais aussi des conditions de Cauchy bien que celles-ci soient non-spécifiées, ainsi
	    posons $\omega'(3) = \delta$ et $\omega''(3) = \theta$, donc en particulier, $ \omega = 
	    \delta \cdot f + \theta \cdot g$, d'où l'égalité par double inclusion.
	\end{question_kholle}

	\begin{question_kholle}
		[
		Résolution générale des systèmes linéaires à 2 équations et 2 inconnues en fonction du déterminant du systèmes (\textbf{tous les cas ne sont pas nécessairement à envisager})
		
		Considérons le système linéaire à deux équations et à deux inconnues $(x,y)$ :
		\begin{equation}
			(S)
			\left\{
				\begin{matrix}
					ax + by = b_1 &(E_1) \\
					cx + dy = b_2 &(E_2)
				\end{matrix}
			\right.
		\end{equation}
		dont $(a,b,c,d) \in \K^4$ sont les coefficients et $(b_1,b_2) \in \K^2$ sont les seconds membres.
		
		\begin{enumerate}
			\item (S) admet une unique solution si et seulement si
			$\begin{vmatrix}
				a & b \\
				c & d
			\end{vmatrix}
			= ad - bc \neq 0$. De plus, dans ce cas, la solution est
			\begin{equation}
				\left(
					\frac
						{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}
						{\begin{vmatrix}a&b\\c&d\end{vmatrix}},
					\frac
						{\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}
						{\begin{vmatrix}a&b\\c&d\end{vmatrix}}
				\right)
			\end{equation}
			\item Si $ad - bc = 0$, alors l'ensemble des solutions est soit vide, soit une droite affine de $\K^2$, soit $\K^2$.
		\end{enumerate}
		]
		{Formules de Cramer pour les systèmes 2 $\times$ 2}
		Procédons par disjonction de cas.
		
		\begin{itemize}[label=$\bullet$ Supposons]
			\item que $ad - bc \neq 0$.
			\begin{itemize}[label=$\bullet$ Supposons]
				\item que $a \neq 0$.
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{
							\begin{array}{cccccc}
								ax &+& by &=& b_1 \\
								&&\left(d - \frac{bc}{a}\right)y &=& b_2 - \frac{c}{a} b_1 &(L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
							\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{cccccc}
							ax &+& by &=& b_1 \\
							&&\left(ad - bc\right)y &=& a b_2 - c b_1 &(L_1 \leftarrow aL_1) \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccc}
							ax &=& \frac{1}{a} \left(b_1 - b\frac{ab_2 - cb_1}{ad - bc}\right) = \frac{1}{a} \frac{adb_1 - bcb_1 + abb_2 - bcb_2}{ad - bc} \\
							y &=& \frac{ab_2 - cb_1}{ad - bc} \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccccc}
							ax &=& \frac{db_1 - bb_2}{ad - bc} &=& \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}} \\
							y &=& \frac{ab_2 - cb_1}{ad - bc} &=& \frac{\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}}\\
						\end{array}
						\right.
					\end{aligned}
				\end{equation*}
				Donc le système admet une unique solution qui est celle annoncée.
				
				\item que a = 0. L'hypothèse $ad - bc \neq 0$ implique $bc \neq 0$ donc $b \neq 0$ et $c \neq 0$.
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{
						\begin{array}{ccccc}
							&& by &=& b_1 \\
							cx &+& dy &=&  b_2 \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccc}
							x &=&  \frac{1}{c} \left( b_2 - d\frac{b_1}{b} \right) \\
							y &=& \frac{b_1}{b} \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccccc}
							ax &=& \frac{db_1 - bb_2}{- bc} &=& \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}} \\
							y &=& \frac{- cb_1}{- bc} &=& \frac{\begin{vmatrix}0&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}}\\
						\end{array}
						\right.
					\end{aligned}
				\end{equation*}
			\end{itemize}
			Donc le système admet une unique solution qui est celle annoncée.
		\end{itemize}
	
		\item $ad - bc = 0$.
		\begin{itemize}[label=$\bullet$ Supposons]
			\item $a \neq 0$. En reprenant la méthode pivot de Gauss,
			\begin{equation*}
				\begin{aligned}
					(S)
					&\iff \left\{
					\begin{array}{cccccc}
						ax &+& by &=& b_1 \\
						&&\left(d - \frac{bc}{a}\right)y &=& b_2 - \frac{c}{a} b_1 &(L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
					\end{array}
					\right. \\
					&\iff \left\{
					\begin{array}{cccccc}
						ax &+& by &=& b_1 \\
						&& \underbrace{\left(ad - bc\right)}_0 y &=& a b_2 - c b_1 &(L_1 \leftarrow aL_1) \\
					\end{array}
					\right. \\
				\end{aligned}
			\end{equation*}
			Donc le système est de rang 1 avec une condition de compatibilité. \\
			Si $ab_2 - cb_1 \neq 0$, (S) n'admet aucune solution. \\
			Sinon $ab_2 - cb_1 = 0$
			\begin{equation}
				(S) \iff
				ax + by = b_1 \iff
				\begin{pmatrix} x \\ y \end{pmatrix} \in \left\{
					\begin{pmatrix} \frac{b_1}{a} - b\frac{t}{a} \\ t \end{pmatrix}
					|\; t \in \K
				\right\}
			\end{equation}
			Donc (S) admet un droite affine de solutions.
			
			\item $a = 0$. Puisque $ad - bc = 0$, alors $bc = 0$ donc b ou c est nul.
			
			\begin{itemize}[label=$\bullet$ Si]
				\item $c = 0$,
				\begin{equation*}
					(S) \iff
					\left\{ \begin{array}{ccc}
							by &=& b_1 \\
							dy &=& b_2
					\end{array} \right.
				\end{equation*}
				
				\begin{itemize}[label=$\bullet$ Si]
					\item $b = 0$, 
					\begin{equation*}
						(S) \iff
						\left\{ \begin{array}{ccc}
							by &=& b_1 \\
							0 &=& b_2
						\end{array} \right.
					\end{equation*}
					\begin{itemize}[label=$\bullet$ Si]
						\item $b_2 = 0$, (S) n'admet aucune solution.
						\item $b_2 \neq 0$, $(S) \iff dy = b_2$
							\subitem$\bullet$ Si $d = 0$, $(S) \iff 0 = b_2$. (S) n'admet aucune solution ($b_2 \neq 0$) ou admet $\K^2$ comme ensemble des solutions ($b_2 = 0$).
							\subitem$\bullet$ Si $d \neq 0$, $(S) \iff y = \frac{b_2}{d} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_2}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$. Donc (S) admet une droite affine de solutions.
					\end{itemize}
					\item $b \neq 0$
					\begin{equation*}
						(S) \iff
						\left\{ \begin{array}{ccc}
							y &=& \frac{b_1}{b} \\
							0 &=& b_2 - \frac{db_1}{b}
						\end{array} \right.
					\end{equation*}
					\begin{itemize}[label=$\bullet$ Si]
						\item $b_2 - \frac{db_1}{b} \neq 0$, (S) n'admet aucune solution.
						\item $b_2 - \frac{db_1}{b} = 0$, $(S) \iff y = \frac{b_1}{b} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_1}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
					\end{itemize}
				\end{itemize}
				\item $c \neq 0$ alors $b = 0$
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{ \begin{array}{ccc}
							0 &=& b_1 \\
							cx + dy &=& b_2
						\end{array} \right.
					\end{aligned}
				\end{equation*}
				\begin{itemize}[label=$\bullet$ Si]
					\item $b_1 \neq 0$, (S) n'admet aucune solution.
					\item $b_1 = 0$, $(S) \iff x = \frac{b_2}{c} - \frac{d}{c}y \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} \frac{b_2}{c} - \frac{d}{c}t \\ t \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
				\end{itemize}
			\end{itemize}
		\end{itemize}
		
	\end{question_kholle}

\pagebreak\section{Semaine 9}
	
	\begin{question_kholle}
		[\noindent Soit \Rel une relation d'équivalence sur $E$. \\
		Soit $x \in E$. \\
		La classe de $x$, notée $\bar{x}$, est l'ensemble des éléments de $E$ en relation avec x.
		\begin{equation}
			\bar{x} = \left\{ y \in E \;|\; x \Rel y \right\}
		\end{equation}]
		{Deux classes d'équivalence sont disjointes ou confondues. Les classes d'équivalence constituent une partition de l'ensemble sur lequel on considère la relation d'équivalence.}
		
		\textit{Montrons que deux classes d'équivalence sont disjointes ou confondues.}
		
		Soit $(x, y) \in E^2$ fq.
		\begin{itemize}[label=\textemdash]
			\item Si $\bar{x} \cap \bar{y} = \emptyset$, rien à démontrer.
			\item Sinon $\bar{x} \cap \bar{y} \neq \emptyset$ donc $\exists z \in \bar{x} \cap \bar{y}$. Fixons un tel $z$.

			Soit $x' \in \bar{x}$ fq.
			\begin{equation*}
				\left.
				\begin{matrix}
					\left. \begin{matrix}
						x' \in \bar{x} \implies x \Rel x' \underset{sym\acute{e}trie}{\implies} x' \Rel x \\
						z \in \bar{x} \implies x \Rel z
					\end{matrix}
					\right\} \underset{transitivit\acute{e}}{\implies} x' \Rel z \\
					z \in \bar{y} \implies y \Rel z \underset{sym\acute{e}trie}{\implies} z \Rel y
				\end{matrix}
				\right\} \underset{transitivit\acute{e}}{\implies} x' \Rel y
				\underset{sym\acute{e}trie}{\implies} y \Rel x'
			\end{equation*}
			
			Donc $x' \in \bar{y}$ donc $\bar{x} \subset \bar{y}$.
			
			En échangeant les rôles de $x$ et $y$, on montre la deuxième inclusion $\bar{y} \subset \bar{x}$.
		\end{itemize}
		\bigbreak
	
		\textit{Montrons que les classes d'équivalence de E constituent une partition de E.}
		
		Soit $\Sol$ un système de représentant des classes fixé quelconque.
		
		\begin{itemize}[label=\textemdash]
			\item Soit $s\in \Sol$ fq. $\bar{s} \neq \emptyset$ car $s \Rel s$ par réflexivité.
			\item Soit $(s, s') \in \Sol^2$ fq. D'après la démonstration ci-dessus ci-dessus, $\bar{s} \cap \bar{s'} = \emptyset$ ou $\bar{s} = \bar{s'}$. Si $\bar{s} = \bar{s'}$ alors $s$ et $s'$ représente la même classe ce qui est impossible car un système de représentants des classes contient un unique représentant de chaque classe. Par conséquent, $\bar{s}$ et $\bar{s'}$ sont disjoints.
			\item $\underset{s \in \Sol}{\bigcup} \bar{s} \subset E$ car $\forall s \in \Sol, \bar{s} \in E$ par définition d'une classe d'équivalence. \\
			Réciproquement, soit $x \in E$ fq. \\
			Par réflexivité de \Rel, $x \in \bar{x}$. \\
			Par définition d'un système de classe $\exists ! s_x \in \Sol : s_x \in \bar{x}$ donc $\bar{s_x} = \bar{x}$. Donc $x \in \bar{s_x} \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. Donc $E \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. \\
			Par double inclusion, $E = \underset{s \in \Sol}{\bigcup} \bar{s}$.			
		\end{itemize}
		
		Ainsi,
		\begin{equation}
			E = \coprod_{s \in \Sol} \bar{s}
		\end{equation}
		
	\end{question_kholle}

	\begin{question_kholle}
		[\noindent Soit $(E, \leq)$ un ensemble ordonné, et $A$ une partie non-vide de $E$. \\
		Si $A$ admet un plus grand élément alors $A$ admet une borne supérieure et $\sup{A} = \max{A}$. \\
		Si $A$ admet une borne supérieure appartenant à elle-même alors $A$ admet un plus grand élément et $\max{A} = \sup{A}$.]
		{Si $A$ admet un plus grand élément c'est aussi sa borne supérieure. Si $A$ admet une borne supérieure dans $A$ c'est sont plus grand élément.}
		
		Soient un tel ensemble $E$ et une telle partie $A$ et notons $M$ son plus grand élément. \\
		Posons l'ensemble des majorants de $A$, $M(A) = \{ m\in E \ | \ \forall a \in A, \ a \leq m\}$. \\
		Par définition : 
		\[
		\forall m \in M(A), \ M \leq m,
		\]
		car $M\in A$, mais comme $M\in M(A)$, on a directement que $M = \min{M(A)} = \sup{A}$. \\
		
		Pseudo-réciproquement, soit $A$ une partie de $E$ admettant une borne supérieure dans elle même, notons cette borne $S$. \\
		Comme $S \in M(A)$, par définition, $S$ est plus grand que tous les éléments de $A$ mais appartient à $A$, donc de tous les éléments de $A$, $S$ est le plus grand.
	\end{question_kholle}

	\begin{question_kholle}
		[\begin{equation}
			\forall (a, b) \in \Z^2,
			\exists ! (q, r) \in \Z \times \N :
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in {[\![} 0 ; |b|-1 {]\!]}
			\end{matrix} \right.
		\end{equation}]
		{Théorème de la division Euclidienne dans \Z}
		
		\textit{Unicité} \;
		Soient deux tels entiers $(a,b) \in \Z^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \N\right)^2$ tels que
		\begin{equation*}
			\left\{ \begin{matrix}
				a = b q + r \\
				0 \leqslant r \leqslant |b| - 1
			\end{matrix} \right.
			\qquad
			\left\{ \begin{matrix}
				a = b q' + r' \\
				0 \leqslant r' \leqslant |b| - 1
			\end{matrix} \right.
		\end{equation*}
		Directement, 
		\[
		b(q-q') = r'-r,
		\]
		mais comme $-(|b|-1) \leqslant r' - r \leqslant |b| -1$, il vient en divisant par $|b|$ l'inégalité précédente :
		\[
		-1 < q - q' < 1,
		\]
		puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
		\newline
		\\
		\textit{Existence} \; Posons pour $b \geqslant 1$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
		\begin{itemize}
			\item $\Omega \subset \Z$
			\item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots) 
			\item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
		\end{itemize}
		Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
		Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $\Omega \subset \Z$, $q \in \Z$ donc $r \in \Z$.
		\\
		Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r\in [\![ 0, |b| -1 ]\!]$.
		
		Si $b< 1$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
	\end{question_kholle}

	\begin{question_kholle}{Une suite décroissante et minorée de nombres entiers relatifs est stationnaire}
		Soit $u \in \Z^\N$ une suite décroissante et minorée fixée quelconque. \\
		Considérons $A = \{ u_n \;|\; n \in \N \}$ c'est-à-dire l'ensemble des valeurs prises par la suite $u$. \\
		$A$ est : \begin{itemize}[label=\textemdash]
			\item une partie de \Z car $u$ est à valeur dans \Z
			\item non vide car $u_0 \in A$
			\item minoré car $u$ est minorée
		\end{itemize}
		Donc $A$ admet un plus petit élément. Donc $\exists n_0 \in \N: u_{n_0} = min A$. Fixons un tel $n_0$. \\
		Soit $n \in \N$ fq tq $n \geqslant n_0$.
		\begin{equation*}
			\left. \begin{matrix}
				u_n \in A \implies u_n \geqslant min A = u_{n_0} \\
				 u \text{ est décroissante et } n \geqslant n_0 \text{ donc } u_n \leqslant u_{n_0}
			\end{matrix}
			\right\} \implies u_n = u_{n_0}
		\end{equation*}
  		Ainsi, $u$ est stationnaire.
	\end{question_kholle}
\pagebreak\section{Semaine 10}

	\begin{question_kholle}
		[\noindent Soient $(A, B) \in \mathcal{P}(\R)^2$ fq. \\
		\textit{Définition de la densité}
		\begin{align}
			A \text{ est dense dans } B
			\text{ si } \left\{ \begin{array}{ll}
				A \subset B \\
				\mathrm{et} \\
				\forall (u,v) \in \R^2, B \cap {]}u;v{[} \neq \emptyset \implies A \cap  {]}u;v{[} \neq \emptyset
			\end{array} \right.
		\end{align}
		\textit{Caractérisation de la densité par les $\varepsilon$}
		\begin{align}
			A \text{ est dense dans } B
		 	\iff \left\{ \begin{array}{ll}
				A \subset B \\
				\mathrm{et} \\
				\forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon
			\end{array} \right.
		\end{align}
		]
		{Caractérisation de la densité d’une partie $A$ de \R dans une partie $B$ de \R la contenant avec des $\varepsilon$.}

		\textit{Montrons la caractérisation de la densité}\\
		\emph{Sens Direct} Supposons $A$ dense dans $B$
		\begin{itemize}[label=\textemdash]
            \item Par déf $A \subset B$
            \item Soit $b \in B$ et $\varepsilon \in \R_+^*$ fq

            Appliquons le (ii) de la déf de Densité pour $u \leftarrow b - \varepsilon$ et $v \leftarrow b + \varepsilon$
            $$B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset \implies A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$$
            Or, $B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset$ est vraie
            donc $A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$

            Ce qui permet de choisir $a \in A \cap ]b - \varepsilon,  b + \varepsilon[$.
            Un tel $a$ vérifie $a \in A$ et $a \in ]b - \varepsilon,  b + \varepsilon[ \iff |b-a| < \varepsilon$
		\end{itemize}
		\bigbreak
    	\noindent \emph{Sens réciproque} Supposons $\left\{\begin{array}{ll} A \subset B \\\mathrm{et}\\ \forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon \end{array}\right.$

        \begin{itemize}
            \item On a donc $A \subset B$
            \item Soient $(u, v) \in \R^2$ fq tq $B \cap ]u, v[ \neq \emptyset$

			Soit $b \in B \cap ]u, v[$ fq.
            Appliquons l'hypothèse pour $b\leftarrow b$ et $\varepsilon \leftarrow \min\{v - b, b - u\}$, qui est autorisé $v-b$ et $b-u$ sont positifs

            Donc $\exists a \in A: | b - a| < \varepsilon $

            Fixons un tel a, alors:
            $$
                b-\varepsilon < a < b + \varepsilon
            $$

            Donc $$
            \left\{\begin{array}{ll}
            a < b + \varepsilon = b + \underbrace{\min\{v - b, b - u\}}_{\leqslant v - b} \leqslant b + v - b = v \\ \mathrm{et}\\
            a > b - \varepsilon = b - \underbrace{\min\{v - b, b - u\}}_{\leqslant b - u} \geqslant b - (b - u) = u
            \end{array}\right.
            $$

            Donc $a \in ]u, v[$.
        \end{itemize}
        Donc $A \cap ]u, v[ \neq \emptyset$

	\end{question_kholle}

	\begin{question_kholle}
		[\begin{equation}
			\forall (a, b) \in \R \times \R^*,
			\exists ! (q, r) \in \Z \times \R :
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in [0;|b|[
			\end{matrix} \right.
		\end{equation}]
		{Théorème de la division pseudo-euclidienne dans \R}

		\textit{Unicité} \;
		Soient deux tels entiers $(a,b) \in \R^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \R\right)^2$ tels que
		\begin{equation*}
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in [0;|b|[
			\end{matrix} \right.
			\qquad
			\left\{ \begin{matrix}
				a = b q' + r' \\
				r' \in [0;|b|[
			\end{matrix} \right.
		\end{equation*}
		Directement,
		\[
		b(q-q') = r'-r,
		\]
		mais comme $-|b| < r' - r < |b|$, il vient en divisant par $|b|$ l'inégalité précédente :
		\[
		-1 < q - q' < 1,
		\]
		puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
		\newline
		\\
		\textit{Existence} \; Posons pour $b > 0$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
		\begin{itemize}
			\item $\Omega \subset \Z$
			\item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots)
			\item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
		\end{itemize}
		Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
		Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $r \in \R$.
		\\
		Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r \in [ 0, |b| [$.

		Si $b < 0$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
	\end{question_kholle}

	\begin{question_kholle}
		{\Q est dense dans \R et $\R \setminus \Q$ est aussi dense dans \R}

		Soit $x \in \R$ fq.
		Posons $\forall n \in \N, a_n = \frac{\lfloor2^n x\rfloor}{2^n}$. \\
		Soit $n \in \N$ fq. \\
		\begin{itemize}
			\item $a_n \in \Q$ car $\lfloor2^n x\rfloor \in \Z$ et $2^n \in \N$.
			\item \begin{equation*}
					a_n = \frac{\lfloor2^n x\rfloor}{2^n}
					\implies \frac{2^n x - 1}{2^n} \leqslant a_n \leqslant \frac{2^n x}{2^n}
					\implies x - \frac{1}{2^n} \leqslant a_n \leqslant x
				\end{equation*}
				Or $\nicefrac{1}{2^n} \arrowlim{n}{+\infty} 0$ donc d'après le théorème d'existence de limite par encadrement, \\ $a_n \arrowlim{n}{+\infty} x$.
		\end{itemize}
		Donc d'après la caractérisation séquentielle de la densité, \Q est dense dans \R.
		\bigbreak

		\noindent Soit $x \in \R$ fq. \\
		Alors $x + \sqrt{2} \in \R$.
		D'après la démonstration précédente, $\exists b \in \Q^\N : b_n \arrowlim{n}{+\infty} x + \sqrt{2}$. \\
		Fixons un telle suite $b$.
		Considérons $c = b - \sqrt{2}$. \\
		Soit $n \in \N$ fq.
		\begin{itemize}
			\item $c_n \in \R\setminus\Q$ car $b_n \in \Q$ et $\sqrt{2} \in \R \setminus \Q$.
			\item \begin{equation*}
				\left. \begin{matrix}
					b_n \arrowlim{n}{+\infty} x + \sqrt{2} \\
					c_n = b_n - \sqrt{2}
				\end{matrix} \right\}
				\implies c_n \arrowlim{n}{+\infty} x
			\end{equation*}
		\end{itemize}
		Donc d'après la caractérisation séquentielle de la densité, $\R\setminus \Q$ est dense dans \R.
	\end{question_kholle}

	\begin{question_kholle}[
        Soit u $\in \K ^ \N, (\ell_1, \ell_2) \in \K ^2$
        Si u converge vers $\ell_1$ et $\ell_2$, alors $\ell_1 = \ell_2$
    ]{Preuve de l'unicité de la limite d'une suite convergente}
    Par l'absurde, supponsons que $u$ converge vers $\ell_1$ et $\ell_2$, et $\ell_1 \neq \ell_2$.
    On prendra $\varepsilon_0 = \varepsilon_1 = \varepsilon_2$ assez petit pour que les tubes soient disjoints.\\
    Posons donc $\varepsilon_0 = \frac{|\ell_1 - \ell_2|}{3}$
    \begin{itemize}
        \item Appliquons la définition de la convergence de u vers $\ell_1$, pour $\varepsilon \leftarrow \varepsilon_0$, ce qui est autorisé car $\varepsilon_0 \in \R_+^*$
        \begin{equation}\label{eq:1}
            \exists N_1 \in \N : \forall n \in \N, n \geqslant N_1 \implies |u_n - \ell_1| \leqslant \varepsilon_0
        \end{equation}
        \begin{equation}\label{eq:2}
            \exists N_2 \in \N : \forall n \in \N, n \geqslant N_2 \implies |u_n - \ell_2| \leqslant \varepsilon_0
        \end{equation}
        Fixons de tels $N_1$ et $N_2$.
        \item Posons $n_0 = N_1 + N_2$
        \begin{itemize}
            \item $n_0 \geqslant N_1$, donc (\ref{eq:1}) s'applique: $|u_{n_0} - \ell_1| \leqslant \varepsilon_0$
            \item $n_0 \geqslant N_2$, donc (\ref{eq:2}) s'applique: $|u_{n_0} - \ell_2| \leqslant \varepsilon_0$
        \end{itemize}
        \item \begin{align*}
            |\ell_1 - \ell_2| &= |\ell_1 - u_{n_0} + u_{n_0} - \ell_2|\\
            &\leqslant \underbrace{|\ell_1 - u_{n_0}|}_{\leqslant \varepsilon_0} + \underbrace{|u_{n_0} - \ell_2|}_{\leqslant \varepsilon_0}\\
            &\leqslant 2 \frac{|\ell_1 - \ell_2|}{3}\\
            \implies 1 &\leqslant \frac 2 3
        \end{align*}
        Contradiction
    \end{itemize}
	\end{question_kholle}

	\begin{question_kholle}{Une suite convergente est bornée}

        Soit $u \in \mathbb{K}^{\mathbb{N}}$ convergente.
Posons $\ell = \lim u$
Appliquons la définition de la convergence pour $\varepsilon \leftarrow 1$
$$
\exists N_{1}\in \mathbb{N}: \forall n \in \mathbb{N}, n \geqslant N_{1} \implies |u_{n}-\ell| \leqslant 1
$$
Fixons un tel $N_{1}$
Posons alors $M = \max\left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$, qui est bien défini, car toute partie finie, non vide d'un ensemble totalement ordonné (ici $(\mathbb{R}, \leqslant)$) admet un pgE.

Soit $n \in \mathbb{N}$ fq.
\begin{itemize}
    \item Si $n \in [[0, N_{1}]], |u_{n}| \in \left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$ donc $|u_{n}| \leqslant M$
	\item Sinon,
\end{itemize}

\begin{align*}
n> N_{1} &\implies |u_{n} - \ell| \leqslant 1 \\
&\implies |u_{n}| - |\ell| \leqslant 1 \\
 & \implies |u_{n}| \leqslant 1+ |\ell| \leqslant M
\end{align*}

Ainsi, $\forall n \in \mathbb{N}, |u_{n}| \leqslant M$.
    \end{question_kholle}
\pagebreak\section{Semaine 11}

	\begin{question_kholle}
		[Soient $(A,B) \in (\mathcal{P}(\R) \setminus \{\varnothing\})^{2}$. Montrons que :
		\\
		$$A \text{ est dense dans } B \iff \left\{ \begin{array}{l}
			A \subset B \\
			\forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b
		\end{array}\right. $$
		]
		{Caractérisation séquentielle de la densité.}
		
		Sens indirect : supposons $A \subset B$ et $\forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b$ :\\
		\begin{itemize}
			\item[$\star$] $A \subset B$ par hypothèse.
			\item[$\star$] Montrons que $\forall b \in B, \forall \varepsilon \in \R^{*}_{+}, \exists a \in A : |b - a| < \varepsilon$ (on utilise la caractérisation de la densité avec les $\varepsilon$) \\
			Soient $b \in B$ et $\varepsilon \in \R^{*}_{+}$ fixés quelconques : \\
			Par hypothèse appliquée pour $b \leftarrow b$ : $\exists(a_{n}) \in A^{\N} : a_{n} \underset{n \to +\infty}{\longrightarrow}b$ \\
			Appliquons la définition de la convergence de $(a_{n})$ vers $b$ pour $\varepsilon \leftarrow \frac{\varepsilon}{2}$ : \\
			$$\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow |a_{n} - b_{n}| \leqslant \frac{\varepsilon}{2}$$
			Fixons un tel N : \\
			En particulier, $a_{N} \in A$ et $|a_{N} - b| \leqslant \frac{\varepsilon}{2} \leqslant \varepsilon$ \\
			Donc $A$ est dense dans $B$.
		\end{itemize}
		
		Sens direct : supposons $A$ dense dans $B$ : \\
		\begin{itemize}
			\item[$\star$] Par définition, $A \subset B$
			\item[$\star$] Soit $b \in B$ fixé quelconque. \\
			Soit $n \in \N$ fixé quelconque :  \\
			Appliquons la caractérisation de la densité par les $\varepsilon$ pour $\varepsilon \leftarrow \frac{1}{2^{n}}$ (autorisé car $\frac{1}{2^{n}} > 0$), et $b \leftarrow b$ : 
			$$\exists a \in A : |a - b| \leqslant \frac{1}{2^{n}}$$
			Notons $a_{n}$ un tel élément. Nous venons de construire $(a_{n})_{n \in \N} \in A^{\N}$ vérifiant : \\
			$\forall n \in \N, |a - b| \leqslant \frac{1}{2^{n}}$ \\
			Or : $\underset{n \to +\infty}{\lim} \frac{1}{2^{n}} = 0$ \\
			Ainsi, d'après le théorème sans nom, $(a_{n})_{n \in \N}$ converge vers $b$.
		\end{itemize}
	\end{question_kholle}
	
	\begin{question_kholle}
	    [Soit $u\in \R ^{\N}$ qui converge vers $\ell \in \R$. \\
	    Alors la moyenne arithmérique des $n\in \N$ premiers termes (appelée moyenne de Césarò) converge vers $\ell$.]
	    {Théorème de Césarò}
	
	    Soient $u$ une telle suite, $\varepsilon \in \R ^*_+$ et $\ell \in \R$ ladite limite de $u$. Appliquons la définition de la convergence de $u$ pour $\varepsilon \gets \frac{\varepsilon}{2}$ : 
	    \[
	    \exists N \in \N \ : \ \forall n \in \N , \ n\geq N \ \implies \ |u_n - \ell | \leq \frac{\varepsilon}{2}.
	    \]
	    Fixons un tel $N$. Posons $\omega = \sum_{k=0}^{N-1} |u_k - \ell | \in \R$. Soit $n\in \N$ tel que $n \geq N$. Calculons : 
	    \[
	    \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| = \left| \frac{1}{n} \left( \sum_{k=0}^{n-1}u_k - n\ell \right) \right|  = \left| \frac{1}{n} \sum_{k=0}^{n-1}(u_k - \ell)  \right| \leq \frac{1}{n} \underset{= \ \omega \in \R}{\underbrace{\sum_{k=0}^{N-1}|u_k - \ell|}} + \frac{1}{n} \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\sum_{k=N}^{n}|u_k - \ell|}} \leq \frac{\omega}{n} + \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\varepsilon}{2n}}}.
	    \]
	    Ces majorations sont issues de l'inégalité triangulaire et de la convergence de $u$. De plus, comme la suite $(v_n) _{n\in \N} = \left( \frac{\omega}{n} \right) _{n\in \N}$ converge vers $0$, on écrit sa définition pour $\varepsilon \gets \frac{\varepsilon}{2}$ : 
	    \[
	    \exists N' \in \N \ : \ \forall n \in \N , \ n\geq N' \ \implies \ |v_n| \leq \frac{\varepsilon}{2}.
	    \]
	    On fixe un tel $N'$ et on pose $\Lambda = \max{(N, N')}$ qui a bien un sens car $\{N, \ N'\}$ est une partie finie de $\N$.
	    De la même manière qu'auparavant, pour $n\in \N$ tel que $n \geq \Lambda$, on a : 
	    \[
	     \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| \leq \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\omega}{n}}} + \frac{\varepsilon}{2} \leq \varepsilon.
	    \] 
	    C'est le théorème souhaité.
	\end{question_kholle}

	\begin{question_kholle}
		[Soient $(u,v) \in \R^{\N}$ : \\
		{\begin{enumerate}[label=($\roman*$)]
			\item Si $\begin{array}{|l}
				\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0   \\
				u \text{ converge}
			\end{array}$ \\
			Alors $\lim u \geqslant 0$
			\item Si $\begin{array}{|l}
				\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n}   \\
				u \text{ et } v \text{ convergent}
			\end{array}$ \\
			Alors $\lim u \leqslant \lim v$
		\end{enumerate}}
		]
		{Théorème de passage à la limite dans une inégalité.}
		~\smallbreak
		\begin{enumerate}[label=($\roman*$)]
			\item L'hypothèse $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0$ permet d'affirmer que $u$ et $|u|$ coïncident à partir d'un certain rang. \\
			Par ailleurs, la convergence de $u$ et la continuité de $|\cdot|$ sur $\R$ donc en $\lim u$ donnent $|u|$ converge vers $|\lim u|$. \\
			Le caractère asymptotique de la limite permet de conclure que $u$ et $|u|$ ont la même limite. \\
			Donc $\lim u = |\lim u| \geqslant 0$
			\item $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n} \Rightarrow v_{n} - u_{n} \geqslant 0$ \\
			$u$ et $v$ convergent $\Rightarrow v-u$ converge vers $\lim v - \lim u$. \\
			On applique $(i)$ pour $u \leftarrow v - u$, autorisé car $u \text{ et }v$ convergent. \\
			On obtient $\lim v - \lim u \geqslant 0$ d'où $\lim u \leqslant \lim v$.
		\end{enumerate}
	\end{question_kholle}

	\begin{question_kholle}
	    [Soient $u$ et $v$ deux suites réelles adjacentes. Alors $u$ et $v$ convergent et ont la même limite.]
	    {Théorème des suites adjacentes}
	
	    Soient $u$ et $v$ de telles suites. Quitte à inverser les rôles desdites suites, prenons $u$ croissante et $v$ décroissante. \\
	    On a donc : 
	    \[
	    \forall n \in \N, \ (u_n \leq v_n \leq \underset{\in \R}{\underbrace{v_0}}) \wedge (\underset{\in \R}{\underbrace{u_0}}\leq  u_n \leq v_n),
	    \]
	    car la monotonie des suites induit ces inégalités. D'après le théorème de limite monotone, $u$ étant croissante et majorée elle converge, $v$ étant décroissante et minorée elle converge. \\
	    Il s'en suit que par définition des suites adjacentes : 
	    \[
	    0 \ = \lim_{n \to +\infty} (u_n - v_n) \ \underset{u,v \ \text{ convergent}}{\underbrace{=}} \ \lim_{n \to +\infty} u_n - \lim_{n \to +\infty} v_n.
	    \]
	    Ainsi, $\lim u = \lim v$.
	\end{question_kholle}
	
	\begin{question_kholle}
	    [Soit $u$ une suite bornée. $u$ converge si et seulement si il existe $\ell \in \mathbb{K}$ tel que $L(u)$ est le singleton $\ell$ ]
	    {*Facultative* Caracterisation de la convergence par l'unicité d'une valeur d'adhérence pour une suite bornée.}
	    Traitons le cas réel, celui sur $\C$ est à adapter sans peine.\\
	    Supposons que $u$ converge et posons $\lim u =\ell \in \R  $. Toutes les sous-suites de $u$ convergent vers $\ell$ donc $L(u)=\{\ell \}$. \\
	    Supposons maintenant qu'il existe un unique $\ell \in \R$ tel que $L(u) = \{ \ell \}$. Par l'absurde, supposons que $u$ ne converge pas vers $\ell$, c'est-à-dire : 
	    \[
	    \exists \varepsilon \in \R ^* _+ \ : \ \forall N \in \N, \ \exists n \in \N \ : \ n\geq N \text{ et } |u_n - \ell | > \varepsilon.
	    \]
	    Fixons un tel $\varepsilon$. \\
	    %\textbf{Etape 1} : \textit{Construction d'une sous-suite de $u$ dont les termes sont $\varepsilon$-éloignés de $\ell$.} \\
	    Posons $\varphi (0) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon \} }$, ce qui a du sens car c'est une partie non-vide de $\N$. Posons ensuite $\varphi (1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(0) < k \} } $, ce qui a du sens pour les mêmes raisons. On construit en itérant ce procédé $\varphi (n)$ tel que : 
	    \[
	    \forall n \in \N, \ \varphi(n+1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(n) < k \} }.
	    \]
	    De cette manière, nous venons de construire une extractrice telle que : 
	    \[
	    \forall n \in \N, \ |u_{\varphi(n)} - \ell| > \varepsilon.
	    \]
	    Par hypothèse $u$ est bornée, donc il existe $M\in \R _+$ tel que : 
	    \[
	    \forall n \in \N, \ |u_n| \leq M,
	    \]
	    donc pour tout $n$ dans $\N$, $|u_{\varphi(n)}| \leq M$, donc $(u_{\varphi(n)})_{n\in \N}$ est bornée. \\
	    Par le théorème de Bolzano-Weierstrass, il existe $\psi$ une extractrice et $\ell ' \in \R$, avec $\varphi \circ \psi$ qui est aussi une extractrice par composition d'applications strictement croissantes, donc$(u_{\varphi \circ \psi (n)})_{n\in \N}$ est une sous-suite de $u$ et $\ell ' \in L(u) = \{ \ell \}$.\\
	    Par ailleurs, pour tout $n$ dans $\N$ :
	    \[
	    \underset{\xrightarrow[n\to +\infty]{}|\ell' -\ell|}{\underbrace{|u_{\varphi \circ \psi (n)} - \ell|}} > \varepsilon,
	    \]
	    donc en passant à la limite dans l'inégalité on a pour tout $n$ dans $\N$, $|\ell ' - \ell | \geq \varepsilon > 0$, ce qui n'est pas possible car $\ell$ est la seule valeur d'adhérence possible et ici la différence n'est pas nulle.
	\end{question_kholle}


\end{document}
