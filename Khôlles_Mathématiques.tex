% Ceci est un fichier généré automatiquement.
% Ne pas le modifier directement. Exécuter $ make pour le générer.
% Il rassemble les questions de khôlles de toutes les semaines.

\documentclass{article}

\usepackage{kholles}
\usepackage{braket}

\renewenvironment{question_kholle}[2][ ]
{
	\subsection{\texorpdfstring{#2}{}}
	\notblank{#1}
	{
		\noindent #1
		\bigbreak
	}
	{}
	\begin{proof}
}
{
	\end{proof}
}
\renewcommand{\setnbquestion}[1]{
	\setcounter{subsection}{\numexpr#1-1\relax}
}

\begin{document}
	\maketitle

	\begin{abstract}
		Bienvenue très chers camarades sereins, ce document contient les questions de khôlles de mathématiques de la MP1 de Fermat. Il est coécrit par Kylian Boyet, George Ober, Hugo \textit{Vangi}lluwen (qui maintient la structure du projet, la compilation et le paquet kholles.sty) avec la contribution de Jérémie Menard.
		Il n'est malheureusement pas exhaustif. Si vous voulez nous aider, lisez CONTRIBUER.md et envoyez-nous votre code \LaTeX \ ou plus simplement dites-nous quand vous rencontrez une erreur.
	\end{abstract}

	\pagenumbering{roman}
	\tableofcontents
	\clearpage
	\pagenumbering{arabic}

\pagebreak\section{Semaine 1}


\begin{question_kholle}{Preuve formelle de la somme des entiers et des termes d'une suite géométrique} 
    \begin{itemize}[label=$\lozenge$]
        \item Soit $n \in \mathbb{N}$ fq. Posons $$S_{n} = \sum_{k=0}^{n}k$$
        En posant la symétrie d'indice $i = n-k$, on a aussi
        $$
        S_{n}= \sum_{i=0}^{n}(n-i)=\sum_{i=0}^{n}n - \sum_{i=0}^{n}i=(n \times \mathrm{card}[ \! [ 0, n ] \!]) - \sum_{i=0}^{n} i
        $$
        Or, puisque $\mathrm{card}[ \! [ 0, n ] \!] = n + 1$ et que $\sum_{i=0}^{n} i = S_{n}$
        $$
        S_{n} = n \times (n+1) + S_{n}
        $$
        Donc
        $$
        S_{n} = \frac{n(n+1)}{2}
        $$
        \item Soient $q \in \mathbb{R}$ , $k \in \mathbb{N}$ fixés quelconques.
        \begin{itemize}[label=$\star$]
            \item Si $q = 1$, 
            $$
            \sum_{i=0}^{k}q^{i} = \sum_{i=0}^{k}1 = k+1
            $$
            \item Sinon, avec l'identité algébrique, on a
            $$
            q^{k+1}-1^{k+1} = (q-1) \sum_{i=0}^{k}q^{i}\times 1^{k-i}
            $$
            Ainsi, puisque $q \neq 1$ on a, par multiplication par $(q-1)^{-1}$
            $$
            \sum_{i=0}^{k}q^{i} = \frac{q^{k+1}-1}{q-1}
            $$
            Nous avons donc établi que
            $$
            \sum_{i=0}^{k} q^{i} = \left\{ \begin{array}{ll}
                \frac{1-q^{k+1}}{1-q}  &\text{ si } q \neq 1 \\
                k + 1 &\text{ sinon}
            \end{array}\right.
            $$
        \end{itemize}
    \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Preuve de la factorisation de $a^n - b^n$ puis de celle de $a^{2m+1} + b^{2m+1} $}
    Calculons 
    
    \begin{align*}
        (a-b)\sum_{k=0}^{m-1}a^{k}b^{m-1-k} 
        &=a \sum_{k=0}^{m-1}a^{k}b^{m-1-k} -b \sum_{k=0}^{m-1}a^{k}b^{m-1-k} \\
        &= \sum_{k=0}^{m-1}a^{k+1}b^{m-1-k} - \sum_{k=0}^{m-1}a^{k}b^{m-k} \\
    \end{align*}
    
    Si bien qu'en posant le changement d'indice $j = k + 1$ on reconnait le téléscopage.
    $$
    \sum_{j=1}^{m}a^{j}b^{m-j} - \sum_{k=0}^{m-1}a^{k}b^{m-k} = a^{m} - b ^{m}
    $$
\end{question_kholle}
\begin{question_kholle}[
    {$$\left( \sum_{k=1}^{n}x_{k} \right)^{2} = \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j} = 2 \sum_{1\leqslant k < j \leqslant n} x_{k}x_{j}+\sum_{k=1}^{n}x_{i}^{2}$$}
    ]{Développement d'une somme}
    
    \begin{align*}
        \left( \sum_{k=0}^{n}x_{k} \right)^{2}
        &= \left( \sum_{k=1}^{n} x_{k} \right) \times \left( \sum_{j=1}^{n} x_{j} \right) \\
        &= \sum_{k=1}^{n}\left[ x_{k} \times \sum_{j=1}^{n}x_{j} \right] \\
        &= \sum_{k=1}^{n}\left( \sum_{j=1}^{n} x_{k}\times x_{j} \right) \\
        &= \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j}
    \end{align*}
    
    
    On peut aussi séparer cette somme
    
    \begin{align*}
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j}
        &=
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} &+
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n\\k=j}} x_{k}x_{j} &+
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}\\
        &=
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} & +
        \underbrace{ \sum_{k=1}^{n}x_{k}^{2} }_{ \text{somme sur les indices }(k,j) \text{ tels que } k = j } &+
        \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}
    \end{align*}
    
    On remarque aussi qu'en permutant les indices des deux sommes (les variables sont muettes)
    $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} = \sum_{\substack{1\leqslant j \leqslant n\\ 1 \leqslant k \leqslant n\\j<k}} x_{j}x_{k}
    $$
    Qui, par commutativité du produit dans $\mathbb{C}$ nous donne cette égalité
    $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} =
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n\\k>j}} x_{k}x_{j}
    $$
    On a donc bien l'identité attendue :
    $$
    \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n}} x_{k}x_{j} = 2 \sum_{\substack{1\leqslant k \leqslant n\\ 1 \leqslant j \leqslant n \\ k<j}} x_{k}x_{j} +
    \sum_{k=1}^{n}x_{k}^{2}
    $$
\end{question_kholle}

\begin{question_kholle}[{Pour tout $(a, b) \in \C^2$, $n \in \N$: $$(a+b)^{n} = \sum_{k=0}^{n}\binom n k a^{k} b^{n-k}$$}]{Preuve de la formule du binôme de Newton}
    Soient $(a, b) \in \mathbb{C}^{2}$ fixés quelconques.
    Posons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \mathbb{N}$ par
    $$
    \mathcal{P}(n): (a+b)^{n} = \sum_{k=0}^{n}\binom n k a^{k} b^{n-k}
    $$
    \begin{itemize}[label=$\star$]
        \item Initialisation, $n \leftarrow 0$
        D'une part $(a+b)^{0} = 0$, même si les deux sont nuls (par convention $0^{0} =0$)
        D'autre part
        $$
        \sum_{k=0}^{0}\binom 0 k a^{k}b^{n-k} = \binom 0 0 a^{0} b^{0} = 0
        $$
        Donc $\mathcal{P}(0)$ est vérifée.
        
        \item Soit $n \in \mathbb{N}$ fixé quelconque tel que $\mathcal{P}(n)$ est vraie
        
        \begin{align*}
            (a+b)^{n+1} &= (a+b)\times(a+b)^{n} \\
            &= (a+b) \times \sum_{k=0}^{n}\binom n k a^{k} b^{n-k} \\
            &= a\sum_{k=0}^{n}\binom n k a^{k} b^{n-k} + b\sum_{k=0}^{n}\binom n k a^{k} b^{n-k} \\
            &= \sum_{k=0}^{n}\binom n k a^{k+1} b^{n-k} + \sum_{k=0}^{n}\binom n k a^{k} b^{n+1-k} \\
            &= \sum_{j=1}^{n+1}\binom n {j-1} a^{j} b^{n+1-j} + \sum_{k=0}^{n}\binom n k a^{k} b^{n+1-k} \\
            (\text{en posant } j = k + 1) &= a^{n+1} + \sum_{j=1}^{n}\binom n {j-1} a^{j} b^{n+1-j} + \sum_{k=1}^{n}\binom n k a^{k} b^{n+1-k} + b^{n+1} \\
            &= a^{n+1} + \left( \sum_{k=1}^{n} \left(\binom n {k-1} + \binom n k \right) a^{k} b^{n+1-k} \right) + b^{n+1} \\
            (\text{en utilisant la relation de Pascal}) &= a^{n+1} + \sum_{k=1}^{n} \binom {n+1} k a^{k} b^{n+1-k} + b^{n+1} \\
            &=\sum_{k=0}^{n+1} \binom {n+1} k a^{k} b^{n+1-k}
        \end{align*}
        
        Donc $\mathcal{P}(n+1)$ est vraie.
    \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer que tout entier $n > 2$ admet un diviseur premier}
    Raisonnons par récurrence forte avec la propriété $\mathcal{P}(\cdot)$ définie pour tout $n > 2$ par
    $$
    \mathcal{P}(n) : « \forall k \in [ \! [ 2, n ] \!], k \text{ admet un diviseur premier } »
    $$
    \begin{itemize}
        \item Initialisation: $n \leftarrow 2$
        
        Soit $k \in [ \! [ 2, 2 ] \!]$ fixé quelconque. Nécéssairement, $k = 2$.
        or, $2$ admet $2$ pour diviseur premier.
        
        Donc $\forall k \in [ \! [ 2, 2 ] \!], k \text{ admet un diviseur premier}$, ce qui prouve $\mathcal{P}(2)$.
        
        \item Hérédité: Soit $n \in \mathbb{N} \setminus \{ 1, 0 \}$ fixé quelconque tel que $\mathcal{P}(n)$ est vraie.
        
        Pour montrer $\mathcal{P}(n+1)$, il nous faudra montrer que $\forall k \in [ \! [ 2, n+1 ] \!], k \text{ admet un diviseur premier }$
        
        Soit $k \in [ \! [ 2, n+1 ] \!]$ fixé quelconque.
        \begin{itemize}[label=$\star$]
            \item    Si $k \in [ \! [ 2, n ] \!]$, alors la véracité de $\mathcal{P}(n)$ nous permet de conclure, et de dire que k admet un diviseur premier.
            
            \item Sinon $k = n + 1$
            \begin{itemize}[label=$\lozenge$]
                \item Si $n+1$ est premier, alors il admet $k$ comme diviseur premier
                \item Sinon, $\exists d \in [ \! [ 2, n ] \! ]: d \mid n+1$ 
                
                Mais, puisque $d \in [ \! [ 2, n ] \! ]$, la véracité de $\mathcal{P}(n)$ nous permet d'affirmer que $d$ admet un diviseur premier $p$. Donc par transitivité de la relation de divisibilité $$(p \mid d \text{ et } d \mid n) \implies p \mid n$$
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer par récurrence qu'une fonction polynomiale à coefficients réels est nulle si et seulement si tous ses coefficients sont nuls}
    Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \mathbb{N}$
    $$
    \mathcal{P}(n) : \text{toute fonction polynômiale identiquement nulle sur }\mathbb{R} \text{ a tous ses coefficients nuls}
    $$
    Autrement dit
    $$
    \mathcal{P}(n) : \forall(a_{0}, \dots ,a_{n}) \in \mathbb{R}^{n+1}\left( \forall x \in \mathbb{R}, \sum_{k=0}^{n}a_{k}x^{k} = 0 \right) \implies \forall k \in [ \! [ 0, n ] \!], a_{k} = 0
    $$
    \begin{itemize}[label=$\lozenge$]
        \item Pour $n \leftarrow 0$
        Soit $a_{0} \in \mathbb{R}$ fixé quelconque tel que $\forall x \in \mathbb{R}, a_{0}x^{0} = 0$ Alors $a_{0} = 0$
        
        \item Soit $n \in \mathbb{N}$ fixé quelconque tel que $\mathcal{P}(n)$ est vraie
        Soient $(a_{0},\dots, a_{n+1}) \in \mathbb{R}^{n+2}$
        Posons $Q(x) = \sum_{k=0}^{n+1}a_{k}x^{k}$ tel que $\forall x \in \mathbb{R}, Q(x) = 0$
        D'une part
        $$
        \forall x \in \R, \underbrace{ Q(2x) }_{ =0 } - 2^{n+1}\underbrace{ Q(x) }_{ =0 } = 0
        $$
        D'autre part
        
        \begin{align*}
            \forall x \in \mathbb{R}, Q(2x) - 2^{n+1}Q(x) &= \sum_{k=0}^{n+1}a_{k}(2x)^{k}-2^{n+1}\sum_{k=0}^{n+1}a_{k}x^{k} \\
            &= \sum_{k=0}^{n+1}a_{k}(2^{k}-2^{n+1})x^{k}
        \end{align*}
        
        Le terme d'indice $n+1$ s'annule, si bien que l'on peut écrire
        $$\forall x \in \mathbb{R}, Q(2x) - 2^{n+1}Q(x) = \sum_{k=0}^{n}a_{k}(2^{k}-2^{n+1})x^{k}$$
        Qui est une fonction polynômiale de degré $\leqslant n$, ce qui permet d'appliquer $\mathcal{P}(n)$ pour $(a_{k})_{k \in[ \! [ 0, n ] \!]} \leftarrow (a_{k}(2^{k}-2^{n+1}))_{k \in[ \! [ 0, n ] \!]}$.
        
        Donc $\forall x \in [ \! [ 0, n ] \!]: a_{k}(2^{k}-2^{n+1}) = 0$ et puisque $2^{k}-2^{n+1} \neq 0$, on en déduit que $$\forall k \in [ \! [ 0, n ] \!], a_{k} = 0$$
        
        L'expression de $Q$ devient:
        $$
        \forall x \in \mathbb{R}, \underbrace{ \sum_{k=0}^{n}a_{k}x^{k} }_{ =0 }+ a_{n+1}x^{n+1} = 0
        $$
        Donc en particularisant pour $x\leftarrow 1$, on en déduit que $a_{n+1} = 0$
        
        Donc $\mathcal{P}(n+1)$ est vraie.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer par analyse/synthèse qu'une fonction réelle d'une variable réelle s'écrit de manière unique comme somme d'une fonction paire et d'une fonction impaire}
    Soit $f \in \mathcal{F}(\mathbb{R}, \mathbb{R})$ fixée quelconque.
    \begin{itemize}[label=$\lozenge$]
        
        
        \item \underline{Analyse}:
        Supposons que $f : \mathbb{R} \to \mathbb{R}$ se décompose de manière unique en $f = g+h$ avec $g$ paire et $h$ impaire (i.e. $\forall x \in \mathbb{R}, g(-x)= g(x)$ et $h(-x)= -h(x)$).
        Soit $x \in \mathbb{R}$ fixé quelconque
        Calculons $f(-x)$:
        $$f(-x) = g(-x) + h(-x) = g(x) -h(x)$$
        Par demi somme, nous avons donc
        $$
        \left\{ \begin{array}{ll}
            2g(x)  = f(x)+f(-x) \\
            2h(x) = f(x)-f(-x)
        \end{array}\right.
        $$
        Ainsi, si une telle décomposition existe, c'est
        $$
        \left\{ \begin{array}{ll}
            g : x \mapsto \frac{f(x)+f(-x)}{2}  \\
            h : x \mapsto \frac{f(x)-f(-x)}{2}
        \end{array}\right.
        $$
        
        \item \underline{Synthèse}:
        Posons
        
        \begin{align} 
            g\left|\begin{array}{ll} \mathbb{R} &\to \mathbb{R} \\ x &\mapsto \frac{f(x)+f(-x)}{2} \end{array}\right.
            \text{ et } h \left|\begin{array}{ll} \mathbb{R} &\to \mathbb{R} \\ x &\mapsto \frac{f(x)-f(-x)}{2} \end{array}\right.
        \end{align}
        
        
        Remarquons, d'une part que:
        $$
        \forall x \in \mathbb{R}, g(x)+h(x) = \frac{f(x)+f(-x)}{2} + \frac{f(x)-f(-x)}{2} = f(x)
        $$
        Vérifions si les fonctions $g$ et $h$ vérifient les conditions de parité:
        $$
        \forall x \in \mathbb{R}, g(-x) = \frac{f(-x)+f(-(-x))}{2}= \frac{f(x)+f(-x)}{2} = g(x) \text{ ainsi } g \text{ est paire.}
        $$
        $$
        \forall x \in \mathbb{R}, h(-x) = \frac{f(-x)-f(-(-x))}{2} = -\frac{f(x)-f(-x)}{2} = -h(x) \text{ ainsi } h \text{ est impaire}
        $$
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Illustration graphique de certaines identités trigonométriques}
    
\end{question_kholle}
\begin{question_kholle}{Technique de résolution des équations trigonométriques du type $A \cos x + B \sin x = C$}
    Étudions l'équation d'inconnue $x$
    
    $$
    A \cos x + B \sin x = C
    $$
    \begin{itemize}[label=$\star$]
        \item Si $A = 0$ et $B = 0$
        \begin{itemize}[label=$\lozenge$]
            \item Si $C = 0$ l'équation admet $\mathbb{R}$ pour ensemble de solutions
            \item Sinon, l'équation n'admet pas de solutions
        \end{itemize}
        \item Sinon,
        
        Factorisons par $\sqrt{ A^{2}+B^{2} }$ (ce qui a un sens car $(A, B) \neq (0, 0) \implies \sqrt{ A^{2}+ B^{2} } \neq 0$)
        
        $$
        \frac{A}{\sqrt{ A^{2}+B^{2} }}\cos x + \frac{B}{\sqrt{ A^{2}+B^{2} }} \sin x = \frac{C}{ \sqrt{ A^{2}+B^{2} }}
        $$
        Le nombre complexe $\frac{A}{\sqrt{ A^{2}+B^{2}} }+i \frac{B}{\sqrt{ A^{2} +B^{2}}}$ est de module $1$, donc $\exists \varphi \in \mathbb{R}$ tel que
        
        $$
        e^{i\varphi} = \underbrace{ \frac{A}{\sqrt{ A^{2}+B^{2}} } }_{ \cos \varphi }+i \underbrace{ \frac{B}{\sqrt{ A^{2} +B^{2}}} }_{ \sin \varphi }
        $$
        
        Ainsi,
        $$
        (\cos \varphi \cos x-\sin \varphi \sin x) = \frac{C}{ \sqrt{ A^{2}+B^{2} }}
        $$
        donc
        $$
        \cos(\varphi+x) = \frac{C}{\sqrt{ A^{2}+B^{2} }}
        $$
        \begin{itemize}[label=$\lozenge$]
            \item Si $\frac{C}{\sqrt{ A^{2}+B^{2} }}\leqslant 1$
            
            \begin{align*}
                \cos(\varphi+x) = \frac{C}{\sqrt{ A^{2}+B^{2} }} &\iff
                \left\{ \begin{array}{ll}
                    \phi + x \equiv \arccos\frac{C}{\sqrt{ A^{2}+B^{2} }}  [2\pi]\\
                    \text{ou} \\
                    \phi + x \equiv - \arccos\frac{C}{\sqrt{ A^{2}+B^{2} }} [2\pi]
                \end{array}\right. \\
                &\iff x \in \begin{array}{ll}\left\{ \arccos \frac{C}{\sqrt{ A^{2}+B^{2} }} + 2k\pi \mid k \in \mathbb{Z} \right\} \\ \cup \\ \left\{- \arccos \frac{C}{\sqrt{ A^{2}+B^{2} }} + 2k\pi \mid k \in \mathbb{Z} \right\} 
                \end{array}
            \end{align*}
            
            
            \item Sinon, l'équation n'admet aucune solution
        \end{itemize}
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Étude complète de la fonction tangente, tracé du graphe et en déduire celui de cotangente.}
    
\end{question_kholle}
\begin{question_kholle}{Expression $\sin \theta$, $\cos \theta$, $\tan \theta$ en fonction de $\tan \frac \theta 2$}
    Soit $\theta \in \mathbb{R} \setminus \pi \mathbb{Z}$.
    Posons $u = \tan \frac{\theta}{2}$
    \begin{itemize}[label=$\lozenge$]
        \item $\tan \theta = \frac{2u}{1-u^{2}}$
        
        En utilisant la formule classique de trigonométrie
        
        $$\tan (a+b) = \frac{\tan a+\tan b}{1-\tan a\tan b}$$
        
        On obtient, avec $(a,b) \leftarrow (\frac{\theta}{2}, \frac{\theta}{2})$
        $$
        \tan \theta = \frac{2 \tan \frac{\theta}{2}}{1-\tan ^{2} \frac{\theta}{2}}=\frac{2u}{1-u^{2}}
        $$
        
        
        \item $\cos \theta = \frac{1-u^{2}}{1+u^{2}}$
        \begin{align*}
            \cos \theta  & = 2\cos ^{2} \frac{\theta}{2} -1 \\
            & = \frac{2}{1+\tan ^{2} \frac{\theta}{2}} - 1 \\
            &= \frac{2}{1+u^{2}}-1 \\
            &= \frac{1-u^{2}}{1+u^{2}}
        \end{align*}
        
        \item $\sin \theta \frac{2u}{1+u^{2}}$
        
        \begin{align*}
            \sin \theta  & = \cos \theta \tan \theta \\
            & = \frac{1-u^{2}}{1+u^{2}} \times \frac{2u}{1-u^{2}} \\
            &= \frac{2u}{1+u^{2}}
        \end{align*}
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Preuve des formules du type $\cos p + \cos q = \dots$}
    Partons des formules d'addition
    
    $$
    \cos (a+b) = \cos a\cos b-\sin a\sin b\\
    $$
    $$
    \cos (a-b) = \cos a\cos b+\sin a\sin b
    $$
    
    \begin{align*}        
        \cos(a+b) + \cos (a-b) = 2\cos a\cos b 
        \tag{$\spadesuit$}\label{eqn:cos}
    \end{align*}
    
    
    Si bien qu'en posant
    $$
    \left\{ \begin{array}{ll}
        p = a+b \\
        q= a-b 
    \end{array}\right.
    \iff
    \left\{ \begin{array}{ll}
        a = \frac{p+q}{2} \\
        b= \frac{p-q}{2} 
    \end{array}\right.
    $$
    D'où, en injectant dans \eqref{eqn:cos}
    $$
    \cos p + \cos q = 2\cos \frac{p+q}{2} \cos \frac{p-q}{2}
    $$
    
\end{question_kholle}
\pagebreak\section{Semaine 2}

\begin{question_kholle}{Montrer qu'une composée d'applications inj/surj/bij est inj/surj/bij}
    Soient $u : E \to F$ et $v: F \to G$
    \begin{itemize}[label = $\lozenge$]
        \item Supposons $u$ et $v$ injectives.
        
        Soient $(x_{1}, x_{2}) \in E^{2}$ fq tels que $(v \circ u) (x_{1}) =(v\circ u)(x_{2})$.
        
        Alors $v(u(x_{1})) = v(u(x_{2}))$, mais $v$ est injective donc, $u(x_{1})= u(x_{2})$ mais $u$ est injective donc $x_{1} = x_{2}$.
        Ainsi $v \circ u$ est injective.
        \item Supposons $u$ et $v$ surjectives
        Soit $y \in G$ fixé quelconque.
        
        $v$ est surjective donc $\exists t \in F : v(t) = y$
        
        $u$ est surjective, donc $\exists x \in E : u(x) = t$
        Ainsi,
        $$
        (v \circ u) (x) = v(u(x)) = v(t) = y
        $$
        Donc $v \circ u$ est surjective.
        
        \item Supposons $u$ et $v$ bijectives.
        
        Le fait que $v \circ u$ est une bijection est une conséquence des deux points précédents.
        
    \end{itemize}
    
\end{question_kholle}
\begin{question_kholle}{Montrer que, si $u$ est une application de $E$ dans $F$, si $v$ est une application de $F$ dans $E$ telle que $v \circ u = \mathrm{Id}_E$ et $u \circ v = \mathrm{Id}_F$ alors $u$ est bijective ($v$ aussi) et sa bijection réciproque est $v$}
    Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, E)$ qui satisfont les conditions de l'énnoncé.
    \begin{itemize}[label=$\lozenge$]
        \item $u$ est injective
        
        Soient $(x_{1}, x_{2}) \in E^{2}$ fixés quelconques tels que $u(x_{1}) = u(x_{2})$. Alors $v(u(x_{1})) = v(u(x_{2}))$. Donc $x_{1} = x_{2}$ puisque $v \circ u = \mathrm{Id}_{E}$
        
        \item $u$ est surjective
        Soit $y \in F$ fixé quelconque. Posons $t = v(y)$. Ainsi, $u(t) = u(v(y))= y$ car $u \circ v = \mathrm{Id}_{F}$
        
        
    \end{itemize}
    Ainsi, $u$ est bijective, notons $u^{-1}$ sa bijection réciproque
    \begin{align*}
        u^{-1} \circ (u \circ v ) &= (u^{-1} \circ u) \circ v\\
        u^{-1} \circ \mathrm{Id}_{F} &= \mathrm{Id}_{E}\circ v\\
        u^{-1} &= v
    \end{align*}
    
\end{question_kholle}

\begin{question_kholle}{Montrer que $v \circ u$ injective implique $u$ injective + montrer que cela n'implique pas $v$ injective.}
    Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, G)$.
    
    Supposons $v \circ u$ est injective
    Soient $(x_1, x_2) \in E^2$ fixés quelconques tels que $u(x_1) = u(x_2)$.
    Composons par $v$ à gauche : $v \circ u ( x_1 ) = v \circ u (x_2)$
    Puisque $v \circ u$ est injective, cela implique que $x_1 = x_2$.
    
    
    \begin{figure}[!h]
        \centering
        \begin{tikzpicture}[ele/.style={fill=black,circle,minimum width=.8pt,inner sep=1pt},every fit/.style={ellipse,draw,inner sep=-2pt}]
            \node[label=above:$E$] at (0, 4.5) {};
            \node[label=above:$F$] at (4, 4.5) {};
            \node[label=above:$G$] at (8, 4.5) {};
            
            \node[label=above:$u$] at (2, 4) {};
            \node[label=above:$v$] at (6, 4) {};
            
            
            \node[ele,label=left:$a$] (a1) at (0,4) {};    
            \node[ele,label=left:$b$] (a2) at (0,3) {};    
            
            \node[ele,,label=below:$1$] (b1) at (4,4) {};
            \node[ele,,label=above:$2$] (b2) at (4,3) {};
            \node[ele,,label=above:$3$] (b3) at (4,2) {};
            
            \node[ele,,label=right:$\star$] (c1) at (8,4) {};
            \node[ele,,label=right:$\triangle$] (c2) at (8,3) {};
            
            \node[draw,fit= (a1) (a2) ,minimum width=2cm] {} ;
            \node[draw,fit= (b1) (b2) (b3) ,minimum width=2cm] {} ;  
            \node[draw,fit= (c1) (c2) ,minimum width=2cm] {} ; 
            
            \draw[->,thick,shorten <=2pt,shorten >=2pt] (a1) -- (b1);
            \draw[->,thick,shorten <=2pt,shorten >=2] (a2) -- (b2);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b1) -- (c1);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b2) -- (c2);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b3) -- (c2);
        \end{tikzpicture}
    \end{figure}
    Ici, $v \circ u$ est injective, on a montré que cela impliquait $u$ injective. Pourtant, $v$ n'est pas injective.
    
\end{question_kholle}

\begin{question_kholle}{Montrer que $v \circ u$ surjective implique $v$ surjective + montrer que cela n'implique pas $u$ surjective.}
    Soient $(u, v) \in \mathcal{F}(E, F) \times \mathcal{F}(F, G)$.
    
    Supposons $v \circ u$ est surjective
    Soit $y \in G$ fixé quelconque.
    Puisque $v \circ u$ est surjective, $\exists x \in E : (v \circ u)(x) = y$
    Donc $v(u(x)) = y$.
    Donc, en posant $t = u(x)$, on a $v(t) = y$.
    Ainsi, $v$ est surjective.
    
    \begin{figure}[!h]
        \centering
        \begin{tikzpicture}[ele/.style={fill=black,circle,minimum width=.8pt,inner sep=1pt},every fit/.style={ellipse,draw,inner sep=-2pt}]
            \node[label=above:$E$] at (0, 4.5) {};
            \node[label=above:$F$] at (4, 4.5) {};
            \node[label=above:$G$] at (8, 4.5) {};
            
            \node[label=above:$u$] at (2, 4) {};
            \node[label=above:$v$] at (6, 4) {};
            
            
            \node[ele,label=left:$a$] (a1) at (0,4) {};    
            \node[ele,label=left:$b$] (a2) at (0,3) {};    
            
            \node[ele,,label=below:$1$] (b1) at (4,4) {};
            \node[ele,,label=above:$2$] (b2) at (4,3) {};
            \node[ele,,label=above:$3$] (b3) at (4,2) {};
            
            \node[ele,,label=right:$\star$] (c1) at (8,4) {};
            \node[ele,,label=right:$\triangle$] (c2) at (8,3) {};
            
            \node[draw,fit= (a1) (a2) ,minimum width=2cm] {} ;
            \node[draw,fit= (b1) (b2) (b3) ,minimum width=2cm] {} ;  
            \node[draw,fit= (c1) (c2) ,minimum width=2cm] {} ; 
            
            \draw[->,thick,shorten <=2pt,shorten >=2pt] (a1) -- (b1);
            \draw[->,thick,shorten <=2pt,shorten >=2] (a2) -- (b2);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b1) -- (c1);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b2) -- (c2);
            \draw[->,thick,shorten <=2pt,shorten >=2] (b3) -- (c2);
        \end{tikzpicture}
    \end{figure}
    
    Ici, $v \circ u$ est surjective, on a montré que cela impliquait $v$ surjective. Pourtant, $u$ n'est pas surjective. 
    
\end{question_kholle}

\textbf{Remarque} : Les deux contre contre-exemples exhibés ici sont les mêmes, mais il y en a bien d'autres où $v \circ u$ n'est pas bijective.

\begin{question_kholle}{Soit $u$ une application de $E$ dans $F$. Si $A$ et $A'$ sont des parties de $E$, y'a-t-il égalité entre $u(A \cap A')$ et $u(A) \cap u(A')$? (On justifiera les réponses aux deux inclusions suggérées par la question)}
    Soit $u \in \mathcal{F}(E, F)$ fixée quelconque, $(A, A') \in \mathcal{P}(E)^{2}$, deux parties de $E$.
    \begin{itemize}[label=$\lozenge$]
        \item Soit $y \in u(A \cap A')$ fixé quelconque.
        Par définition $\exists x \in (A \cap A') : u(x) = y$.
        Ainsi, $x \in A \implies u(x) \in u(A)$
        $x \in A' \implies u(x) \in u(A')$
        
        $$
        \left.
        \begin{array}{ll}
            u(x) \in u(A) \\
            u(x) \in u(A')
        \end{array}\right\} \implies u(x) \in u(A) \cap u(A')
        $$
        Donc $u(A \cap A') \subset u(A) \cap u(A')$.
        
        \item En revanche l'inclusion réciproque est fausse: considérons
        $$
        u \left|\begin{array}{ll} \{ 1, 2, 3, 4 \} &\to \{ a, b, c, d \} \\ 1 &\mapsto a \\
            2  & \mapsto b  \\
            3  & \mapsto a \\
            4  & \mapsto d\end{array}\right.
            $$
            Si on choisit $A = \{ 1, 2 \}$ et $A' = \{ 2, 3 \}$.
            
            Alors, $u(A) = \{ a, b \}$, et $u(A') = \{ a, b \}$
            $u(A \cap A') = u(\{ 2 \}) = \{ b \}$
            
            et $u(A) \cap u(A') = \{ a, b \} \not\subset \{  b \}$
        \end{itemize}
    \end{question_kholle}
    \begin{question_kholle}{Montrer que, si $u$ est une application de $E$ dans $F$. Si $B$ est une partie de $F$, alors $u^{-1}(F\setminus B) = E \setminus u^{-1}(B)$.}
        Soit $x \in u^{-1}(F\setminus B)$. Raisonnons par équivalences. 
        
        \begin{align*}
            x \in u^{-1}(F \setminus B) 
            & \iff u(x) \in F \setminus B \\
            & \iff \mathrm{non} (u(x) \in B) \\
            & \iff \mathrm{non} (x \in u^{-1}(B)) \\
            & \iff x \in E \setminus u^{-1}(B)
        \end{align*}
        
    \end{question_kholle}
    \begin{question_kholle}{Montrer que, parmi les entiers ne s'écrivant qu'avec des 7, il existe au moins un multiple de 61.}
        
        Posons $s$ la suite des entiers tels que $\forall n \in \mathbb{N}^{*}, s_{n} = \underbrace{ 7\dots 7 }_{ n \text{ fois} }$.
        Considérons les $62$ premiers termes de la suite.
        
        Puisqu'il y a $61$ classes de congruences modulo $61$, le principe des tiroirs de Dirichlet nous permet d'affirmer que $\exists (k, l) \in [ \! [ 1, 62 ] \!]^{2}, k < l:s_{k} \equiv s_{l}[61]$.
        
        Remarquons maintenant que $s_{l}-s_{k} \equiv 0 [61]$, autrement dit que $61\mid s_{l} - s_{k}$.
        
        Cependant, $$
        s_{l} - s_{k} = \underbrace{ 7\dots7 }_{ l \text{ fois} } - \underbrace{ 7 \dots 7 }_{ k \text{ fois} } = \underbrace{ 7 \dots 7 }_{ l-k \text{ fois} } \underbrace{ 0 \dots 0 }_{ k \text{ fois} } = s_{l-k} \times 10^{k}
        $$
        Donc $61 \mid 10^{k} \times s_{l-k}$, mais $\mathrm{pgcd}(61, 10^{k}) = 1$ donc le théorème de Gauss donne $61\mid s_{l-k}$.
        
        Ainsi, parmi les entiers ne s'écrivant qu'avec des 7, il existe au moins un multiple de 61.
    \end{question_kholle}
\pagebreak\section{Semaine 3}

\begin{question_kholle}
    [{Pour tout $(z_{1}, z_{2})\in \mathbb{C}^{2}$,
    
    (i) $\lvert z_{1}+z_{2} \rvert \leqslant \lvert z_{1} \rvert + \lvert z_{2} \rvert$
    
    (ii) $\bigg| \lvert z_{1} \rvert-\lvert z_{2} \rvert \bigg|\leqslant \lvert z_{1}-z_{2} \rvert$
    
    }]
    {Preuve de l'inégalité triangulaire et de l'inégalité montrant que le module est 1-lipschitzien + dessin et interprétation géométrique}
    Soient $(z_1,z_2) \in \C^2$ fixés quelconques.
    \begin{itemize}[label=$\lozenge$]
        \item 
        
        Si $z_{2} = 0$ l'inégalité est évidente
        Sinon, $z_{2} \neq 0$ alors $\lvert z_{1}+z_{2} \rvert \leqslant \lvert z_{1} \rvert + \lvert z_{2} \rvert \iff \left| 1+\frac{z_{1}}{z_{2}} \right|\leqslant 1 + \left\lvert  \frac{z_{1}}{z_{2}}  \right\rvert$.
        Posons $u = \frac{z_{1}}{z_{2}}$
        
        \begin{align*}
            \lvert 1+u \rvert ^{2} - (1+\lvert u \rvert )^{2}  & = (1+u)(\overline{1+u})-(1+2\lvert u \rvert +\lvert u \rvert ^{2}) \\
            & = (1+u)(1+ \overline u) - 1 - 2\lvert u \rvert  - \lvert u \rvert ^{2} \\
            &= u + \overline u -2 \lvert u \rvert  \\
            &= 2(\mathrm{Re}(u)-u) \leqslant 0
        \end{align*}
        
        \item Appliquons l'inégalité triangulaire
        $$
        \lvert  z_{1} \rvert  = \lvert z_{1} - z_{2} + z_{2} \rvert  \leqslant \lvert z_{1}-z_{2} \rvert +\lvert z_{2} \rvert  \implies \lvert z_{1} \rvert - \lvert z_{2} \rvert \leqslant \lvert z_{1}-z_{2} \rvert 
        $$
        Puisque $z_{1}$ et $z_{2}$ jouent de rôles symétriques on a aussi
        $$
        \lvert z_{2} \rvert - \lvert z_{1} \rvert \leqslant \lvert z_{2}-z_{1} \rvert =\lvert z_{1}-z_{2} \rvert 
        $$
        Donc 
        $$
        \bigg| \lvert z_{1} \rvert-\lvert z_{2} \rvert \bigg|\leqslant \lvert z_{1}-z_{2} \rvert
        $$
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Caractérisation du cas d'égalité de l'inégalité triangulaire dans $\mathbb C$}
    \begin{itemize}[label=$\star$]
        \item ($\implies$) Supposons qu'il y ait égalité dans l'inégalité triangulaire
        \begin{itemize}[label=$\lozenge$]
            \item Si $z_{2} = 0$ alors $z_{1}$ et $z_{2}$ sont positivement liés
            \item Sinon $\lvert 1+u \rvert ^{2} - (1+\lvert u \rvert )^{2}  = 0$ donc $\mathrm{Re}(u) - \lvert u \rvert = 0$ Donc $u \in \mathbb{R}_{+}$ mais $z_{1} = uz_{2}$.
            Donc $z_{1}$ et $z_{2}$ sont positivement liés.
        \end{itemize}
        \item ($\impliedby$) Supposons que $z_{1}$ et $z_{2}$ sont positivement liés. Alors il existe $\lambda \in \mathbb{R}_{+}$ tel que $z_{1} = \lambda z_{2}$
        
        Si $z_{1} = \lambda z_{2}$
        $$
        \lvert z_{1}+z_{2} \rvert  = \lvert (\lambda+1) z_{2} \rvert  = \lvert \lambda + 1  \rvert \lvert z_{2} \rvert = (\lambda+1)\lvert z_{2} \rvert = \lambda \lvert z_{2} \rvert +\lvert z_{2} \rvert  = \lvert \lambda z_{2} \rvert +\lvert z_{2} \rvert = \lvert z_{1} \rvert + \lvert z_{2} \rvert 
        $$
        Donc l'inégalité est une égalité
        
        Si $z_{2} = \lambda z_{1}$, en échangeant les rôles joués par $z_{1}$ et $z_{2}$ on obtient que l'inégalité est une égalité.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Calcul de $\sum_{k=0}^{n}\cos(k\theta)$ pour tout $\theta \in \mathbb R$}
    Soit $\theta \in \mathbb{R}$ fixé quelconque, $n \in \mathbb{N}$ fixé quelconque.
    
    
    \begin{align*}
        C_{n}(\theta)=\sum_{k=0}^{n} \cos(k\theta) &= \sum_{k=0}^{n} \mathrm{Re}(e^{i k \theta}) \\
        &= \mathrm{Re} \left(\sum_{k=0}^{n} e^{i k \theta}\right) \\
        &= \mathrm{Re} \left( \sum_{k=0}^{n} (e^{i \theta})^{k} \right) \text{ par les formules de moivre} \\
    \end{align*}
    
    Ainsi, si $e^{i\theta} = 1 \iff \theta \equiv 0 [2\pi]$,
    $$
    C_{n}(\theta) = \mathrm{Re}\left( \sum_{k=0}^{n}(1)^{k} \right) = \mathrm{Re}(n+1) = n+1
    $$
    Sinon, 
    $$
    C_{n}(\theta) = \mathrm{Re}\left( \frac{1-(e^{i\theta})^{n+1}}{1-e^{i\theta}} \right)
    $$
    Simplifions donc ce quotient.
    
    \begin{align*}
        \frac{1-(e^{i\theta})^{n+1}}{1-e^{i\theta}} = \frac{1-e^{i\theta(n+1)}}{1-e^{i\theta}}
        &=\frac{e^{\frac{i\theta(n+1)}2{}}\left( e^{-\frac{i\theta(n+1)}2{}}-e^{\frac{i\theta(n+1)}2{}} \right)}{e^{i\frac{\theta}{2}}\left( e^{-i\frac{\theta}{2}}- e^{i \frac{\theta}{2}} \right)}\\
        &= e^{i \frac{\theta n}2}\left( \frac{-2i \sin\left( \frac{\theta (n+1)}{2} \right)}{-2i\sin \frac{\theta}{2}} \right) \\
        &= \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}}\left( \cos \frac{\theta n}{2} + i \sin \frac{\theta n}{2} \right) \tag{$\clubsuit$}\label{eqn:somme1}
    \end{align*}
    
    
    En prenant la partie réelle de ce résultat, on a
    $$
    C_{n}(\theta) = \mathrm{Re}\left[ \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}}\left( \cos \frac{\theta n}{2} + i \sin \frac{\theta n}{2} \right) \right] = \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \cos \frac{n\theta}{2}
    $$
    
    Donc
    $$
    C_{n}(\theta) = \left\{ \begin{array}{ll}
        n+1 & \text{ si } \theta \equiv 0 [2 \pi] \\
        \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \cos \frac{n\theta}{2}  & \text{ sinon}
    \end{array}\right.
    $$
\end{question_kholle}
\textbf{Remarque}
En prenant la partie imaginaire de \eqref{eqn:somme1}, on peut retrouver la somme $S_n(\theta)$:
$$
S_{n}(\theta)= \sum_{k=0}^n \sin(k \theta) = \left\{ \begin{array}{ll}
    0 & \text{ si } \theta \equiv 0 [2 \pi] \\
    \frac{\sin\left( \frac{\theta(n+1)}{2} \right)}{\sin \frac{\theta}{2}} \sin \frac{n\theta}{2}  & \text{ sinon}
\end{array}\right.
$$
\begin{question_kholle}
    [{Soient $n \in \mathbb{N}, (a_{0}, \dots, a_{n})\in\mathbb{C}^{n+1}$ et $z_{0} \in \mathbb{C}$ Posons pour tout $z \in \mathbb{C}, P(z) = \sum_{k=0}^{n}a_{k}z^{k}$
    
    (i) Si $P(z_{0}) = 0$, alors $\exists Q \in\mathbb{C}[z]:\forall z \in \mathbb{C}, P(z)=(z-z_{0})Q(z)$}]
    {Si $z_0$ est racine de la fonction polynômiale $P$, alors $P$ se factorise par $(z-z_0)$}
    Soit $z \in \mathbb{C}$ fixé quelconque,
    
    \begin{align*}
        P(z)  & = P(z) - P(z_{0}) \\
        & = \sum_{k=0}^{n}a_{k}z^{k} - \sum_{k=0}^{n}a_{k}z_{0}^{k}  \\
        & = \sum_{k=0}^{n}a_{k}(z^{k}-z_{0}^{k}) &\text{ nul pour }k = 0\\
        & = \sum_{k=1}^{n}\left( a_{k}(z-z_{0})\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right) \right) \\
        & = (z-z_{0}) \sum_{k=1}^{n}a_{k}\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right)
    \end{align*}
    
    Donc en posant $Q(z) = \sum_{k=1}^{n}a_{k}\left( \sum _{j=0}^{k-1}z^{j}z_{0}^{k-1-j} \right), \in \mathbb{C}[z]$, on a montré que $P$ se factorise.
\end{question_kholle}
\begin{question_kholle}
    [{Soient $n \in \mathbb{N}, (a_{0}, \dots, a_{n})\in\mathbb{C}^{n+1}$ et $z_{0} \in \mathbb{C}$ Posons pour tout $z \in \mathbb{C}, P(z) = \sum_{k=0}^{n}a_{k}z^{k}$
    
    (ii) Si $\exists p \in \mathbb{N}^{*}: \exists(z_{1},\dots ,z_{p})\in\mathbb{C}^{p}$ deux à deux distincts tels que $\forall k \in [ \! [ 1, p ] \!], P(z_{k}) = 0$ alors, $\exists Q \in \mathbb{C}[x]:\forall z \in \mathbb{C}, P(z) = Q(z) \times \prod_{k=1}^{p}(z-z_{k})$.
    }]
    {Si $z_1, \dots , z_n$ sont $n$ racines distinctes de la fonction polynômiale $P$ de degré $n$, alors $P(z)$ se factorise en ... }
    Considérons la propriété $\mathcal{P}(\cdot)$ définie pour tout $p \in \mathbb{N}^{*}$ par
    $$
    \mathcal{P}(p) : \forall P \in \mathbb{C}[z], (\exists (z_{1}, \dots, z_{p}) \in \mathbb{C}^{p}, \text{ 2 à 2 distincts }: \forall i \in [ \! [ 1, p ] \!], P(z_{i}) = 0)\\
    \implies \exists Q \in \mathbb{C}[z]: P(z) = Q(z) \prod_{i=1}^{p}(z-z_{i})
    $$
    \begin{itemize}[label=$\lozenge$]
        \item $\mathcal{P}(1)$ est vraie d'après la preuve précédente.
        \item Soit $p \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{P}(p)$ est vraie.
        Soit $P \in \mathbb{C}[z]$ fq tq $\exists (z_{1}, \dots, z_{p+1}) \in \mathbb{C} ^{p+1}$ deux à deux distincts tels que $\forall i \in [ \! [ 1, p + 1 ] \!], P(z_{i}) = 0$.
        Appliquons $\mathcal{P}(p)$ à $P \in \mathbb{C}[z]$ dont $(z_{1}, \dots, z_{p})$ sont les $p$ racines deux à deux distinctes.
        
        $$\exists Q_{1} \in \mathbb{C}[z]:\forall z \in \mathbb{C}, P(z) = Q_{1}(z)\prod_{i=1}^{p}(z-z_{i})$$
        Évaluons cette expression en $z_{p+1}$
        $$\underbrace{ P(z_{p+1}) }_{ =0 } = Q_{1}(z_{p+1}) \prod_{i=1}^{p}\underbrace{ (z_{p+1}-z_{i}) }_{ \neq 0 \text{ car distincts} }$$
        Donc $Q_{1}(z_{p+1}) = 0$, ce qui permet d'appliquer (i) pour $P \leftarrow Q_{1}$, $z_{0} \leftarrow z_{p+1}$.
        $$
        \exists Q \in \mathbb{C}[z]:\forall z \in \mathbb{C}, Q_{1}(z)=(z-z_{p+1})Q(z)
        $$
        Donc
        $$
        \forall z \in \mathbb{C}, P(z) = (z-z_{p+1})Q(z) \prod_{i=1}^{p}(z-z_{i}) = Q(z) \prod_{i=1}^{p+1}(z-z_{i})
        $$
        Donc $\mathcal{P}(p+1)$ est vraie.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Calculer le module et un argument de $z=1+ e^{i \theta}$ en fonction de $\theta \in [0, 2 \pi[$}
    Soit $\theta \in [0, 2\pi[$
    $$
    z = 1+ e^{i \theta} = e^{i\times0}+e^{i\theta} = e^{i \frac{\theta}{2}}\left( e^{-i\frac{\theta}{2}} + e^{i \frac{\theta}{2}} \right)= 2 \cos \frac{\theta}{2}e^{i\frac{\theta}{2}}
    $$
    Cette dernière notation est une notation exponentielle seulement si $2 \cos \frac{\theta}{2} \geqslant 0$.
    \begin{itemize}[label=$\star$]
        \item Si $\theta \in [0, \pi[$,
        $$\left\{ \begin{array}{ll}
            |z| = 2\cos \frac{\theta}{2} \\
            \frac{\theta}{2} \in \mathrm{Arg}  (z)
        \end{array}\right.$$
        \item Si $\theta = \pi$, $z = 0$ donc $\lvert z \rvert = 0$
        
        \item Si $\theta \in ]\pi, 2\pi[$,
        
        \begin{align*}
            z = 2 \cos \frac{\theta}{2}e^{i\frac{\theta}{2}} &= -2 \left\lvert  \cos \frac{\theta}{2}  \right\rvert e^{i\frac{\theta}{2}} \\
            & =-2 \left\lvert  \cos \frac{\theta}{2}  \right\rvert e^{i \left( \frac{\theta}{2} + \pi \right)}
        \end{align*}
        
        Donc 
        $$
        \left\{ \begin{array}{ll}
            |z| = -2 \lvert  \cos \frac{\theta}{2}  \rvert \\
            \frac{\theta}{2} + \pi \in \mathrm{Arg}  (z)
        \end{array}\right.
        $$
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{
    Pour tout $n \in \mathbb N ^{*} \setminus \{ 1 \}$, 
    $$
    \mathbb U _n = \left\{ e^{\frac{2ik\pi}{n}}\mid k \in [ \! [ 0, n - 1 ] \!] \right\} 
    $$
    }]{Décrire (avec preuve) l'ensemble des racines $n$-ièmes de l'unité et les localiser géométriquement dans le plan complexe.}
    \begin{itemize}
        \item \underline{Description de l'ensemble $\mathbb U _n$}
        \begin{align*}
            \left\{ \begin{array}{ll}
                z^{n}=1 \\
                z \in \mathbb{C} 
            \end{array}\right.
            &\iff 
            \left\{ \begin{array}{ll}
                z^{n} = 1 \\
                z \in \mathbb{C}^{*}
            \end{array}\right. \text{ ou } 
            \left\{ \begin{array}{ll}
                z^{n} = 1 \\
                z = 0 
            \end{array}\right. \\
            & \iff 
            \left\{ \begin{array}{ll}
                \rho^{n}e^{in \theta}  = 1 \\
                z = \rho e^{i \theta} \\
                (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
            \end{array}\right.
            \\ & \iff 
            \left\{ \begin{array}{ll} 
                \rho^{n} = 1 \\
                n \theta \equiv 0 [2 \pi]\\
                z = \rho e^{i \theta} \\
                (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
            \end{array}\right. 
            \\ & \iff 
            \left\{ \begin{array}{ll} 
                \rho = 1 \text{ car } \rho > 0 \\
                \theta \equiv 0 \left[ \frac{2\pi}{n} \right]\\
                z = \rho e^{i \theta} \\
                (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
            \end{array}\right.
            \\ & \iff 
            \left\{ \begin{array}{ll} 
                \rho = 1 \\
                \exists k \in \mathbb{Z} : \theta = \frac{2k\pi}{n} \\
                z = \rho e^{i \theta} \\
                (\rho, \theta) \in \mathbb{R}_{+}^{*}\times \mathbb{R}
            \end{array}\right.
            \\  & \iff \exists k \in \mathbb{Z}: z=e^{\frac{2ik\pi}{n}} \\
            & \iff z \in \left\{ e^{\frac{2ik\pi}{n}}\mid k \in \mathbb{Z} \right\} 
        \end{align*}
        
        
        L'ensemble des solutions est paramétré par l'entier $k$ qui parcourt un ensemble infini. Toutefois, en représentant graphiquement les solutions, il semblerait que "tous les $n$", on fait un tour de cercle trigonométrique de plus, en redécrivant les solutions déjà obtenues pour $k \in [ \! [ 0, n-1 ] \!]$.
        \item \underline{Localisation géométrique}
        \begin{itemize}[label=$\star$]
            \item $\mathbb U _3$ est l'ensemble des sommets du triangle équilatéral inscrit dans le cercle unité, et dont $1$ est l'un des sommets
            \begin{figure}[!h]
                \centering
                \begin{tikzpicture}[
                    dot/.style={draw,fill,circle,inner sep=1pt}
                    ]
                    \draw[->] (-2,0) -- (2,0) node[below] {$\Re$};
                    \draw[->] (0,-2) -- (0,2) node[left] {$\Im$};
                    \draw[help lines] (0,0) circle (1);
                    
                    \node[dot,label={below right:$O$}] (O) at (0,0) {};
                    \foreach \i in {1,...,3} {
                    \node[dot,label={\i*360/3-(\i==3)*45:$e^\frac{2 \times {\i} i \pi}{3}$}] (w\i) at (\i*360/3:1) {};
                    \draw[->] (O) -- (w\i);
                    }
                    \draw[->] (0:.3) arc (0:360/3:.3);
                    \node at (360/3/2:.5) {$\alpha$};
                \end{tikzpicture}
            \end{figure}
            
            \item $\mathbb U _4$ est l'ensemble des sommets du carré inscrit dans le cercle unité et dont 1 est l'un des sommets. Le côté du carré vaut $\lvert 1 - i\rvert = \sqrt 2$.
            \item $\mathbb U _5$ est l'ensemble des sommets du pentagone régulier inscrit dans le cercle unité et dont 1 est l'un des sommets.
            \begin{figure}[!h]
                \centering
                \begin{tikzpicture}[
                    dot/.style={draw,fill,circle,inner sep=1pt}
                    ]
                    \draw[->] (-2,0) -- (2,0) node[below] {$\Re$};
                    \draw[->] (0,-2) -- (0,2) node[left] {$\Im$};
                    \draw[help lines] (0,0) circle (1);
                    
                    \node[dot,label={below right:$O$}] (O) at (0,0) {};
                    \foreach \i in {1,...,5} {
                    \node[dot,label={\i*360/5-(\i==5)*45:$e^\frac{2 \times {\i} i \pi}{5}$}] (w\i) at (\i*360/5:1) {};
                    \draw[->] (O) -- (w\i);
                    }
                    \draw[->] (0:.3) arc (0:360/5:.3);
                    \node at (360/5/2:.5) {$\alpha$};
                \end{tikzpicture}
            \end{figure}
        \end{itemize}
    \end{itemize}
    
\end{question_kholle}
\begin{question_kholle}{Somme et Produit des racines $n$-ièmes}
    \begin{itemize}[label=$\lozenge$]
        \item \textbf{Méthode 1} En utilisant les relations coefficients racines.

        $\mathbb{U}_{n}$ sont les n racines disctinctes de $z^{n}-1$
        $$S_{n} = - \frac{1}{\text{coefficient dominant}}\times(\text{coefficient de }z^{n-1} \text{ dans }z^{n}-1)= \left\{ \begin{array}{ll}
            -0  & \text{ si }  n\geqslant 2 \\
            -(-1)   & \text{ sinon}
        \end{array}\right.$$
        $$
        P_{n} = (-1)^{n} \frac{\text{coefficient constant}}{\text{coefficient dominant}} = (-1) ^{n}\times \frac{-1}{1} = (-1)^{n+1}
        $$
        
        \item \textbf{Méthode 2} Manipulation des symboles sommatoires
        
        \begin{align*}
            S_{n} = \sum_{\omega \in \mathbb{U}_{n}}\omega &= \sum_{k=0}^{n-1}\omega_{0}^{k}\\
            &= \left\{ \begin{array}{ll}
                1 & \text{si }  n =1 \\
                1 \times \frac{1 - \omega_{0}^{n}}{1-\omega_{0}}  & \text{sinon}
            \end{array}\right.
        \end{align*}
        
        Puisqu'on ne peut appliquer la formule de la somme des termes d'une suite géométrique seulement si la raison $\omega_{0} = 1 \iff e^{\frac{2i\pi}{n}} = 1 \iff \frac{2\pi}{n} \equiv 0 [2\pi] \iff n = 1$
        
        De même
        
        \begin{align*}
            P_{n} = \prod_{\omega \in \mathbb{U}_{n}}\omega = \prod_{k=0}^{n-1}\omega_{0}^{k}= \omega_{0}^{\sum_{k=0}^{n-1}k} &= \omega_{0} ^{\frac{n(n-1)}{2}}\\ \\
            &=\left\{ \begin{array}{ll}
                (\omega_{0}^{n})^{\frac{n-1}{2}} = 1^{\frac{n-1}{2}} = 1 & \text{si } n \equiv 1 [2] \\
                e^{\frac{2i\pi n(n-1)}{2n}} = e^{i \pi(n-1)} = (-1)^{n-1}
            \end{array}\right.
            \\ &= (-1)^{n-1}
        \end{align*}
    \end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 4}
\begin{question_kholle}[{Considérons l'équation algébrique de degré 2:
    $$az^{2}+bz+c=0$$
    Où $z\in L$ est l'inconnue et $(a,b,c)\in\mathbb{C}^*\times\mathbb{C}^2$ sont des paramètres. Posons $\Delta = b^{2}-4ac$ que l'on appelle le discriminant de l'équation.
    \begin{itemize}
        \item Si $\Delta=0$, l'équation admet une unique solution dite double qui est $-\frac b{2a}$ et la forme factorisée du trinôme est
        $$
        az^2+bz+c = a \left( z + \frac b {2a} \right)^2
        $$
        \item Si $\Delta\neq0$, notons $\delta$ une racine carrée de $\Delta$, l'équation admet deux solutions distinctes $\frac{-b-\delta}{2a}$ et $\frac{-b+\delta}{2a}$ dites simples et la forme factorisée du trinôme est
        $$
        az^2+bz+c = a\left(z - \frac{-b-\delta}{2a}\right)\left(z - \frac{-b+\delta}{2a}\right)
        $$
    \end{itemize}
    }]{Résolution des équations algébriques de degré $2$ dans $\C$ et algorithme de recherche d'une racine carrée sous forme cartésienne (sur un exemple explicite).}
    La preuve est immédiate à partir de la forme canonique du trinôme du second degré :
    
    La preuve est immédiate à partir de la forme canonique du trinôme du second degré :
    
    \begin{align*}
        az^{2}+bz+c = a\left[ \underbrace{ z^{2}+\frac{b}{a}z }_{ \text{But: Absorber ces termes dans un carré} }+\frac{c}{a} \right] &= a\left[ z^{2}+2\frac{b}{2a}z +\frac{b^{2}}{4a^{2}}-\frac{b^{2}}{4a^{2}}+\frac{c}{a} \right]\\
        &=a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{b^{2}}{4a^{2}}+\frac{c}{a} \right]  \\
        &=a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{b^{2}-4ac}{4a^{2}}\right]  \\
        &=a \left[ \left( z+\frac{b}{2a} \right)^{2} - \frac{\Delta}{4a^{2}}\right] 
    \end{align*}
    \begin{itemize}
        \item Si $\Delta = 0$
        $$
        az^{2}+bz+c = a\left( z-\frac{-b}{2a} \right)^{2}
        $$
        de sorte que 
        $$
        az^{2}+bz+c = 0 \iff a\left( z-\frac{-b}{2a} \right)^{2} = 0 \iff z = -\frac{b}{2a}
        $$
        
        \item Sinon
        \begin{align*}
            az^{2}+bz+c = a \left[ \left( z+\frac{b}{2a} \right)^{2}-\left( \frac{\delta}{2a} \right)^{2} \right] &= a \left( z + \frac{b}{2a}-\frac{\delta}{2a} \right)\left( z+\frac{b}{2a}+\frac{\delta}{2a} \right)\\ &= a \left( z - \frac{-z+\delta}{2a} \right)\left( z- \frac{-z - \delta}{2a} \right)
        \end{align*}
        de sorte que
        
        \begin{align*}
            az^{2}+bz+c =0 &\iff a \left( z - \frac{-z+\delta}{2a} \right)\left( z- \frac{-z - \delta}{2a} \right) = 0\\ \\
            &\iff \left\{ \begin{array}{ll}
                z -  \frac{-z-\delta}{2a}  = 0 \\
                \text{ou} \\
                z- \frac{-z+\delta}{2a} =0
            \end{array}\right. \\
            &\iff \left\{ \begin{array}{ll}
                z = \frac{-z-\delta}{2a}  \\ \text{ou} \\
                z = \frac{-z+\delta}{2a} 
            \end{array}\right.
        \end{align*}
        
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}
    [{L'exponentielle complexe a pour image $\mathbb{C}^{*}$ et, pour tout $z_{0} \in \mathbb{C}^{*}$,
    $$
    \exp_{\mathbb{C}}^{-1}(\{ z_{0} \}) = \{ \ln \lvert z_{0} \rvert +i\theta_{0} +2ik\pi \mid k \in \mathbb{Z}\}
    $$
    où $\theta_{0} \in \arg(z_{0})$}]{Résolution de $e^z = z_0$ où $z_0 \in \C^*$}
    
    La propriété, pour tout $z \in \mathbb{C}$, $\lvert e^{z} \rvert = \lvert e^{\mathrm{Re}(z)} \rvert>0$ montre que $0 \not\in \exp_{\mathbb{C}}(\mathbb{C})$.
    
    $z_{0}\neq 0$ donc $\exists \theta_{0} \in \arg(z_{0}): z_{0} = \lvert z_{0} \rvert e^{i\theta_{0}}$.
    Résolvons l'équation d'inconnue $z \in \mathbb{C}$
    
    \begin{align*}
        \exp_{\mathbb{C}}(z) = z_{0}  & \iff e^{\mathrm{Re}(z)}e^{i\mathrm{Im}(z)} = \lvert z_{0} \rvert e^{i\theta_{0}} \\
        & \iff \left\{ \begin{array}{ll}
            e^{\mathrm{Re}(z)} = \lvert z_{0} \rvert  \\
            \text{et} \\
            \mathrm{Im}(z) \equiv \theta_{0} [2\pi]
        \end{array}\right. \\
        & \iff \left\{ \begin{array}{ll}
            \mathrm{Re}(z)  = \ln \lvert  z_{0} \rvert  \\
            \text{et} \\
            \mathrm{Im}(z) \equiv \theta_{0} [2\pi]
        \end{array}\right. \\
        &\iff z \in \left\{ \ln \lvert z_{0} \rvert +i \theta_{0} +2ik\pi \mid k \in \mathbb{Z} \right\} 
    \end{align*}
    
\end{question_kholle}
\begin{question_kholle}{Montrer l'unicité de l'élément neutre et du symétrique d'un élément sous des hypothèses sur la loi à préciser.}
    \begin{itemize}[label=$\lozenge$]
        \item Unicité de l'élément neutre bilatère
        
        Soient $(e_{1}, e_{2}) \in E^{2}$ fixés quelconques tels que $\left\{ \begin{array}{ll} \forall x \in E, x * e_{1} = e_{1} * x = x \\ \forall x \in E, x*e_{2}=e_{2}*x = x\end{array}\right.$
        Particularisons la première relation pour $x \leftarrow e_{2}$:
        $$
        e_{2}*e_{1} = e_{1}*e_{2} = e_{2}
        $$
        En particularisant, de même la deuxième relation pour $x \leftarrow e_{1}$
        $$
        e_{1}*e_{2} = e_{2}*e_{1} = e_{2}
        $$
        D'où, par transitivité de l'égalité : $e_{1} = e_{2}$
        
        \item Unicité du symétrique sous réserve d'existence (LCI associative d'unité $e$)
        Soit $a \in E$ symétrisable
        $$
        \exists z \in E : a * z = z*a = e
        $$
        Fixons un tel $z$ pour la suite de la preuve
        \begin{itemize}
            \item L'ensemble $\{ y \in E \mid a * y = y * a = e \}$ n'est pas vide puisqu'il contient $z$.
            
            \item Soit $b \in \{ y \in E \mid a * y = y * a = e \}$ fixé quelconque.
            Alors
            \begin{align*}
                a * b = e &\implies z * ( a * b ) = z * e\\
                &\implies \underbrace{ z*a }_{ e } * b = z * e \text{ par associativité}\\
                &\implies b = z
            \end{align*}
        \end{itemize}
        Donc l'ensemble $\{ y \in E \mid a * y = y * a = e \}$ contient au plus un élément neutre, qui est $z$.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{Soit $(G, *)$ un groupe, et $H$ une partie de $G$ $$
    H\text{ est un sous groupe de }G \iff \left\{ \begin{array}{ll}
        H \neq \emptyset \\
        \forall (x, y) \in H^{2}, x * y^{-1} \in H
    \end{array}\right.
    $$}]{Preuve de la caractérisation d'un sous-groupe, application au fait que $(\mathbb U _n, \times)$ est un sous-groupe de $(\mathbb U, \times)$.}
    \begin{itemize}[label=$\star$]
        \item Supposons que $H$ est un sous groupe de $G$
        Par définition d'un sous groupe, $H \neq \emptyset$.
        
        Soient $(x, y) \in H^{2}$ fixés quelconques
        $H$ est un sous groupe, donc $y$ est symétrisable dans $H$ : $y^{-1} \in H$
        De plus, c'est un groupe, donc stable pour la loi $*$, donc $x*y^{-1} \in H$
        
        \item Supposons que $\left\{ \begin{array}{ll}H \neq \emptyset &(1) \\\forall (x, y) \in H^{2}, x * y^{-1} \in H & (2)\end{array}\right.$
        \begin{itemize}[label=$\lozenge$]
            \item $H$ est non vide par hypothèse
            \item Puisque $H \neq \emptyset$, $\exists x \in H$
            Ainsi, $x * x^{-1} \in H$ donc $e \in H$
            \item Soient $(x, y) \in H^{2}$ fixés quelconques.
            $y \in H$ permet d'appliquer (2) pour $x \leftarrow y$ et $y \leftarrow e$:
            $$
            e * y^{-1} \in H
            $$
            Donc $y^{-1} \in H$. Tout élément est symétrisable dans $H$
            \item Soient $(x, y) \in H^{2}$ fixés quelconques. On a montré que $y$ est symétrisable dans H, donc en appliquant (2) pour $x \leftarrow x$ et $y \leftarrow y^{-1}$:
            $$
            x * (y^{-1})^{-1} \in H \implies x* y \in H
            $$
            Donc $H$ est stable pour la loi $H$.
        \end{itemize}
        Donc $H$ est un sous groupe de $G$.
    \end{itemize}
\end{question_kholle}
\textbf{Application aux racines n-ièmes de l'unité}

Soit $n \in \mathbb{N}^{*}$ fixé quelconque
\begin{itemize}[label=$\star$]
    \item $\forall z \in \mathbb{U}_{n}, z^{n} = 1$ donc $1 = \lvert  z^{n} \rvert = \lvert z \rvert^{n}$. Or $\lvert z \rvert \geqslant 0$ donc $\lvert z \rvert = 1$, si bien que $\mathbb{U}_{n} \subset \mathbb{U}$
    
    \item $\mathbb{U}_{n} \neq \emptyset$ car $1 \in \mathbb{U}_{n}$
    
    \item Soient $(z_{1}, z_{2}) \in \mathbb{U}_{n}$ fixés quelconques.
    Calculons
    $$
    (z_{1}z_{2}^{-1})^{n} = \left( \frac{z_{1}}{z_{2}} \right)^{n} = \frac{z_{1}^{n}}{z_{2}^{n}} = \frac{1}{1} = 1
    $$
\end{itemize}
Donc $z_{1}z_{2}^{-1} \in \mathbb{U}_{n}$. On a donc montré que $(\mathbb{U}_{n, \times})$ est un sous groupe de $(\mathbb{U}, \times)$.

\begin{question_kholle}{Si $\varphi$ est un morphisme de groupes de $G_1$ de neutre $e_1$ dans $G_2$ de neutre $e_2$, calculer $\varphi(e_1)$ et $\varphi(x^{-1})$}
    \begin{itemize}[label=$\star$]
        \item Soit $f$ un morphisme de groupe de $(G_{1}, *_{1})$ dans $(G_{2}, *_{2})$
        
        D'une part $f(e_{1} *_{1} e_{1}) = f(e_{1})$
        D'autre part, par propriété de morphisme, $f(e_{1} *_{1} e_{1}) =f(e_{1})*_{2} f(e_{1})$
        Donc
        $$
        f(e_{1}) *_{2} f(e_{1}) = f(e_{1})
        $$
        Si l'on compose à gauche par $f(e_{1})^{-1}$
        
        \begin{align*}
            f(e_{1})^{-1} *_{2} f(e_{1}) *_{2} f(e_{1}) &= f(e_{1})^{-1} *_{2}f(e_{1}) \\
            \implies f(e_{1}) &= e_{2}
        \end{align*}
        
        
        \item Soit $x \in G_{1}$ fixé quelconque
        $$
        f(x)*_{2}f(x^{-1}) = f(x*_{1}x^{-1})=f(e_{1}) = e_{2}
        $$
        Composons les deux membres à gauche par $f(x)^{-1}$
        $$
        f(x)^{-1} *_{2} f(x)*_{2}f(x^{-1}) = f(x)^{-1} *_{2} * e_{2}
        $$
        Donc
        $$
        f(x^{-1}) = f(x)^{-1}
        $$
        
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer que l'image d'un sous-groupe par un morphisme de groupes est un sous-groupe du groupe d'arrivée}
    Soit $f$ un morphisme de groupe de $(G_{1},*_{1})$ dans $(G_{2}, *_{2})$. Notons $e_{1}$ et $e_{2}$ les neutres respectifs de $G_{1}$ et $G_{2}$.
    
    Soit $H_{1}$ un sous groupe de $G_{1}$ fixé quelconque
    \begin{itemize}[label=$\star$]
        \item $f(H_{1})$ est par définition une partie de $G_{2}$
        \item $f(H_{1}) \neq \emptyset$ car $H_{1}$ est un groupe, qui contient $e_{1}$ $f(e_{1}) = e_{2}$ donc $e_{2} \in f(H_{1})$
        \item Soient $(g_{2},h_{2}) \in f(H_{1})^{2}$ fixés quelconques
        alors $\exists (g_{1}, h_{1}) \in H_{1} : f(g_{1})= g_{2}, f(h_{1})=h_{2}$
        donc
        $$
        g_{2}*_{2}h_{2}^{-1} = f(g_{1}) *_{2} f(h_{1}^{-1}) = f(\underbrace{ g_{1} *_{1} h_{1}^{-1} }_{ \in H_{1} \text{ car sous groupe de }G_{1} })
        $$
        Donc $g_{2} *_{2} h_{2}^{-1} \in f(H_{1})$
        Donc $f(H_{1})$ est un sous groupe de $G_{2}$
    \end{itemize}
\end{question_kholle}

\begin{question_kholle}{Montrer que l'image réciproque par un morphisme de groupes d'un sous-groupe est toujours un sous-groupe du groupe de départ, }
    Soit $f$ un morphisme de groupe de $(G_{1},*_{1})$ dans $(G_{2}, *_{2})$. Notons $e_{1}$ et $e_{2}$ les neutres respectifs de $G_{1}$ et $G_{2}$.
    
    Soit $H_{2}$ un sous groupe de $G_{2}$ fixé quelconque
    \begin{itemize}[label=$\star$]
        \item $f^{-1}(H_{2})$ est par définition une partie de $G_{1}$
        \item $f(H_{2}) \neq \emptyset$ car $H_{2}$ est un groupe, qui contient $e_{2}$ $f(e_{1}) = e_{2}$ donc $e_{1} \in f^{-1}(H_{2})$
        \item Soient $(g_{1},h_{1}) \in f^{-1}(H_{2})^{2}$ fixés quelconques
        alors $f(g_{1}) \in H_{2}$ et $f(h_{1}) \in H_{2}$
        donc
        $$
        f(g_{1} *_{1} h_{1}^{-1}) = \underbrace{ f(g_{1}) }_{ \in H_{2} }*_{2}\underbrace{ f(h_{1})^{-1} }_{ \in H_{2} } \in H_{2} \text{ car c'est un sous groupe}$$
        Donc $f(g_{1} *_{1} h_{1}^{-1}) \in H_{2}$ donc $g_{1} *_{1} h_{1}^{-1} \in f^{-1}(H_{2})$
    \end{itemize}
    Donc $f^{-1}(H_{2})$ est un sous groupe de $G_{1}$
\end{question_kholle}
    
\textbf{Application 1:} Le noyau d'un morphisme est un sous-groupe

Le noyau, noté $\ker f$ est par définition égal à $f^{-1}(\{e_2\})$ c'est donc un sous groupe de $G_1$

\textbf{Application 2:} Pour tout $n \in \mathbb{N}$, $\phi _{n}\left|\begin{array}{ll} (\mathbb{C}^{*},\times) &\to (\mathbb{C}^{*},\times) \\ z &\mapsto z^{n} \end{array}\right.$ est un morphisme de groupes. Son noyau, $\ker \phi _n = \{ z \in \mathbb{C}^{*} | z^{n} = 1 \} = \mathbb{U}_{n}$. D'après l'application 1, $\ker \phi_{n}$ est un sous groupe de $(\mathbb{C}^{*}, \times)$. Donc $(\mathbb{U}_{n}, \times)$ est un sous groupe de $(\mathbb{C}^{*},\times)$.
\pagebreak\section{Semaine 5}

\pagebreak\section{Semaine 6}


\begin{question_kholle}{Montrer que si $f$ est impaire et bijective, alors $f^{-1}$ est aussi impaire. Donnez un/des exemples.} 
	Soit $f: I \to F$, avec $I,F$ deux parties non-vides de $\mathbb{R}$, une telle fonction et notons $f^{-1}$ sa bijection réciproque. Si $f$ est impaire sur $I$, alors pour tout $x\in I$, $-x\in I$, ainsi $I$ est centré en $0$ et on a : 
	
	\begin{equation*}
	    \forall x \in I, \ f(-x) = -f(x).
	\end{equation*}
	
	Ainsi, prenons $y\in F$, alors $-y \in F$ par imparité et bijectivité de $f$. On a donc : 
	
	\begin{eqnarray*}
		f^{-1}(-y) & = & f^{-1}(-f(f^{-1}(y))) \\
			& = & f^{-1}(f(-f^{-1}(y))) \\
		    & = & -f^{-1}(y).
	\end{eqnarray*}
	
	\
	
	D'où l'imparité de $f^{-1}$.
	
	\
	
	Pour ce qui est de l'exemple, prenons notre fonction bijective impaire préférée, la fonction $\textstyle \sin |_{\left[ -\frac{\pi}{2}, \frac{\pi}{2}\right] }^{[-1,1]}$ que l'on notera $\widetilde{\sin}$. Sa bijection réciproque est bien entendu $\textstyle \arcsin : [-1,1] \to \left[ -\frac{\pi}{2}, \frac{\pi}{2}\right]$.
	
	De la même manière que dans la démonstration du cas général, prenons $y\in [-1, 1]$, comme $[-1,1]$ est centré en $0$, $-y\in [-1,1]$, on a dès lors : 
	
	\begin{eqnarray*}
		\arcsin(-y) & = & \arcsin(-\widetilde{\sin}(\arcsin(y))) \\
			& = & \arcsin(\widetilde{\sin}(-\arcsin(y))) \\
		    & = & -\arcsin(y).
	\end{eqnarray*}
\end{question_kholle}

\begin{question_kholle}{Limite (et preuve) lorsque $x$ tend vers $+\infty$ de $\frac{(\ln x)^{\alpha}}{x^{\beta}}$ pour $\alpha ,\beta \in \left( \mathbb{R}_+^*\right) ^2$.} 

	Premièrement, posons : 
	
	\begin{equation*}
	    \forall  (x,\alpha,\beta)\in [1,+\infty[ \times \left( \mathbb{R}_+^*\right) ^2, \quad  f_{\alpha,\beta}(x)=\frac{(\ln x)^{\alpha}}{x^{\beta}}.
	\end{equation*}
	
	Deuxièmement, montrons que : 
	\[ \frac{\ln (x)}{x^2}\xrightarrow[n \to +\infty ]{} 0. \]
	
	Soit $x \in [1,+\infty[ \ = \mathcal{A}$. Nous savons que la fonction $\ln$ est concave sur $\mathbb{R}_+^*$, donc en particulier sur $\mathcal{A}$. Ainsi, $\ln$ est en dessous de toutes ses tangentes, d'où : 
	\[ 
	\forall x \in \mathcal{A}, \quad 0 \; \leq \; \ln (x) \; \leq \; x-1.
	\]
	
	\newpage
	
	Illustration de l'inégalité : 
	
	\
	
	\begin{center}
		\definecolor{ttttff}{rgb}{0.2,0.2,1}
		\definecolor{ffttww}{rgb}{1,0.2,0.4}
		\definecolor{cqcqcq}{rgb}{0.75,0.75,0.75}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
			\draw [color=cqcqcq,dash pattern=on 1pt off 1pt, xstep=1.0cm,ystep=1.0cm] (-1,-2) grid (3,2);
			\draw[->,color=black] (-1,0) -- (3,0);
			\foreach \x in {-1,1,2}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0,-2) -- (0,2);
			\foreach \y in {-2,-1,1}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1,-2) rectangle (3,2);
			\draw[color=ffttww, smooth,samples=100,domain=3.244085366007135E-3:3.0] plot(\x,{ln(\x)});
			\draw[color=ttttff, smooth,samples=100,domain=-1.0:3.0] plot(\x,{\x-1});
			\draw (12.46,5.36) node[anchor=north west] {y = x-1};
			\draw (9.62,5.22) node[anchor=north west] {y = ln(x)};
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 1.} $\ln$ en rouge et la première bissectrice en bleu. 
	\end{center}
	
	\
	
	On peut alors diviser par $x^2$ (car $x \neq 0$): 
	
	\
	
	\[
		\forall x \in \mathcal{A},\quad 0 \; \leq \; \underset{f_{1,2}(x)}{\underbrace{\frac{\ln (x)}{x^2}}} \; \leq \; \underset{\xrightarrow[x\to+\infty]{} \ 0}{\underbrace{\frac{1}{x}}} - \underset{\xrightarrow[x\to+\infty]{} \ 0}{\underbrace{\frac{1}{x^2}}}.
	\]
	
	\
	
	Donc par théorème d'encadrement $f_{1,2}(x)\xrightarrow[x\to+\infty]{} 0$.
	
	\
	
	Dernièrement, le cas général. Soit $x\in \mathcal{A}$ et soient $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2$. On fait une preuve directe. 
	
	\begin{eqnarray*}
	    \frac{(\ln (x))^\alpha}{x^\beta} & = & \left( \frac{\ln (x)}{x^{\frac{\beta}{\alpha}}} \right)^\alpha \\[1.5ex]
	    & = & \underset{\underset{\text{par produit}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{\underset{c^{\underline{te}} \ \text{(définie!)}}{\underbrace{\left( \frac{2\alpha}{\beta} \right)^\alpha}} \cdot \underset{\underset{\text{par composition des limites}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{\left[ \underset{\underset{\text{d'après le dernier point}}{\xrightarrow[x\to+\infty]{} \ 0}}{\underbrace{ \frac{\ln \left( x^{\frac{\beta}{2 \alpha}} \right) }{\left( x^{\frac{\beta}{2\alpha}} \right)^2} }}\right]^\alpha}}}}.
	\end{eqnarray*}

\end{question_kholle}

\

%-------------------------------------------------------------

\begin{question_kholle}{Limite en $0$ de $\frac{1-\cos (x)}{x^2}$ et limite en $+\infty$ suivant $n$ de $\frac{\left(q^n \right)^\alpha}{(n!)^\beta}$ pour $q\in \mathbb{R}$ et $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2.$} 

	\
	
	Montrons que $\frac{1-\cos (x)}{x^2} \xrightarrow[x\to 0]{} \frac{1}{2}$.
	
	\
	
	On fait toujours une preuve directe. 
	\begin{eqnarray*}
	    \lim_{x\to0} \ \frac{1-\cos (x)}{x^2} & = & \lim_{x\to0} \ \frac{1-\cos \left( \frac{2x}{2}\right) }{x^2} \\[1ex]
	    & = & \lim_{x\to0} \ \frac{1-\left( 1-2\sin ^2 \left( \frac{x}{2}\right) \right) }{x^2} \\[1ex]
	    & = &  \lim_{x\to0} \ \frac{2\sin ^2 \left( \frac{x}{2}\right) }{4 \left( \frac{x}{2}\right) ^2} \\[1ex]
	    & = & \lim_{x\to0} \ \underset{\underset{\text{par produit}}{\xrightarrow[x\to 0]{} \ \frac{1}{2}}}{\underbrace{\underset{c^{\underline{te}}}{\underbrace{\frac{1}{2}}} \cdot \underset{\underset{\text{par composition}}{\xrightarrow[x\to 0]{} \ 1}}{\underbrace{\left[\underset{\underset{\text{limite usuelle}}{\xrightarrow[x\to 0]{} \ 1}}{\underbrace{\frac{\sin \left( \frac{x}{2}\right) }{\left( \frac{x}{2}\right)} }} \right] ^2}}}} \\[1ex]
	    & = & \frac{1}{2}
	\end{eqnarray*}
	
	\
	
	
	
	\
	
	Trouvons la limite, sous réserve d'existence, de $\frac{\left(q^n \right)^\alpha}{(n!)^\beta}$ pour $q\in \mathbb{R}$ et $(\alpha,\beta)\in \left( \mathbb{R}_+^* \right)^2$ suivant $n$ en $+\infty$.
	
	\
	
	Remarquons que si $q\leq0$, il est \textbf{\textit{nécessaire}} d'avoir $\alpha\in\mathbb{Z}^*$ sinon l'expression n'a tout simplement \textbf{\textit{aucun sens}}. De fait, on supposera $q>0$ tout le long, les cas $q<0$ se font naturellement (convergence pour $q\in \mathbb{R_-}$).
	
	\
	
	Soit donc $0<q<1$, ce cas est immédiat, $\left( \left(q^n \right)^\alpha\right)_{n\in\mathbb{N}}=\left( \left(q^\alpha \right)^n\right)_{n\in\mathbb{N}}$, donc il s'agit de la suite géométrique de raison $q^\alpha \in ]0,1[$ et de premier terme $q^{\min_{I}(n)\alpha}$ ($\min_{I}(n)$, avec $I$ une partie non vide de $\mathbb{N}$, car la suite ne démarre pas forcément à $0$), donc elle converge vers $0$.
	
	\
	
	Si $q\geq 1$, on montre le cas trivial $\alpha = \beta =1$ : 
	\[
	\forall n\in [\![ \lfloor q \rfloor +1,+\infty [\![, \quad 0 \leq \frac{q^n}{n!} = \underset{=\ \lambda \text{ (une constante)}}{\underbrace{\frac{q}{1} \times \frac{q}{2} \times \dots \times \frac{q}{\lfloor q \rfloor} }}\times \underset{\leq 1}{\underbrace{\frac{q}{\lfloor q\rfloor +1}}} \times \dots \times \underset{\leq1}{\underbrace{\frac{q}{n-1}}} \times \frac{q}{n} \leq \underset{\xrightarrow[n\to +\infty]{} \ 0}{\underbrace{\frac{\lambda q}{n}}}
	\]
	
	\
	
	Par théorème d'existence de limite par encadrement, $\left( \frac{q^n}{n!} \right)_{n\in \mathbb{N}}$ converge et sa limite est $0$.
	
	\
	
	Soient $(\alpha,\beta)\in \mathbb{R}^*_+$, montrons le cas général pour $q\geq 1$.
	
	\[
	\forall n \in \mathbb{N}, \quad \frac{(q^n)^\alpha}{(n!)\beta} = \left( \frac{\left(q^{\frac{\alpha}{\beta}}\right)^n}{n!} \right)^\beta = \underset{\underset{\text{par composition des limites }(\beta>0)}{\xrightarrow[n\to +\infty]{} 0}}{\underbrace{\left( \underset{\underset{\text{c'est le cas trivial}}{\xrightarrow[n\to +\infty]{} 0}}{\underbrace{\frac{\left(q^{\frac{\alpha}{\beta}}\right)^n}{n!}}} \right)^\beta}}
	\]
	
\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arcsin$.}
	Premièrement, ladite fonction est la bijection réciproque de la fonction $\widetilde{\sin}$ (voir \textbf{1}.). D'où : 
	\begin{equation*}
		\arcsin = \left\{  
		\begin{array}{c c c}
		[-1,1] & \to & [-\frac{\pi}{2} , \frac{\pi}{2}] \\ [1ex]
		x & \mapsto & \left( \widetilde{\sin} \right)^{-1}(x)
		\end{array} 
		\right.
	\end{equation*}
	
	\
	
	Ainsi, pour $x\in [-1,1]$, $\arcsin (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left[-\frac{\pi}{2} , \frac{\pi}{2}\right]$, $\sin(\theta) = x$. 
	
	\
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\sin}$ : 
	
	\begin{enumerate}
	    \item $\arcsin$ est impaire.
	    \item $\arcsin$ est strictement croissante sur $[-1,1]$.
	    \item $\arcsin \in \mathcal{C}^0\left([-1,1],[-\frac{\pi}{2} , \frac{\pi}{2}] \right)$.
	    \item $\arcsin \in \mathcal{D}^1\left(]-1,1[,\left]-\frac{\pi}{2} , \frac{\pi}{2}\right[ \right)$.
	    \item $\arcsin'(x) = \frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
	    \item $\arcsin$ admet deux demi-tangentes verticales en $-1$ et $1$.
	\end{enumerate}
	
	\
	
	Graphe de $\arcsin$ : 
	\begin{center}
		\definecolor{ffttww}{rgb}{1,0.2,0.4}
		\definecolor{ttzzff}{rgb}{0.2,0.6,1}
		\definecolor{zzffzz}{rgb}{0.6,1,0.6}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.910828025477707cm,y=1.910828025477707cm]
			\draw[->,color=black] (-1.57,0) -- (1.57,0);
			\foreach \x in {-1.5,-1,-0.5,0.5,1,1.5}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0,-1.57) -- (0,1.57);
			\foreach \y in {-1.5,-1,-0.5,0.5,1,1.5}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1.57,-1.57) rectangle (1.57,1.57);
			\draw[color=zzffzz] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.57:1.57]; plot sin(x)};
			\draw[color=ttzzff] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-1.57:1.57]; plot asin(x)};
			\draw[color=ffttww] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-1.57:1.57]; plot x};
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(--1.57-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (1,-1.57) -- (1,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (1.57,-1.57) -- (1.57,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(--1-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (-1.57,-1.57) -- (-1.57,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt] (-1,-1.57) -- (-1,1.57);
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(-1-0*\x)/1});
			\draw [line width=0.4pt,dash pattern=on 1pt off 1pt,domain=-1.57:1.57] plot(\x,{(-1.57-0*\x)/1});
			\draw [->] (-1.57,-1) -- (-1.18,-1);
			\draw [->] (-1,-1.57) -- (-1,-1.16);
			\draw [->] (1,1.57) -- (1,1.16);
			\draw [->] (1.57,1) -- (1.12,1);
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 2.} $\arcsin$ en bleu, $\widetilde{\sin}$ en vert et la première bissectrice en rouge.
	\end{center}
	
	\
	
	On a aussi, grâce au taux d'accroissement en 0 d'$\arcsin$ : 
	\[
		\lim_{x\to0} \frac{\arcsin(x)}{x} \ = \ 1.
	\]
	
	\
	
	Puis finalement (visible sur le graphe) : 
	\[
		\forall x \in [0,1], \quad \arcsin(x) \geq x.
	\]
\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arccos$.} 

	Premièrement, ladite fonction est la bijection réciproque de la fonction $\cos |_{[0,\pi]}^{[-1,1]} := \widetilde{\cos}$. D'où : 
	\begin{equation*}
		\arccos = \left\{  
		\begin{array}{c c c}
		[-1,1] & \to & [0 , \pi] \\ [1ex]
		x & \mapsto & \left( \widetilde{\cos} \right)^{-1}(x)
		
		\end{array} 
		\right.
	\end{equation*}
	
	\
	
	Ainsi, pour $x\in [-1,1]$, $\arccos (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle [0 ,\pi]$, $\cos(\theta) = x$.
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\cos}$ : 
	
	\begin{enumerate}
	    \item $\arccos$ est strictement décroissante sur $[-1,1]$.
	    \item $\arccos \in \mathcal{C}^0\left([-1,1],[0 , \pi] \right)$.
	    \item $\arccos \in \mathcal{D}^1\left(]-1,1[,]0 ,\pi [ \right)$.
	    \item $\arccos'(x) = -\frac{1}{\sqrt{1-x^2}}$ pour tout $x\in]-1,1[$.
	    \item $\arccos$ admet deux demi-tangentes verticales en $-1$ et $1$.
	\end{enumerate}
	
	\
	
	Graphe de $\arccos$ : 
	\begin{center}
		\definecolor{cczzff}{rgb}{0.8,0.6,1.0}
		\definecolor{qqffqq}{rgb}{0.0,1.0,0.0}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{xfqqff}{rgb}{0.4980392156862745,0.0,1.0}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.3636363636363635cm,y=1.3636363636363635cm]
			\draw[->,color=black] (-1.2,0.0) -- (3.2,0.0);
			\foreach \x in {-1.0,-0.5,0.5,1.0,1.5,2.0,2.5,3.0}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0.0,-1.2) -- (0.0,3.2);
			\foreach \y in {-1.0,-0.5,0.5,1.0,1.5,2.0,2.5,3.0}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-1.2,-1.2) rectangle (3.2,3.2);
			\draw[color=xfqqff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [0:3.14]; plot cos(x)};
			\draw[color=ffqqqq] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-1.2:3.2]; plot x};
			\draw[color=qqffqq] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-1:1]; plot acos(x)};
			\draw[dotted,color=cczzff] plot[raw gnuplot, id=func3] function{set samples 100; set xrange [-1.2:3.2]; plot 3.1415926535/2.0-x};
			\draw [dotted] (-1.0,0.0)-- (-1.0,3.141592653589793);
			\draw [dotted] (-1.0,3.141592653589793)-- (0.0,3.141592653589793);
			\draw [dotted] (0.0,1.0)-- (1.0,1.0);
			\draw [dotted] (1.0,0.0)-- (1.0,1.0);
			\draw [dotted] (0.0,-1.0)-- (3.141592653589793,-1.0);
			\draw [dotted] (3.141592653589793,0.0)-- (3.141592653589793,-1.0);
			\draw [->] (-1.0,3.141592653589793) -- (-1.0,2.6982051899765422);
			\draw [->] (0.0,1.0) -- (0.41398424427733616,1.0);
			\draw [->] (1.0,0.0) -- (1.0,0.41498464079523883);
			\draw [->] (3.141592653589793,-1.0) -- (2.697204793458636,-1.0);
		\end{tikzpicture}
	
		\
	
		\textbf{Figure 3.} $\arccos$ en vert, $\widetilde{\cos}$ en violet, la première bissectrice en rouge et $y = \frac{\pi}{2} - x$ en rose.
	\end{center}

\end{question_kholle}

\begin{question_kholle}{Présentation exhaustive de la fonction $\arctan$.} 

	\
	
	Premièrement, ladite fonction est la bijection réciproque de la fonction $\tan |_{\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ }:=\widetilde{\tan}$. D'où : 
	\begin{center}
	
	$\arctan = \left\{  
	\begin{array}{c c c}
	\mathbb{R} & \to & \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \\ [1ex]
	x & \mapsto & \left( \widetilde{\tan} \right)^{-1}(x)
	\end{array} 
	\right.
	$
	\end{center}
	
	\
	
	Ainsi, pour $x\in \mathbb{R}$, $\arctan (x)$ est l'unique solution de l'équation d'inconnue $\theta \in \textstyle \left] -\frac{\pi}{2}, \frac{\pi}{2}\right[$, $\tan(\theta) = x$. 
	
	\
	
	\noindent Il découle alors naturellement des propriétés héréditairement acquises de $\widetilde{\tan}$ : 
	
	\begin{enumerate}
	    \item $\arctan$ est impaire.
	    \item $\arctan \in \mathcal{C}^0\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
	    \item $\arctan \in \mathcal{D}^1\left(\mathbb{R},\left] -\frac{\pi}{2}, \frac{\pi}{2}\right[ \right)$.
	    \item $\arctan'(x) = \frac{1}{1+x^2}$ pour tout $x\in\mathbb{R}$.
	\end{enumerate}
	
	\newpage
	
	Graphe de $\arctan$ : 
	\begin{center}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{qqffqq}{rgb}{0.0,1.0,0.0}
		\definecolor{qqffff}{rgb}{0.0,1.0,1.0}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
			\draw[->,color=black] (-3.0,0.0) -- (3.0,0.0);
			\foreach \x in {-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.5,1.0,1.5,2.0,2.5}
			\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
			\draw[->,color=black] (0.0,-3.0) -- (0.0,3.0);
			\foreach \y in {-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.5,1.0,1.5,2.0,2.5}
			\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
			\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
			\clip(-3.0,-3.0) rectangle (3.0,3.0);
			\draw[color=qqffff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.56:1.56]; plot tan(x)};
			\draw[color=qqffqq] plot[raw gnuplot, id=func1] function{set samples 100; set xrange [-3:3]; plot atan(x)};
			\draw[color=ffqqqq] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-3:3]; plot x};
			\draw [domain=-3.0:3.0] plot(\x,{(--1.5707963267948966-0.0*\x)/1.0});
			\draw (1.5707963267948966,-3.0) -- (1.5707963267948966,3.0);
			\draw [domain=-3.0:3.0] plot(\x,{(-1.5707963267948966-0.0*\x)/1.0});
			\draw (-1.5707963267948966,-3.0) -- (-1.5707963267948966,3.0);
			\draw [dotted] (0.7853981633974483,1.0)-- (0.7853981633974483,0.0);
			\draw [dotted] (0.0,0.7853981633974483)-- (1.0,0.7853981633974483);
			\draw [dotted] (0.0,1.0)-- (1.0,1.0);
			\draw [dotted] (1.0,0.0)-- (1.0,1.0);
			\draw [dotted] (-1.0,0.0)-- (-1.0,-1.0);
			\draw [dotted] (0.0,-1.0)-- (-1.0,-1.0);
			\draw [dotted] (-0.7853981633974483,0.0)-- (-0.7853981633974483,-1.0);
			\draw [dotted] (0.0,-0.7853981633974483)-- (-1.0,-0.7853981633974483);
			\begin{scriptsize}
				\draw[color=qqffff] (-3.1177254400173355,-0.014744648606977613) node {$f$};
				\draw[color=qqffqq] (-3.1177254400173355,-1.3072326284088318) node {};
				\draw[color=ffqqqq] (-1.8026940652189338,-1.8332451783281911) node {};
				\draw[color=black] (-3.1177254400173355,1.5106917461591645) node {$a$};
				\draw[color=black] (1.4660982092799508,2.322253966034747) node {};
				\draw[color=black] (-3.1177254400173355,-1.4500074633869435) node {$c$};
				\draw[color=black] (-1.4946010002661654,2.322253966034747) node {};
			\end{scriptsize}
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 4.} $\arctan$ en vert, $\widetilde{\tan}$ en bleu, la première bissectrice en rouge, et les fonctions $y = \pm \frac{\pi}{2}$ et $x = \pm \frac{\pi}{2}$ en noir.
	\end{center}
	
	\
	
	On a aussi (visible sur le graphe) : 
	\[
		\forall x \in \mathbb{R}_+, \quad \arctan(x) \leq x.
	\]
	
	Et enfin : 
	\[
		\forall x \in \mathbb{R}^*, \quad \arctan(x) + \arctan \left( \frac{1}{x} \right) = 
		\left\{ \begin{array}{cl}
		\frac{\pi}{2} & \text{si } x \ > \ 0 \\
		-\frac{\pi}{2} & \text{si } x \ < \ 0.
		\end{array} \right.
	\]

\end{question_kholle}

%-------------------------------------------------------------
\begin{question_kholle}{$2$ preuves de $\arcsin(x) + \arccos(x) =\frac{\pi}{2}$ sur $[-1,1]$, dont une basée sur une interprétation géométrique du cercle trigonométrique.} 

	L'interprétation géométrique sur $[0,1]$, celle sur $[-1,0]$ est laissée au lecteur car il s'agit du même principe modulo des détails : 
	
	\begin{center}
		\definecolor{eqbqff}{rgb}{0.8784313725490196,0.6901960784313725,1.0}
		\definecolor{xfqqff}{rgb}{0.4980392156862745,0.0,1.0}
		\definecolor{uuuuuu}{rgb}{0.26666666666666666,0.26666666666666666,0.26666666666666666}
		\definecolor{ffqqqq}{rgb}{1.0,0.0,0.0}
		\definecolor{qqffff}{rgb}{0.0,1.0,1.0}
		\definecolor{cqcqcq}{rgb}{0.7529411764705882,0.7529411764705882,0.7529411764705882}
		\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=2.5cm,y=2.5cm]
			\clip(-1.2,-1.2) rectangle (1.2,1.2);
			\fill[fill=black,fill opacity=1.0] (0.08956860342780414,0.20707624336357297) -- (0.08145277478981197,0.18983010750783963) -- (0.0742401101509475,0.2039730262575521) -- cycle;
			\fill[fill=black,fill opacity=1.0] (0.3604343842207928,0.11272973544691409) -- (0.3441578360574464,0.12526320821437803) -- (0.33811585546631434,0.10562838538867093) -- cycle;
			\draw [color=cqcqcq] (0.0,0.0) circle (2.5cm);
			\draw [shift={(0.0,0.0)},line width=1.2000000000000002pt,color=qqffff]  plot[domain=0.0:3.141592653589793,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [shift={(3.061616997868383E-17,0.0)},line width=1.2000000000000002pt,color=ffqqqq]  plot[domain=-1.5707963267948966:1.5707963267948963,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [shift={(0.0,-0.0)},line width=1.2000000000000002pt,color=xfqqff]  plot[domain=0.0:1.5707963267948966,variable=\t]({1.0*1.0*cos(\t r)+-0.0*1.0*sin(\t r)},{0.0*1.0*cos(\t r)+1.0*1.0*sin(\t r)});
			\draw [line width=1.2000000000000002pt,color=qqffff] (-1.0589461703485412,0.0)-- (-0.9410538296514588,0.0);
			\draw [line width=1.2000000000000002pt,color=qqffff] (1.0589461703485412,0.0)-- (0.9410538296514588,0.0);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,1.0589461703485412)-- (5.762292801540088E-17,0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (-1.945252556980033E-16,-1.0589461703485412)-- (-1.7286878404620264E-16,-0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,1.0589461703485412)-- (0.05398708109469719,1.0589461703485412);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (5.762292801540088E-17,0.9410538296514588)-- (0.05398708109469719,0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (6.484175189933444E-17,-1.0589461703485412)-- (0.05398708109469719,-1.0589461703485412);
			\draw [line width=1.2000000000000002pt,color=ffqqqq] (5.762292801540088E-17,-0.9410538296514588)-- (0.05398708109469719,-0.9410538296514588);
			\draw [line width=1.2000000000000002pt,color=qqffff] (-1.0589461703485412,1.2968350379866888E-16)-- (-1.0589461703485412,0.05398708109469725);
			\draw [line width=1.2000000000000002pt,color=qqffff] (-0.9410538296514588,1.1524585603080177E-16)-- (-0.9410538296514588,0.05398708109469724);
			\draw [line width=1.2000000000000002pt,color=qqffff] (0.9410538296514588,1.1524585603080177E-16)-- (0.9410538296514588,0.05398708109469724);
			\draw [line width=1.2000000000000002pt,color=qqffff] (1.0589461703485412,1.2968350379866888E-16)-- (1.0589461703485412,0.05398708109469725);
			\draw [dash pattern=on 1pt off 1pt on 3pt off 4pt] (6.123233995736766E-17,1.0)-- (0.0,-1.0);
			\draw [dash pattern=on 1pt off 1pt on 3pt off 4pt] (-1.0,0.0)-- (1.0,-0.0);
			\draw[color=eqbqff] plot[raw gnuplot, id=func0] function{set samples 100; set xrange [-1.0999999999999999:1.0999999999999999]; plot x};
			\draw (0.3420201433256688,0.9396926207859083)-- (0.0,-0.0);
			\draw (0.0,-0.0)-- (0.9396926207859084,0.3420201433256687);
			\draw [shift={(0.0,-0.0)}] plot[domain=0.0:0.3490658503988659,variable=\t]({1.0*0.36624511935574344*cos(\t r)+-0.0*0.36624511935574344*sin(\t r)},{0.0*0.36624511935574344*cos(\t r)+1.0*0.36624511935574344*sin(\t r)});
			\draw [shift={(0.0,-0.0)}] plot[domain=0.0:1.2217304763960306,variable=\t]({1.0*0.21706356072793254*cos(\t r)+-0.0*0.21706356072793254*sin(\t r)},{0.0*0.21706356072793254*cos(\t r)+1.0*0.21706356072793254*sin(\t r)});
			\draw (0.08956860342780414,0.20707624336357297)-- (0.08145277478981197,0.18983010750783963);
			\draw (0.08145277478981197,0.18983010750783963)-- (0.0742401101509475,0.2039730262575521);
			\draw (0.0742401101509475,0.2039730262575521)-- (0.08956860342780414,0.20707624336357297);
			\draw (0.3604343842207928,0.11272973544691409)-- (0.3441578360574464,0.12526320821437803);
			\draw (0.3441578360574464,0.12526320821437803)-- (0.33811585546631434,0.10562838538867093);
			\draw (0.33811585546631434,0.10562838538867093)-- (0.3604343842207928,0.11272973544691409);
			\draw [dash pattern=on 3pt off 3pt] (5.938595898438009E-17,0.9396926207859083)-- (0.3420201433256688,0.9396926207859083);
			\draw [dash pattern=on 3pt off 3pt] (4.108751682287631E-17,0.34202014332566877)-- (0.9396926207859084,0.3420201433256687);
			\draw [dash pattern=on 3pt off 3pt] (0.3420201433256688,0.9396926207859083)-- (0.3420201433256688,-0.0);
			\draw [dash pattern=on 3pt off 3pt] (0.9396926207859084,0.3420201433256687)-- (0.9396926207859084,-0.0);
			\draw (0.3106196735830377,0.20690669566269085) node[anchor=north west] {arccos(x)};
			\draw (0.06201603682796799,0.3536235960427321) node[anchor=north west] {arcsin(x)};
			\draw (-0.18658759992710172,0.9853213615679096) node[anchor=north west] {x};
			\draw (0.8241288249131816,0.0031332229126335774) node[anchor=north west] {x};
			\begin{scriptsize}
				\draw [fill=uuuuuu] (0.0,-0.0) circle (1.5pt);
				\draw[color=uuuuuu] (-0.011342413362052578,0.09279355092265877) node {$\Omega$};
				\draw [fill=uuuuuu] (5.938595898438009E-17,0.9396926207859083) circle (1.5pt);
				\draw [fill=uuuuuu] (0.9396926207859084,-0.0) circle (1.5pt);
			\end{scriptsize}
		\end{tikzpicture}
		
		\
		
		\textbf{Figure 5.}
	\end{center}
	
	\
	
	Preuve formelle : 
	
	\
	
	Soit $x\in [-1,1]$. Posons  $\varphi \ = \ \arcsin(x) \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$. Ainsi : 
	
	\[
		\arcsin(x) + \arccos(x) \ = \ \varphi + \arccos(\sin(\varphi)) \ = \ \varphi + \arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right),
	\]
	
	\
	
	or $\varphi \in \left[-\frac{\pi}{2},\frac{\pi}{2}\right]$ donc
	$\frac{\pi}{2}- \varphi \in [0,\pi]$ d'où $\arccos \left( \cos \left( \frac{\pi}{2}- \varphi \right) \right) = \frac{\pi}{2}- \varphi$ si bien que : 
	\[
		\arcsin(x) + \arccos(x) \ = \  \varphi +\frac{\pi}{2} - \varphi \ = \ \frac{\pi}{2}.
	\]

\end{question_kholle}

\begin{question_kholle}{Présentation analytique rapide des fonctions \(\cosh\) et \(\sinh \).}
	~\smallbreak
	
	\begin{itemize}[label=$\bullet$]
		\item Domaine de définition et symétries.
		\newline
		$\sinh$ et $\cosh$ sont définies sur $\mathbb{R}$. 
		\newline
		De plus, 
		\newline
		$(i)$ $\forall x \in \mathbb{R}$, $-x\in \mathbb{R}$, 
		\newline
		$(ii)$ $\forall x \in \mathbb{R}$, 
		$
			\left\{ \begin{array}{c c c c c c c}
			\sinh (-x) & = & \frac{e^{-x} - e^{x}}{2} & = & - \frac{e^x - e^{-x}}{2} & = & -\sinh(x) \\
			\text{et} & & & & & & \\ 
			\cosh (-x) & = & \frac{e^{-x} + e^{-(-x)}}{2} & = &  \frac{e^x + e^{-x}}{2} & = & \cosh(x).
			\end{array} 
			\right.
		$
		\newline
		Donc $\sinh$ et $\cosh$ sont respectivement impaire et paire.
		\newline
		Nous les étudierons sur $\mathbb{R}_+$ et pour les obtenir les graphes $(\mathcal{C}_{\sinh} \text{ et } \mathcal{C}_{\cosh})$ de ces fonctions sur $\mathbb{R}$ à partir de ceux $(\mathcal{C}_{\sinh}^+ \text{ et } \mathcal{C}_{\cosh}^+)$ obtenus sur $\mathbb{R}_+$, nous le complèterons en traçant les images de ces graphes par la symétrie centrale $s$ de centre $O$ et par la réflexion $r$ d'axe $\left( O, \overrightarrow{\jmath} \right)$ : 
		\[
			\mathcal{C}_{\sinh} = \mathcal{C}_{\sinh}^+ \cup s \left( \mathcal{C}_{\sinh}^+ \right) \qquad \text{ et } \qquad \mathcal{C}_{\cosh} = \mathcal{C}_{\cosh}^+ \cup r \left( \mathcal{C}_{\cosh}^+ \right)
		\]
		
		\
		
		\item Variations : triviales.
		
		\
		
		\item Branches infinies en $+\infty$ et position relative de $\mathcal{C}_{\sinh}$ et $\mathcal{C}_{\cosh}$.
		
		\[
			\frac{\cosh(x)}{x} = \underset{\xrightarrow[x\to +\infty]{} \ +\infty}{\underbrace{\frac{e^{x}}{x}}} + \underset{\xrightarrow[x\to +\infty]{} \ 0}{\underbrace{\frac{e^{-x}}{x}}} \xrightarrow[x\to +\infty]{} \ +\infty
		\]
		Donc le graphe de $\cosh$ admet une branche parabolique de direction asymptotique $\left( O, \overrightarrow{\jmath}\right)$.
		\newline
		On a : 
		\[
			\forall x \in \mathbb{R}, \quad \cosh(x) - \sinh(x) = e^{-x} \xrightarrow[x\to +\infty]{} 0^+
		\]
		Donc les graphes des deux fonctions se rapprochent l'un de l'autre arbitrairement près lorsque $x \to +\infty$, et le graphe de $\cosh$ est au-dessus de celui de $\sinh$.
		
		\
		
		\item Tangente au graphe de $\sinh$ à l'origine et position relative.
	\end{itemize}

	Il s'agira d'étudier $g : x\in \mathbb{R}_+ \mapsto \sinh(x) -x$, de remarquer sa dérivabilité d'en étudier les variations puis de conclure, en précisant que cette étude révèle l'inflexion du graphe de $\sinh$ en 0.
\end{question_kholle}

\pagebreak\section{Semaine 7}

\begin{question_kholle}{Calcul de $\int_0^{2\pi}e^{imt} \mathrm d t$ en fonction de $m \in \Z$. En Déduire qu'une fonction polynomiale nulle sur un cercle centré en l'origine a tous ses coefficients nuls.}
	Soit $m \in \Z$ fq. Calculons:
	$$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t$$
	Si $m \neq 0$:
	\begin{align*}
		\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t &= \frac{1}{2 \pi} \Big[ \frac{e^{mt}}{im} \Big]_0^{2\pi}\\
		&= \frac{1}{2 \pi} \Big( \frac{1}{im} - \frac{1}{im} \Big) = 0
	\end{align*}
	Si $m = 0$:
	$$
		\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t = \frac{1}{2 \pi} \int_0^{2\pi} \mathrm d t = \frac{2 \pi}{2 \pi} = 1
	$$
	\\
	Donc $$\frac{1}{2 \pi} \int_0^{2\pi}e^{imt} \mathrm d t = 
	\begin{cases}
		1 \text{ si } m=0\\
		0 \text{ si } m \neq 0
	\end{cases}
	$$
	\\
	Soit $n\in \N$ fq
	
	Soient $(a_0, ..., a_n) \in \C^{n+1}$ les coefficients de $P(z) = \sum_{k=0}^n a_k z^k$, et $s\in \Z$, et $r \in \R_+^*$ fq. tels que P soit nulle lorsqu'elle est évaluée sur $\mathscr C(0,r)$
	\begin{align*}
		\frac{1}{2 \pi} \int_0^{2\pi} P(re^{it}) e^{-imt} \mathrm d t &= \frac{1}{2 \pi} \int_0^{2\pi} \bigg (\sum_{k=0}^n a_k (re^{it})^k \bigg) e^{-imt} \mathrm d t\\
		&= \sum_{k=0}^n a_k r^k \underbrace{\int_0^{2\pi} \frac{e^{it(k-s)}}{2 \pi}  \mathrm d t}_{I_k}
	\end{align*}
	On remarque que:
	\begin{itemize}
		\item Si $s \notin [ \! [0, n] \! ], \{k \in [ \! [0, n] \! ] \text{ }| \text{ } k = s\}$ = $\emptyset$, Donc $$\sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k  = \sum_{\substack{k \in [ \! [0, n] \! ] \\ k = s}} a_k r^k  = 0$$
		\item Si $s \in [ \! [0, n] \! ], \{k \in [ \! [0, n] \! ] \text{ }| \text{ } k = s\}$ = ${s}$, Donc $$\sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k = \sum_{\substack{k \in [ \! [0, n] \! ] \\ k = s}} a_k s^k = a_s r^s \label{1}$$
	\end{itemize}
	Or, puisque $P$ s'annule sur le cercle de rayon $r$ et de centre $0$,  $\mathscr C(0,r)$, ces sommes sont aussi nulles. On en déduit, en particularisant pour un $s \in [ \! [0, n] \! ]$ fixé quelconque que:
	$$
		\sum_{k \in [ \! [0, n] \! ]} a_k s^k I_k = a_sr^s = 0 \implies a_s = 0
	$$
	
	Donc $$
		(\exists r \in \R_+^* : \forall \theta \in \R, P(re^{i\theta})=0) \implies \forall s \in [ \! [0, n] \! ]
	$$
	\\
	Pour la preuve réciproque,  soit $n \in \N$ fq. Soient $(a_0,...,a_n) \in \{ 0 \} ^{n+1}$ les coefficients nuls de la fonction polynomiale $P \in \C[z]$ définie pour tout $z \in \C$.
	
	En remarquant que $\forall z \in \C , P(z) = 0$, puisque n'importe quel cercle centré en 0 est un sous ensemble de $\C$,  $\exists r \in \R_+^*: \forall z \in \mathscr C(0, r), P(z) = 0$.
\end{question_kholle}

\begin{question_kholle}{Preuve de la Linéarité de la dérivation d'une fonction complexe}
	Définissons les fonctions $f_r$ et $f_i$ comme les parties réelles et imaginaires de $f$.

	Soient $(f, g) \in \mathcal{F}(I, \C)^2$, $(\alpha, \beta) \in \C^2$ fixés quelconques.
	\begin{align*}
		f_r = \Re(f) &, f_i = \Im(f) &g_r = \Re(f) &, g_i = \Im(g)\\
		\alpha_r = \Re(\alpha) &, \alpha_i = \Im(f) &\beta_r = \Re(f) &, \beta_i = \Im(g)
	\end{align*}
	
	\begin{align*}
		\Re( \alpha f + \beta g) &= \Re((\alpha_r + i \alpha_i)(f_r + i f_i) + (\beta_r+ i\beta_i)(g_r+ i g_i)) \\
		&= \underbrace{\alpha_r f_r + \beta_r g_r - \alpha_i f_i - \beta_i g_i}_{\text{Combinaison linéaire de } \underbrace{(f_r, f_i, g_r, g_i) \in \mathcal D^1(I, \R)^4}_{car (f,g) \in D^1(I, \R)^2}}
	\end{align*}
	Donc, selon le théorème de stabilité par combinaison linéaire des fonctions à valeurs réelles, $\Re(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\Re(\alpha f + \beta g)\big)' = \alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i'$
	\\
	On montre de même que $\Im(\alpha f + \beta g) \in \mathcal D^1(I, \R)$ et $\big(\alpha f + \beta g\big)' = \alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r'$
	
	Ainsi,
	\begin{align*}
		\big( \alpha f + \beta g \big)' &= (\alpha_r f_r' + \beta_r g_r' - \alpha_i f_i' - \beta_i g_i') + i (\alpha_r f_i' +\alpha f_r' +\beta_r g_i' +\beta_i g_r') \\
		&= \alpha_r(f_r' + if_i') + \beta_r(g_r' + ig_i') + \alpha_i \underbrace{(-f_i' + if_r')}_{i(f_r' + if_i')} + \beta_i \underbrace{( -g_i' + ig_r')}_{i(g_r' + ig_i')} \\
		&=\alpha f' + \beta g'
	\end{align*}
\end{question_kholle}

\begin{question_kholle}{Dérivée composée d'une fonction à valeurs complexes}
	Soient $f \in \mathcal D ^1(J, \C) $ et $h \in \mathcal D^1(I, J)$ (I et J sont deux intervalles réels) fixés quelconques. Notons $f_r$ et $f_i$ respectivement la partie réelle et imaginaire de $f$.
	
	\begin{align*}
		\left .
		\begin{array}{ll}
			h \in \mathcal D^1(I, J) \\
			f_r \in \mathcal D^1(J, \R) \text{, car } f \in \mathcal D^1(J, \C)
		\end{array}
		\right \}
		\implies f_r \circ h \in \mathcal D^1(I, \R)
	\end{align*}
	
	On montre de même que $f_i \circ h \in \mathcal D^1(I, \mathbb  R)$ donc $f \circ h \in \mathcal D^1(I, \C)$.
	
	De plus,
	
	\begin{align*}
		(f \circ h)' &= (f_r \circ h)' + i (f_i \circ h)' \\
		&= (f_r' \circ h ) \times h' + i((f_i' \circ h) \times h') \\
		&=(f_r' \circ h + if_i' \circ h) \times h' \\
		&= (f' \circ h) \times h'
	\end{align*}
\end{question_kholle}

\begin{question_kholle}{Caractérisation des fonctions dérivables de dérivée nulle sur un intervalle}
	Soit $f \in \mathcal D ^1 (I, \C)$ où $I$ est un intervalle réel;
	Posons $f_r = \Re (f)$ et $f_i = \Im(f)$.
	
	\begin{align*}
	\forall t \in I, f'(t) = 0 &\iff \forall t \in I, f_r'(t) + i f_i'(t) = 0 \\
	&\iff \begin{cases}
		\forall t \in I, f_r'(t) = 0 \\
		\forall t \in I, f_i'(t) = 0
	\end{cases} \\
	&\iff \begin{cases}
		\exists \lambda_r \in \R : \forall t \in I,  f_r(t) = \lambda_r \\
			\exists \lambda_i \in \R : \forall t \in I,  f_i(t) = \lambda_i
	\end{cases} \\
	&\iff \exists \lambda \in \C : \forall t \in I,  f(t) = \lambda
	\end{align*}
\end{question_kholle}

\pagebreak\section{Semaine 8}

	\begin{question_kholle}{Preuve de l’expression des solutions réelles des EDL homogènes d’ordre 2 à coefficients constants réels dans le cas $\Delta < 0$ (en admettant la connaissance de l’expression des solutions à valeurs complexes des EDLH2 à coeff. constants).}
		Notons $\Sol_{H, \C}$ et $\Sol_{H, \R}$ les ensembles des solutions complexes et réelles de l'équation différentielle, puisque nous nous plaçons dans le cas $\Delta < 0$ et $\alpha \pm i \beta$ les deux racines complexes conjuguées.
		$$
		\Sol_{H, \C} =
		\left\{
		\begin{array}{l}
	    \R \to \C  \\
	    t \mapsto \lambda e^{(\alpha + i \beta) t}  + \mu e^{(\alpha - i \beta)t}
    	\end{array}
		\middle\vert  (\lambda, \mu) \in \C ^2 \right\}
		$$

		Montrons que $\forall f \in \Sol_{H ,\C}, \Re(f) \in  \Sol_{H ,\R}$\\
		Soit $f \in \Sol_{H ,\C}$ fq.
		$$f \in \mathcal D^2(\R, \C) \implies \Re(f) \in \mathcal D^2(\R, \R)$$
		Et, de plus, par morphisme additif de \Re
		$$
		a_2\Re(f)'' + a_1\Re(f)' + a_0\Re(f) = \Re( a_2 f'' + a_1 f' + a_0 f) = 0
		$$
		D'où, avec $f:t \mapsto e^{(\alpha + i \beta)t}$; $\Re(f(t)) = \Re(e^{(\alpha + i \beta)t}) = e^{\alpha t } \cos (\beta t)$. Qui appartient donc à $\Sol_{H, \R}$\\
		En suivant le même raisonnement pour $\Im(f)$, $(t \mapsto e^\alpha \sin(\beta t)) \in \Sol_{H, \R}$


		Ainsi, par combinaison linéaire (qui se base sur le principe de superposition),
		$$
		\left\{
		\begin{array}{l}
	    \R \to \R  \\
	    t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
	  	\end{array}
		\middle\vert  (\lambda, \mu) \in \R ^2 \right\}
		\subset \Sol_{H ,\R}
		$$

		Réciproquement, soit $ f \in \Sol_{H ,\R}$ fq. Puisque $\R \subset \C$,  $ f \in \Sol_{H ,\C}$.

		$$
		\exists (a, b) \in \C^2 : f \left| \begin{array}{l}
	    \R \to \C  \\
	    t \mapsto a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t}
		\end{array}\right.$$

		Or, puisque toutes les valeurs de $f$ sont réelles, en notant $(a_r, a_i, b_r, b_i)$ les parties réelles et imaginaires respectives de $a$ et $b$.
		\begin{align*}
			\forall t \in \R, f(t) &= \Re(f(t)) \\
					&= \Re(a e^{(\alpha + i \beta) t}  + b e^{(\alpha - i \beta)t})\\
					&= \Re((a_r + i a_i) e^{(\alpha + i \beta) t}  + (b_r + i b_i) e^{(\alpha - i \beta)t})\\
				    &= a_r \cos(\beta t)e^\alpha - a_i\sin(\beta t)e^\alpha + b_r \cos(\beta t)e^\alpha + b_i \sin(\beta t) e^\alpha \\
				    &= (a_r + b_r) \cos(\beta t) e^\alpha + (b_i - a_i) \sin(\beta t) e^\alpha
		\end{align*}
		Ainsi,
		$$f\in \left\{
		\begin{array}{l}
	    \R \to \R  \\
	    t \mapsto \lambda e^{\alpha t } \cos (\beta t)   + \mu e^{\alpha t } \sin (\beta t)
	  \end{array}
		\middle\vert  (\lambda, \mu) \in \R ^2 \right\}
	$$
	Ce qui conclut la preuve par double inclusion.
	\end{question_kholle}

	\begin{question_kholle}[
		Considérons le problème de Cauchy suivant :
		$$\left\{ \begin{array}{l}
			a_{2}y''+a_{1}y'+a_{0}y = b \text{ sur } J  \\
			y(t_{0}) = \alpha_{0} \\
			y'(t_{0}) = \alpha_{1}
		\end{array} \right. \text{ où } (\alpha_{0}, \alpha_{1}) \in \mathbb{K}^{2}, t_{0} \in J, (a_{0}, a_{1}, a_{2}) \in \mathbb{K}^{2} \times \mathbb{K}^{*}, b \in \mathcal{F}(J, \mathbb{K})$$
		Si $b$ est continu sur $J$, alors ce problème de Cauchy admet une unique solution définie sur $J$.
		]
		{Existence et unicité d'une solution au problème de Cauchy pour les EDL d'ordre 2 à coefficients constants et second membre continu sur $I$ (cas complexe puis cas réel).}

	\textbf{Cas 1. } $\mathbb{K} = \mathbb{C}$ \\
	Nous savons que sous l'hyphothèse de continuité de $b$ sur $J$, les solutions de (EDL2) définies sur $J$ constituent le plan affine $S$ :
	$$S = \left\{ \lambda f_{1} + \mu f_{2} + s | (\lambda, \mu) \in \mathbb{C}^{2} \right\}$$
	où $s$ est une solution particulière de (EDL2), $(f_{1}, f_{2})$ sont deux solutions de (EDLH2) qui engendrent $S_{h}$. On a : \\

	$$\begin{array}{ccl}
	    f : J \to \mathbb{C} \text{ est sol. du pb de Cauchy }
	    &\iff &\left\{ \begin{array}{l}
	      f \text{ sol de (EDL2) sur } J    \\
	      f(t_{0}) = \alpha_{0}    \\
	      f'(t_{0}) = \alpha_{1}
	    \end{array}  \right. \\\\
	    &\iff &\left\{ \begin{array}{l}
	      f \in S    \\
	      f(t_{0}) = \alpha_{0}    \\
	      f'(t_{0}) = \alpha_{1}
	    \end{array}\right. \\\\
	    &\iff &\exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
	      f = \lambda f_{1} + \mu f_{2} + s \\
	      \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) + s(t_{0}) = \alpha_{0} \\
	      \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) + s'(t_{0}) = \alpha_{1} \\
	    \end{array} \right. \\\\
	    &\iff &\exists (\lambda, \mu) \in \mathbb{C}^{2}: \left\{ \begin{array}{l}
	      f = \lambda f_{1} + \mu f_{2} + s \\
	      \lambda f_{1}(t_{0}) + \mu f_{2}(t_{0}) = \alpha_{0} - s(t_{0}) \\
	      \lambda f'_{1}(t_{0}) + \mu f'_{2}(t_{0}) = \alpha_{1} - s'(t_{0}) \\
	    \end{array} \right. \\\\
	\end{array} $$
	On en déduit donc que $(\lambda, \mu)$ doit être solution d'un système linéaire $(2,2)$. On a une unique solution si et seulement si les déterminant de ce système est nul. \\
	Explicitons alors le déterminant de ce système, que l'on notera $D$.
	$$D = \left|
	\begin{array}{cc}
	f_{1}(t_{0}) &f_{2}(t_{0}) \\
	f'_{1}(t_{0}) &f'_{2}(t_{0}) \\
	\end{array}
	\right| = f_{1}(t_{0}) \cdot f'_{2}(t_{0}) - f_{2}(t_{0}) \cdot f'_{1}(t_{0}) $$
	Notons $\Delta$ le discriminant de l'équation caractéristique de (EDL2) ($a_{2}r^{2} + a_{1}r^{1} + a_{0} = 0$). On distingue alors deux cas selon la nullité ou non de $\Delta$. Traitons d'abord le cas $\Delta \neq 0$. On peut choisir :
	$$ f_{1}(t_{0}) = e^{r_{1}t_{0}} \text{ et } f_{2}(t_{0}) = e^{r_{2}t_{0}}$$
	$$ f'_{1}(t_{0}) = r_{1}e^{r_{1}t_{0}} \text{ et } f'_{2}(t_{0}) = r_{2}e^{r_{2}t_{0}}$$
	Donc (en sachant que $\Delta \neq 0 \Rightarrow r_{1} \neq r_{2}$):
	$$ D = e^{r_{1}t_{0}} \cdot r_{2}e^{r_{2}t_{0}} - r_{1}e^{r_{1}t_{0}} \cdot e^{r_{2}t_{0}} = (r_{2} - r_{1}) \cdot e^{r_{1}t_{0} + r_{2}t_{0}} \neq 0$$

	Dans le deuxième cas, on a $\Delta = 0$ ; on peut alors prendre :
	$$ f_{1}(t_{0}) = e^{r_{0}t_{0}} \text{ et } f_{2}(t_{0}) = t_{0}e^{r_{0}t_{0}}$$
	Ainsi :
	$$ D = e^{r_{0}t_{0}} \left(r_{0}t_{0}e^{r_{0}t_{0}} + e^{r_{0}t_{0}} \right) - r_{0}e^{r_{0}t_{0}} \times t_{0}e^{r_{0}t_{0}} = e^{2r_{0}t_{0}} \neq 0$$
	On remarque alors que, dans les deux cas, $D \neq 0$, donc le système $(2, 2)$ étudié admet une unique solution, donc il existe un unique couple $(\lambda, \mu)$ le vérifiant d'où l'unicité et existence d'une solution au problème de Cauchy.
	\newline\newline

	\textbf{Cas 2. } $\mathbb{K} = \mathbb{R}$ \\
	$(a_{0}, a_{1}, a_{2}) \in \mathbb{R}^{2} \times \mathbb{R}^{*},(\alpha_{0}, \alpha_{1}) \in \mathbb{R}^{2}, b \in C^{0}(J, \mathbb{R})$
	\newline
	\textbf{Existence :} Puisque $\mathbb{R} \subset \mathbb{C}$, le problème de Cauchy admet, dans $\mathbb{R}$, une solution à valeurs complexes $g$. Posons $f = \Re(g)$ et montrons que $f$ est une solution réelle du problème de Cauchy. \\
	\begin{itemize}
	    \item[$\star$] $g \in \mathcal{D}^{2}(J, \mathbb{C}) \text{ donc } f \in \mathcal{D}^{2}(J, \mathbb{R})$
	    \item[$\star$] $g$ vérifie $a_{2}g'' + a_{1}g' + a_{0}g = b$ sur $J$ donc en prenant $\Re(\cdot)$ :
	    $$\begin{array}{ccl}
	      \Re(a_{2}g'' + a_{1}g' + a_{0}g = b) = \Re(b)
	      &\iff &a_{2}\Re(g'') + a_{1}\Re(g') + a_{0}\Re(g) = b  \\\\
	      &\iff & a_{2}f'' + a_{1}f' + a_{0}f = b \text{ sur } J
	    \end{array}$$
	    \item[$\star$] $f(t_{0}) = \Re(g(t_{0})) = \Re(\alpha_{0}) = \alpha_{0}$
	    \item[$\star$] $f'(t_{0}) = \Re(g(t_{0}))' = \Re(g'(t_{0})) = \Re(\alpha_{1}) = \alpha_{1}$
	\end{itemize}
	Donc $f$ est une solution réelle définie sur $J$ au problème de Cauchy.
	\newline

	\textbf{Unicité : }Soient $f_{1}$ et $f_{2}$ deux fonctions à valeurs réelles solutions du problème de Cauchy ci-dessus fixées quelconques : puisque $\mathbb{R} \subset \mathbb{C}$, $f_{1}$ et $f_{2}$ sont des fonctions à valeurs dans $\mathbb{C}$ solutions du même problème de Cauchy; or il y a unicité de la solution au problème de Cauchy dans les fonctions à valeurs complexes, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{C})$, donc $f_{1} = f_{2}$ dans $\mathcal{F}(J, \mathbb{R})$.
	\end{question_kholle}

	\begin{question_kholle}[
		Soient $(a,b)\in \mathbb{C}^2$, $f$ et $g$ les  solutions, définies sur $\mathbb{R}$ à valeurs
		dans $\mathbb{C}$, des problèmes de Cauchy suivants :
		\[
		    \left\{ \begin{array}{cl}
		        y'' +ay'+by = 0 \\
		        y(3) = 1\\
		        y'(3) = 0
		        \end{array} \right.
			\quad \text{et} \quad
		    \left\{ \begin{array}{cl}
		        y'' +ay'+by = 0 \\
		        y(3) = 0\\
		        y'(3) = 1
		        \end{array} \right.
		\]

		Comment s'exprime la solution définie sur $\mathbb{R}$ de $\left\{ \begin{array}{cl}
		    y'' +ay'+by = 0 \\
		    y(3) = \alpha \\
		    y'(3) = \beta
		    \end{array} \right. $ pour $(\alpha, \beta)\in \mathbb{R}^2$ fixés ?

		Peut-on affirmer que le plan vectoriel des solutions définies sur $\mathbb{R}$ à valeurs dans
		$\mathbb{C}$ de $y'' + ay' + by = 0$ est $\{ \lambda \cdot f + \mu \cdot g  |
		(\lambda, \mu)\in \mathbb{C}^2\}$
		]
		{Les solutions d'une EDL$_2$ constituent un espace vectoriel.}

	    La solution s'exprime simplement comme combinaison linéaire de f et g, plus précisément, la
	    combinaison linéaire en $\alpha$ et $\beta$. En effet, soient de tels scalaires, et soient $f$ et
	    $g$ de telles solutions, on a :
	    \[
	        (\alpha \cdot f + \beta \cdot g)'' + a (\alpha \cdot f + \beta \cdot g)' + b (\alpha \cdot f +
	        \beta \cdot g) = 0 \text{, par définition des espaces vectoriels.}
	    \]
	    Et de même, $(\alpha \cdot f + \beta \cdot g)'(3) = \alpha \cdot f'(3) + \beta \cdot g'(3) = \alpha$,
	    et $(\alpha \cdot f + \beta \cdot g)''(3) = \alpha \cdot f''(3) + \beta \cdot g''(3) = \beta$.
	    \newline
	    Ce qui suffit par unicité des solutions ( de la donc) d'un problème de Cauchy dans le cadre du
	    théorème du cours.
	    \newline
	    Pour ce qui est du plan vectoriel des solutions, noté $\Omega$, notons aussi $\Phi$ l'ensemble proposé.
	    L'inclusion $\Phi \subset \Omega$ est triviale par propriété de linéarité des espaces vectoriels.
	    Finalement, pour $\Omega \subset \Phi$, soit $\omega \in \Omega$, forcément, $\omega$ vérifie
	    l'$EDL_2$, mais aussi des conditions de Cauchy bien que celles-ci soient non-spécifiées, ainsi
	    posons $\omega'(3) = \delta$ et $\omega''(3) = \theta$, donc en particulier, $ \omega =
	    \delta \cdot f + \theta \cdot g$, d'où l'égalité par double inclusion.
	\end{question_kholle}

	\begin{question_kholle}
		[
		Résolution générale des systèmes linéaires à 2 équations et 2 inconnues en fonction du déterminant du systèmes (\textbf{tous les cas ne sont pas nécessairement à envisager})

		Considérons le système linéaire à deux équations et à deux inconnues $(x,y)$ :
		\begin{equation}
			(S)
			\left\{
				\begin{matrix}
					ax + by = b_1 &(E_1) \\
					cx + dy = b_2 &(E_2)
				\end{matrix}
			\right.
		\end{equation}
		dont $(a,b,c,d) \in \K^4$ sont les coefficients et $(b_1,b_2) \in \K^2$ sont les seconds membres.

		\begin{enumerate}
			\item (S) admet une unique solution si et seulement si
			$\begin{vmatrix}
				a & b \\
				c & d
			\end{vmatrix}
			= ad - bc \neq 0$. De plus, dans ce cas, la solution est
			\begin{equation}
				\left(
					\frac
						{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}
						{\begin{vmatrix}a&b\\c&d\end{vmatrix}},
					\frac
						{\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}
						{\begin{vmatrix}a&b\\c&d\end{vmatrix}}
				\right)
			\end{equation}
			\item Si $ad - bc = 0$, alors l'ensemble des solutions est soit vide, soit une droite affine de $\K^2$, soit $\K^2$.
		\end{enumerate}
		]
		{Formules de Cramer pour les systèmes 2 $\times$ 2}
		Procédons par disjonction de cas.

		\begin{itemize}[label=$\bullet$ Supposons]
			\item que $ad - bc \neq 0$.
			\begin{itemize}[label=$\bullet$ Supposons]
				\item que $a \neq 0$.
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{
							\begin{array}{cccccc}
								ax &+& by &=& b_1 \\
								&&\left(d - \frac{bc}{a}\right)y &=& b_2 - \frac{c}{a} b_1 &(L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
							\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{cccccc}
							ax &+& by &=& b_1 \\
							&&\left(ad - bc\right)y &=& a b_2 - c b_1 &(L_1 \leftarrow aL_1) \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccc}
							ax &=& \frac{1}{a} \left(b_1 - b\frac{ab_2 - cb_1}{ad - bc}\right) = \frac{1}{a} \frac{adb_1 - bcb_1 + abb_2 - bcb_2}{ad - bc} \\
							y &=& \frac{ab_2 - cb_1}{ad - bc} \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccccc}
							ax &=& \frac{db_1 - bb_2}{ad - bc} &=& \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}} \\
							y &=& \frac{ab_2 - cb_1}{ad - bc} &=& \frac{\begin{vmatrix}a&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}a&b\\c&d\end{vmatrix}}\\
						\end{array}
						\right.
					\end{aligned}
				\end{equation*}
				Donc le système admet une unique solution qui est celle annoncée.

				\item que a = 0. L'hypothèse $ad - bc \neq 0$ implique $bc \neq 0$ donc $b \neq 0$ et $c \neq 0$.
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{
						\begin{array}{ccccc}
							&& by &=& b_1 \\
							cx &+& dy &=&  b_2 \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccc}
							x &=&  \frac{1}{c} \left( b_2 - d\frac{b_1}{b} \right) \\
							y &=& \frac{b_1}{b} \\
						\end{array}
						\right. \\
						&\iff \left\{
						\begin{array}{ccccc}
							ax &=& \frac{db_1 - bb_2}{- bc} &=& \frac{\begin{vmatrix}b_1&b\\b_2&d\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}} \\
							y &=& \frac{- cb_1}{- bc} &=& \frac{\begin{vmatrix}0&b_1\\c&b_2\end{vmatrix}}{\begin{vmatrix}0&b\\c&d\end{vmatrix}}\\
						\end{array}
						\right.
					\end{aligned}
				\end{equation*}
			\end{itemize}
			Donc le système admet une unique solution qui est celle annoncée.
		\end{itemize}

		\item $ad - bc = 0$.
		\begin{itemize}[label=$\bullet$ Supposons]
			\item $a \neq 0$. En reprenant la méthode pivot de Gauss,
			\begin{equation*}
				\begin{aligned}
					(S)
					&\iff \left\{
					\begin{array}{cccccc}
						ax &+& by &=& b_1 \\
						&&\left(d - \frac{bc}{a}\right)y &=& b_2 - \frac{c}{a} b_1 &(L_1 \leftarrow L_1 - \frac{c}{a} L_2) \\
					\end{array}
					\right. \\
					&\iff \left\{
					\begin{array}{cccccc}
						ax &+& by &=& b_1 \\
						&& \underbrace{\left(ad - bc\right)}_0 y &=& a b_2 - c b_1 &(L_1 \leftarrow aL_1) \\
					\end{array}
					\right. \\
				\end{aligned}
			\end{equation*}
			Donc le système est de rang 1 avec une condition de compatibilité. \\
			Si $ab_2 - cb_1 \neq 0$, (S) n'admet aucune solution. \\
			Sinon $ab_2 - cb_1 = 0$
			\begin{equation}
				(S) \iff
				ax + by = b_1 \iff
				\begin{pmatrix} x \\ y \end{pmatrix} \in \left\{
					\begin{pmatrix} \frac{b_1}{a} - b\frac{t}{a} \\ t \end{pmatrix}
					|\; t \in \K
				\right\}
			\end{equation}
			Donc (S) admet un droite affine de solutions.

			\item $a = 0$. Puisque $ad - bc = 0$, alors $bc = 0$ donc b ou c est nul.

			\begin{itemize}[label=$\bullet$ Si]
				\item $c = 0$,
				\begin{equation*}
					(S) \iff
					\left\{ \begin{array}{ccc}
							by &=& b_1 \\
							dy &=& b_2
					\end{array} \right.
				\end{equation*}

				\begin{itemize}[label=$\bullet$ Si]
					\item $b = 0$,
					\begin{equation*}
						(S) \iff
						\left\{ \begin{array}{ccc}
							by &=& b_1 \\
							0 &=& b_2
						\end{array} \right.
					\end{equation*}
					\begin{itemize}[label=$\bullet$ Si]
						\item $b_2 = 0$, (S) n'admet aucune solution.
						\item $b_2 \neq 0$, $(S) \iff dy = b_2$
							\subitem$\bullet$ Si $d = 0$, $(S) \iff 0 = b_2$. (S) n'admet aucune solution ($b_2 \neq 0$) ou admet $\K^2$ comme ensemble des solutions ($b_2 = 0$).
							\subitem$\bullet$ Si $d \neq 0$, $(S) \iff y = \frac{b_2}{d} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_2}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$. Donc (S) admet une droite affine de solutions.
					\end{itemize}
					\item $b \neq 0$
					\begin{equation*}
						(S) \iff
						\left\{ \begin{array}{ccc}
							y &=& \frac{b_1}{b} \\
							0 &=& b_2 - \frac{db_1}{b}
						\end{array} \right.
					\end{equation*}
					\begin{itemize}[label=$\bullet$ Si]
						\item $b_2 - \frac{db_1}{b} \neq 0$, (S) n'admet aucune solution.
						\item $b_2 - \frac{db_1}{b} = 0$, $(S) \iff y = \frac{b_1}{b} \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} t \\ \frac{b_1}{d} \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
					\end{itemize}
				\end{itemize}
				\item $c \neq 0$ alors $b = 0$
				\begin{equation*}
					\begin{aligned}
						(S)
						&\iff \left\{ \begin{array}{ccc}
							0 &=& b_1 \\
							cx + dy &=& b_2
						\end{array} \right.
					\end{aligned}
				\end{equation*}
				\begin{itemize}[label=$\bullet$ Si]
					\item $b_1 \neq 0$, (S) n'admet aucune solution.
					\item $b_1 = 0$, $(S) \iff x = \frac{b_2}{c} - \frac{d}{c}y \iff \begin{pmatrix} x \\ y \end{pmatrix} \in \begin{Bmatrix} \begin{pmatrix} \frac{b_2}{c} - \frac{d}{c}t \\ t \end{pmatrix} |\; t \in \K \end{Bmatrix}$ donc (S) admet une droite affine de solutions.
				\end{itemize}
			\end{itemize}
		\end{itemize}

	\end{question_kholle}

\pagebreak\section{Semaine 9}
	
	\begin{question_kholle}
		[\noindent Soit \Rel une relation d'équivalence sur $E$. \\
		Soit $x \in E$. \\
		La classe de $x$, notée $\bar{x}$, est l'ensemble des éléments de $E$ en relation avec x.
		\begin{equation}
			\bar{x} = \left\{ y \in E \;|\; x \Rel y \right\}
		\end{equation}]
		{Deux classes d'équivalence sont disjointes ou confondues. Les classes d'équivalence constituent une partition de l'ensemble sur lequel on considère la relation d'équivalence.}
		
		\textit{Montrons que deux classes d'équivalence sont disjointes ou confondues.}
		
		Soit $(x, y) \in E^2$ fq.
		\begin{itemize}[label=\textemdash]
			\item Si $\bar{x} \cap \bar{y} = \emptyset$, rien à démontrer.
			\item Sinon $\bar{x} \cap \bar{y} \neq \emptyset$ donc $\exists z \in \bar{x} \cap \bar{y}$. Fixons un tel $z$.

			Soit $x' \in \bar{x}$ fq.
			\begin{equation*}
				\left.
				\begin{matrix}
					\left. \begin{matrix}
						x' \in \bar{x} \implies x \Rel x' \underset{sym\acute{e}trie}{\implies} x' \Rel x \\
						z \in \bar{x} \implies x \Rel z
					\end{matrix}
					\right\} \underset{transitivit\acute{e}}{\implies} x' \Rel z \\
					z \in \bar{y} \implies y \Rel z \underset{sym\acute{e}trie}{\implies} z \Rel y
				\end{matrix}
				\right\} \underset{transitivit\acute{e}}{\implies} x' \Rel y
				\underset{sym\acute{e}trie}{\implies} y \Rel x'
			\end{equation*}
			
			Donc $x' \in \bar{y}$ donc $\bar{x} \subset \bar{y}$.
			
			En échangeant les rôles de $x$ et $y$, on montre la deuxième inclusion $\bar{y} \subset \bar{x}$.
		\end{itemize}
		\bigbreak
	
		\textit{Montrons que les classes d'équivalence de E constituent une partition de E.}
		
		Soit $\Sol$ un système de représentant des classes fixé quelconque.
		
		\begin{itemize}[label=\textemdash]
			\item Soit $s\in \Sol$ fq. $\bar{s} \neq \emptyset$ car $s \Rel s$ par réflexivité.
			\item Soit $(s, s') \in \Sol^2$ fq. D'après la démonstration ci-dessus ci-dessus, $\bar{s} \cap \bar{s'} = \emptyset$ ou $\bar{s} = \bar{s'}$. Si $\bar{s} = \bar{s'}$ alors $s$ et $s'$ représente la même classe ce qui est impossible car un système de représentants des classes contient un unique représentant de chaque classe. Par conséquent, $\bar{s}$ et $\bar{s'}$ sont disjoints.
			\item $\underset{s \in \Sol}{\bigcup} \bar{s} \subset E$ car $\forall s \in \Sol, \bar{s} \in E$ par définition d'une classe d'équivalence. \\
			Réciproquement, soit $x \in E$ fq. \\
			Par réflexivité de \Rel, $x \in \bar{x}$. \\
			Par définition d'un système de classe $\exists ! s_x \in \Sol : s_x \in \bar{x}$ donc $\bar{s_x} = \bar{x}$. Donc $x \in \bar{s_x} \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. Donc $E \subset \underset{s \in \Sol}{\bigcup} \bar{s}$. \\
			Par double inclusion, $E = \underset{s \in \Sol}{\bigcup} \bar{s}$.			
		\end{itemize}
		
		Ainsi,
		\begin{equation}
			E = \bigsqcup_{s \in \Sol} \bar{s}
		\end{equation}
		
	\end{question_kholle}

	\begin{question_kholle}
		[\noindent Soit $(E, \leq)$ un ensemble ordonné, et $A$ une partie non-vide de $E$. \\
		Si $A$ admet un plus grand élément alors $A$ admet une borne supérieure et $\sup{A} = \max{A}$. \\
		Si $A$ admet une borne supérieure appartenant à elle-même alors $A$ admet un plus grand élément et $\max{A} = \sup{A}$.]
		{Si $A$ admet un plus grand élément c'est aussi sa borne supérieure. Si $A$ admet une borne supérieure dans $A$ c'est sont plus grand élément.}
		
		Soient un tel ensemble $E$ et une telle partie $A$ et notons $M$ son plus grand élément. \\
		Posons l'ensemble des majorants de $A$, $M(A) = \{ m\in E \ | \ \forall a \in A, \ a \leq m\}$. \\
		Par définition : 
		\[
		\forall m \in M(A), \ M \leq m,
		\]
		car $M\in A$, mais comme $M\in M(A)$, on a directement que $M = \min{M(A)} = \sup{A}$. \\
		
		Pseudo-réciproquement, soit $A$ une partie de $E$ admettant une borne supérieure dans elle même, notons cette borne $S$. \\
		Comme $S \in M(A)$, par définition, $S$ est plus grand que tous les éléments de $A$ mais appartient à $A$, donc de tous les éléments de $A$, $S$ est le plus grand.
	\end{question_kholle}

	\begin{question_kholle}
		[\begin{equation}
			\forall (a, b) \in \Z^2,
			\exists ! (q, r) \in \Z \times \N :
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in {[\![} 0 ; |b|-1 {]\!]}
			\end{matrix} \right.
		\end{equation}]
		{Théorème de la division Euclidienne dans \Z}
		
		\textit{Unicité} \;
		Soient deux tels entiers $(a,b) \in \Z^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \N\right)^2$ tels que
		\begin{equation*}
			\left\{ \begin{matrix}
				a = b q + r \\
				0 \leqslant r \leqslant |b| - 1
			\end{matrix} \right.
			\qquad
			\left\{ \begin{matrix}
				a = b q' + r' \\
				0 \leqslant r' \leqslant |b| - 1
			\end{matrix} \right.
		\end{equation*}
		Directement, 
		\[
		b(q-q') = r'-r,
		\]
		mais comme $-(|b|-1) \leqslant r' - r \leqslant |b| -1$, il vient en divisant par $|b|$ l'inégalité précédente :
		\[
		-1 < q - q' < 1,
		\]
		puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
		\newline
		\\
		\textit{Existence} \; Posons pour $b \geqslant 1$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
		\begin{itemize}
			\item $\Omega \subset \Z$
			\item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots) 
			\item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
		\end{itemize}
		Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
		Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $\Omega \subset \Z$, $q \in \Z$ donc $r \in \Z$.
		\\
		Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r\in [\![ 0, |b| -1 ]\!]$.
		
		Si $b< 1$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
	\end{question_kholle}

	\begin{question_kholle}{Une suite décroissante et minorée de nombres entiers relatifs est stationnaire}
		Soit $u \in \Z^\N$ une suite décroissante et minorée fixée quelconque. \\
		Considérons $A = \{ u_n \;|\; n \in \N \}$ c'est-à-dire l'ensemble des valeurs prises par la suite $u$. \\
		$A$ est : \begin{itemize}[label=\textemdash]
			\item une partie de \Z car $u$ est à valeur dans \Z
			\item non vide car $u_0 \in A$
			\item minoré car $u$ est minorée
		\end{itemize}
		Donc $A$ admet un plus petit élément. Donc $\exists n_0 \in \N: u_{n_0} = min A$. Fixons un tel $n_0$. \\
		Soit $n \in \N$ fq tq $n \geqslant n_0$.
		\begin{equation*}
			\left. \begin{matrix}
				u_n \in A \implies u_n \geqslant min A = u_{n_0} \\
				 u \text{ est décroissante et } n \geqslant n_0 \text{ donc } u_n \leqslant u_{n_0}
			\end{matrix}
			\right\} \implies u_n = u_{n_0}
		\end{equation*}
  		Ainsi, $u$ est stationnaire.
	\end{question_kholle}
\pagebreak\section{Semaine 10}

	\begin{question_kholle}
		[\noindent Soient $(A, B) \in \mathcal{P}(\R)^2$ fq. \\
		\textit{Définition de la densité}
		\begin{align}
			A \text{ est dense dans } B
			\text{ si } \left\{ \begin{array}{ll}
				A \subset B \\
				\mathrm{et} \\
				\forall (u,v) \in \R^2, B \cap {]}u;v{[} \neq \emptyset \implies A \cap  {]}u;v{[} \neq \emptyset
			\end{array} \right.
		\end{align}
		\textit{Caractérisation de la densité par les $\varepsilon$}
		\begin{align}
			A \text{ est dense dans } B
		 	\iff \left\{ \begin{array}{ll}
				A \subset B \\
				\mathrm{et} \\
				\forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon
			\end{array} \right.
		\end{align}
		]
		{Caractérisation de la densité d’une partie $A$ de \R dans une partie $B$ de \R la contenant avec des $\varepsilon$.}

		\textit{Montrons la caractérisation de la densité}\\
		\emph{Sens Direct} Supposons $A$ dense dans $B$
		\begin{itemize}[label=\textemdash]
            \item Par déf $A \subset B$
            \item Soit $b \in B$ et $\varepsilon \in \R_+^*$ fq

            Appliquons le (ii) de la déf de Densité pour $u \leftarrow b - \varepsilon$ et $v \leftarrow b + \varepsilon$
            $$B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset \implies A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$$
            Or, $B \cap ]b - \varepsilon, b + \varepsilon[ \neq \emptyset$ est vraie
            donc $A \cap ]b - \varepsilon,  b + \varepsilon[ \neq \emptyset$

            Ce qui permet de choisir $a \in A \cap ]b - \varepsilon,  b + \varepsilon[$.
            Un tel $a$ vérifie $a \in A$ et $a \in ]b - \varepsilon,  b + \varepsilon[ \iff |b-a| < \varepsilon$
		\end{itemize}
		\bigbreak
    	\noindent \emph{Sens réciproque} Supposons $\left\{\begin{array}{ll} A \subset B \\\mathrm{et}\\ \forall b \in B, \forall \varepsilon \in \R_+^*, \exists a \in A: |b-a|< \varepsilon \end{array}\right.$

        \begin{itemize}
            \item On a donc $A \subset B$
            \item Soient $(u, v) \in \R^2$ fq tq $B \cap ]u, v[ \neq \emptyset$

			Soit $b \in B \cap ]u, v[$ fq.
            Appliquons l'hypothèse pour $b\leftarrow b$ et $\varepsilon \leftarrow \min\{v - b, b - u\}$, qui est autorisé $v-b$ et $b-u$ sont positifs

            Donc $\exists a \in A: | b - a| < \varepsilon $

            Fixons un tel a, alors:
            $$
                b-\varepsilon < a < b + \varepsilon
            $$

            Donc $$
            \left\{\begin{array}{ll}
            a < b + \varepsilon = b + \underbrace{\min\{v - b, b - u\}}_{\leqslant v - b} \leqslant b + v - b = v \\ \mathrm{et}\\
            a > b - \varepsilon = b - \underbrace{\min\{v - b, b - u\}}_{\leqslant b - u} \geqslant b - (b - u) = u
            \end{array}\right.
            $$

            Donc $a \in ]u, v[$.
        \end{itemize}
        Donc $A \cap ]u, v[ \neq \emptyset$

	\end{question_kholle}

	\begin{question_kholle}
		[\begin{equation}
			\forall (a, b) \in \R \times \R^*,
			\exists ! (q, r) \in \Z \times \R :
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in [0;|b|[
			\end{matrix} \right.
		\end{equation}]
		{Théorème de la division pseudo-euclidienne dans \R}

		\textit{Unicité} \;
		Soient deux tels entiers $(a,b) \in \R^2$ et deux couples $((q,r),(q',r')) \in \left(\Z \times \R\right)^2$ tels que
		\begin{equation*}
			\left\{ \begin{matrix}
				a = b q + r \\
				r \in [0;|b|[
			\end{matrix} \right.
			\qquad
			\left\{ \begin{matrix}
				a = b q' + r' \\
				r' \in [0;|b|[
			\end{matrix} \right.
		\end{equation*}
		Directement,
		\[
		b(q-q') = r'-r,
		\]
		mais comme $-|b| < r' - r < |b|$, il vient en divisant par $|b|$ l'inégalité précédente :
		\[
		-1 < q - q' < 1,
		\]
		puisque $q$ et $q'$ sont dans $\Z$ leur différence est obligatoirement $0$, ainsi $q = q'$ ce qui implique $ r= r'$ et donc on a unicité de ladite écriture de $a$.
		\newline
		\\
		\textit{Existence} \; Posons pour $b > 0$, $\Omega = \{ k\in \Z  \ | \ kb \leqslant a \}$
		\begin{itemize}
			\item $\Omega \subset \Z$
			\item non-vide car $-|a| \in \Omega$ ($\Z$ archimédien suffit \ldots)
			\item $\Omega$ est majoré par $|a|$ car supposons, par l'absurde, que $\exists k \in \Omega : k > |a|$, alors $kb > |a|b > a$ ce qui contradiction avec la définition d'$\Omega$.
		\end{itemize}
		Donc $\Omega$ admet un plus grand élément, notons-le $q$. \\
		Posons $r = a - bq$. Par construction, $a = bq + r$ et comme $q = \max \Omega$ et $r \in \R$.
		\\
		Par suite, $q \in \Omega$ donc $bq \leqslant a$ d'où $0 \leqslant r$. Et $q = \max \Omega$ donc $b(q+1) > a$ d'où $b > r$, c'est-à-dire, $r \in [ 0, |b| [$.

		Si $b < 0$, il suffit de prendre $q \leftarrow -q$ dans la preuve précédente.C'est donc l'existence de ladite écriture de $a$.
	\end{question_kholle}

	\begin{question_kholle}
		{\Q est dense dans \R et $\R \setminus \Q$ est aussi dense dans \R}

		Soit $x \in \R$ fq.
		Posons $\forall n \in \N, a_n = \frac{\lfloor2^n x\rfloor}{2^n}$. \\
		Soit $n \in \N$ fq. \\
		\begin{itemize}
			\item $a_n \in \Q$ car $\lfloor2^n x\rfloor \in \Z$ et $2^n \in \N$.
			\item \begin{equation*}
					a_n = \frac{\lfloor2^n x\rfloor}{2^n}
					\implies \frac{2^n x - 1}{2^n} \leqslant a_n \leqslant \frac{2^n x}{2^n}
					\implies x - \frac{1}{2^n} \leqslant a_n \leqslant x
				\end{equation*}
				Or $\nicefrac{1}{2^n} \arrowlim{n}{+\infty} 0$ donc d'après le théorème d'existence de limite par encadrement, \\ $a_n \arrowlim{n}{+\infty} x$.
		\end{itemize}
		Donc d'après la caractérisation séquentielle de la densité, \Q est dense dans \R.
		\bigbreak

		\noindent Soit $x \in \R$ fq. \\
		Alors $x + \sqrt{2} \in \R$.
		D'après la démonstration précédente, $\exists b \in \Q^\N : b_n \arrowlim{n}{+\infty} x + \sqrt{2}$. \\
		Fixons un telle suite $b$.
		Considérons $c = b - \sqrt{2}$. \\
		Soit $n \in \N$ fq.
		\begin{itemize}
			\item $c_n \in \R\setminus\Q$ car $b_n \in \Q$ et $\sqrt{2} \in \R \setminus \Q$.
			\item \begin{equation*}
				\left. \begin{matrix}
					b_n \arrowlim{n}{+\infty} x + \sqrt{2} \\
					c_n = b_n - \sqrt{2}
				\end{matrix} \right\}
				\implies c_n \arrowlim{n}{+\infty} x
			\end{equation*}
		\end{itemize}
		Donc d'après la caractérisation séquentielle de la densité, $\R\setminus \Q$ est dense dans \R.
	\end{question_kholle}

	\begin{question_kholle}[
        Soit u $\in \K ^ \N, (\ell_1, \ell_2) \in \K ^2$
        Si u converge vers $\ell_1$ et $\ell_2$, alors $\ell_1 = \ell_2$
    ]{Preuve de l'unicité de la limite d'une suite convergente}
    Par l'absurde, supponsons que $u$ converge vers $\ell_1$ et $\ell_2$, et $\ell_1 \neq \ell_2$.
    On prendra $\varepsilon_0 = \varepsilon_1 = \varepsilon_2$ assez petit pour que les tubes soient disjoints.\\
    Posons donc $\varepsilon_0 = \frac{|\ell_1 - \ell_2|}{3}$
    \begin{itemize}
        \item Appliquons la définition de la convergence de u vers $\ell_1$, pour $\varepsilon \leftarrow \varepsilon_0$, ce qui est autorisé car $\varepsilon_0 \in \R_+^*$
        \begin{equation}\label{eq:1}
            \exists N_1 \in \N : \forall n \in \N, n \geqslant N_1 \implies |u_n - \ell_1| \leqslant \varepsilon_0
        \end{equation}
        \begin{equation}\label{eq:2}
            \exists N_2 \in \N : \forall n \in \N, n \geqslant N_2 \implies |u_n - \ell_2| \leqslant \varepsilon_0
        \end{equation}
        Fixons de tels $N_1$ et $N_2$.
        \item Posons $n_0 = N_1 + N_2$
        \begin{itemize}
            \item $n_0 \geqslant N_1$, donc (\ref{eq:1}) s'applique: $|u_{n_0} - \ell_1| \leqslant \varepsilon_0$
            \item $n_0 \geqslant N_2$, donc (\ref{eq:2}) s'applique: $|u_{n_0} - \ell_2| \leqslant \varepsilon_0$
        \end{itemize}
        \item \begin{align*}
            |\ell_1 - \ell_2| &= |\ell_1 - u_{n_0} + u_{n_0} - \ell_2|\\
            &\leqslant \underbrace{|\ell_1 - u_{n_0}|}_{\leqslant \varepsilon_0} + \underbrace{|u_{n_0} - \ell_2|}_{\leqslant \varepsilon_0}\\
            &\leqslant 2 \frac{|\ell_1 - \ell_2|}{3}\\
            \implies 1 &\leqslant \frac 2 3
        \end{align*}
        Contradiction
    \end{itemize}
	\end{question_kholle}

	\begin{question_kholle}{Une suite convergente est bornée}

        Soit $u \in \mathbb{K}^{\mathbb{N}}$ convergente.
Posons $\ell = \lim u$
Appliquons la définition de la convergence pour $\varepsilon \leftarrow 1$
$$
\exists N_{1}\in \mathbb{N}: \forall n \in \mathbb{N}, n \geqslant N_{1} \implies |u_{n}-\ell| \leqslant 1
$$
Fixons un tel $N_{1}$
Posons alors $M = \max\left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$, qui est bien défini, car toute partie finie, non vide d'un ensemble totalement ordonné (ici $(\mathbb{R}, \leqslant)$) admet un pgE.

Soit $n \in \mathbb{N}$ fq.
\begin{itemize}
    \item Si $n \in [[0, N_{1}]], |u_{n}| \in \left\{ |u_{0}|, |u_{1}|, |u_{2}| \dots |u_{N_{1}}|, |\ell|+1 \right\}$ donc $|u_{n}| \leqslant M$
	\item Sinon,
\end{itemize}

\begin{align*}
n> N_{1} &\implies |u_{n} - \ell| \leqslant 1 \\
&\implies |u_{n}| - |\ell| \leqslant 1 \\
 & \implies |u_{n}| \leqslant 1+ |\ell| \leqslant M
\end{align*}

Ainsi, $\forall n \in \mathbb{N}, |u_{n}| \leqslant M$.
    \end{question_kholle}
\pagebreak\section{Semaine 11}

	\begin{question_kholle}
		[Soient $(A,B) \in (\mathcal{P}(\R) \setminus \{\varnothing\})^{2}$. Montrons que :
		\\
		$$A \text{ est dense dans } B \iff \left\{ \begin{array}{l}
			A \subset B \\
			\forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b
		\end{array}\right. $$
		]
		{Caractérisation séquentielle de la densité.}
		
		Sens indirect : supposons $A \subset B$ et $\forall b \in B, \exists(a_{n}) \in A^{\N} : (a_{n}) \text{ converge vers }b$ :\\
		\begin{itemize}
			\item[$\star$] $A \subset B$ par hypothèse.
			\item[$\star$] Montrons que $\forall b \in B, \forall \varepsilon \in \R^{*}_{+}, \exists a \in A : |b - a| < \varepsilon$ (on utilise la caractérisation de la densité avec les $\varepsilon$) \\
			Soient $b \in B$ et $\varepsilon \in \R^{*}_{+}$ fixés quelconques : \\
			Par hypothèse appliquée pour $b \leftarrow b$ : $\exists(a_{n}) \in A^{\N} : a_{n} \underset{n \to +\infty}{\longrightarrow}b$ \\
			Appliquons la définition de la convergence de $(a_{n})$ vers $b$ pour $\varepsilon \leftarrow \frac{\varepsilon}{2}$ : \\
			$$\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow |a_{n} - b| \leqslant \frac{\varepsilon}{2}$$
			Fixons un tel N : \\
			En particulier, $a_{N} \in A$ et $|a_{N} - b| \leqslant \frac{\varepsilon}{2} \leqslant \varepsilon$ \\
			Donc $A$ est dense dans $B$.
		\end{itemize}
		
		Sens direct : supposons $A$ dense dans $B$ : \\
		\begin{itemize}
			\item[$\star$] Par définition, $A \subset B$
			\item[$\star$] Soit $b \in B$ fixé quelconque. \\
			Soit $n \in \N$ fixé quelconque :  \\
			Appliquons la caractérisation de la densité par les $\varepsilon$ pour $\varepsilon \leftarrow \frac{1}{2^{n}}$ (autorisé car $\frac{1}{2^{n}} > 0$), et $b \leftarrow b$ : 
			$$\exists a \in A : |a - b| \leqslant \frac{1}{2^{n}}$$
			Notons $a_{n}$ un tel élément. Nous venons de construire $(a_{n})_{n \in \N} \in A^{\N}$ vérifiant : \\
			$\forall n \in \N, |a_n - b| \leqslant \frac{1}{2^{n}}$ \\
			Or : $\underset{n \to +\infty}{\lim} \frac{1}{2^{n}} = 0$ \\
			Ainsi, d'après le théorème sans nom, $(a_{n})_{n \in \N}$ converge vers $b$.
		\end{itemize}
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $u \in \R^\N$ une suite monotone :
		{\begin{enumerate}
			\item Si $u$ est croissante
			\begin{enumerate}[label=($\roman*$)]
				\item Soit $u$ est majorée, et dans ce cas, $\lim u = \sup\{ u_k | k \in \N \}$
				\item Soit $u$ n'est pas bornée, et dans ce cas, $u$ diverge vers $+\infty$.
			\end{enumerate}
			\item Si $u$ est décroissante :
			\begin{enumerate}[label=($\roman*$) Soit, leftmargin=4em]
				\item $u$ est minorée, et dans ce cas, $\lim u = \inf\{ u_k | k \in \N \}$
				\item $u$ n'est pas bornée, et dans ce cas, $u$ diverge vers $-\infty$.
			\end{enumerate}
		\end{enumerate} }]
		{Théorème de la convergence monotone}
		
		Soit $u \in \R^\N$ monotone fq.
		
		\begin{enumerate}
			\item Supposons que $u$ est croissante.
			\begin{enumerate}[label=($\roman*$)]
				\item Supposons que $u$ est majorée. \\
				Alors $\exists M \in \R : \forall n \in \N, u_n \leqslant M$. Fixons un tel M. \\
				$\Omega = \{ u_k | k \in \N \}$ est \begin{itemize}
					\item une partie de \R
					\item non vide car $u_0$ y appartient
					\item majorée par M
				\end{itemize}
				donc elle admet un borne supérieure et notons-la $\sigma$. \\
				Soit $\epsilon \in \R_+^*$ fq. \\
				$\sigma - \epsilon < \sigma$ donc $\sigma - \epsilon$ ne majore pas $\Omega$. Donc $\exists N \in \N : u_N > \sigma - \epsilon$. Fixons un tel N. \\
				Soit $n \in \N$ fq tq $n \geqslant N$. \\
				Alors $u_n \underset{\text{par croissant de u}}{\geqslant} u_N \geqslant \sigma - \epsilon$ et $u_n \underset{\text{par défintion de }\sigma}{\leqslant} \sigma$. \\
				Ainsi,
				\begin{equation*}
					\begin{aligned}
						\sigma - \epsilon \leqslant u_n \leqslant \sigma
						&\implies - \epsilon \leqslant u_n - \sigma \leqslant 0 \\
						&\implies | u_n - \sigma | \leqslant \epsilon
					\end{aligned}
				\end{equation*}
				Donc $u_n \arrowlim{n}{+\infty} \sigma$.

				\item Supposons que $u$ n'est pas bornée. \\
				Soit $A \in \R$ fq. \\
				$u$ n'est pas bornée donc $\exists N \in \N : u_N > A$. \\
				Or $u$ est croissante donc $\forall n \in \N, n \geqslant N \implies u_n \geqslant A$. \\
				Donc $u_n \arrowlim{n}{+\infty} +\infty$.
			\end{enumerate}
		
			\item Supposons que $u$ est décroissante. \\
			Il suffit dans la preuve ci-dessus de remplacer les inégalités inférieures par des inégalités supérieures et inversement et d'utiliser la notion de borne inférieure plutôt que de borne supérieure.
			\begin{enumerate}[label=($\roman*$)]
				\item Si $u$ est minorée, $u_n \arrowlim{n}{+\infty} \inf\{u_k|k\in\N\}$.
				\item Si $u$ n'est pas bornée, $u_n \arrowlim{n}{+\infty} -\infty$.
			\end{enumerate}
		\end{enumerate}
	\end{question_kholle}
	
	\begin{question_kholle}
	    [Soit $u\in \R ^{\N}$ qui converge vers $\ell \in \R$. \\
	    Alors la moyenne arithmérique des $n\in \N$ premiers termes (appelée moyenne de Césarò) converge vers $\ell$.]
	    {Théorème de Césarò}
	
	    Soient $u$ une telle suite, $\varepsilon \in \R ^*_+$ et $\ell \in \R$ ladite limite de $u$. Appliquons la définition de la convergence de $u$ pour $\varepsilon \gets \frac{\varepsilon}{2}$ : 
	    \[
	    \exists N \in \N \ : \ \forall n \in \N , \ n\geq N \ \implies \ |u_n - \ell | \leq \frac{\varepsilon}{2}.
	    \]
	    Fixons un tel $N$. Posons $\omega = \sum_{k=0}^{N-1} |u_k - \ell | \in \R$. Soit $n\in \N$ tel que $n \geq N$. Calculons : 
	    \[
	    \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| = \left| \frac{1}{n} \left( \sum_{k=0}^{n-1}u_k - n\ell \right) \right|  = \left| \frac{1}{n} \sum_{k=0}^{n-1}(u_k - \ell)  \right| \leq \frac{1}{n} \underset{= \ \omega \in \R}{\underbrace{\sum_{k=0}^{N-1}|u_k - \ell|}} + \frac{1}{n} \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\sum_{k=N}^{n}|u_k - \ell|}} \leq \frac{\omega}{n} + \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\varepsilon}{2n}}}.
	    \]
	    Ces majorations sont issues de l'inégalité triangulaire et de la convergence de $u$. De plus, comme la suite $(v_n) _{n\in \N} = \left( \frac{\omega}{n} \right) _{n\in \N}$ converge vers $0$, on écrit sa définition pour $\varepsilon \gets \frac{\varepsilon}{2}$ : 
	    \[
	    \exists N' \in \N \ : \ \forall n \in \N , \ n\geq N' \ \implies \ |v_n| \leq \frac{\varepsilon}{2}.
	    \]
	    On fixe un tel $N'$ et on pose $\Lambda = \max{(N, N')}$ qui a bien un sens car $\{N, \ N'\}$ est une partie finie de $\N$.
	    De la même manière qu'auparavant, pour $n\in \N$ tel que $n \geq \Lambda$, on a : 
	    \[
	     \left| \frac{1}{n} \sum_{k=0}^{n-1}u_k - \ell \right| \leq \underset{\leq \ \frac{\varepsilon}{2}}{\underbrace{\frac{\omega}{n}}} + \frac{\varepsilon}{2} \leq \varepsilon.
	    \] 
	    C'est le théorème souhaité.
	\end{question_kholle}

	\begin{question_kholle}
		[Soient $(u,v) \in \R^{\N}$ : \\
		{\begin{enumerate}[label=($\roman*$)]
			\item Si $\begin{array}{|l}
				\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0   \\
				u \text{ converge}
			\end{array}$ \\
			Alors $\lim u \geqslant 0$
			\item Si $\begin{array}{|l}
				\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n}   \\
				u \text{ et } v \text{ convergent}
			\end{array}$ \\
			Alors $\lim u \leqslant \lim v$
		\end{enumerate}}
		]
		{Théorème de passage à la limite dans une inégalité.}
		~\smallbreak
		\begin{enumerate}[label=($\roman*$)]
			\item L'hypothèse $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \geqslant 0$ permet d'affirmer que $u$ et $|u|$ coïncident à partir d'un certain rang. \\
			Par ailleurs, la convergence de $u$ et la continuité de $|\cdot|$ sur $\R$ donc en $\lim u$ donnent $|u|$ converge vers $|\lim u|$. \\
			Le caractère asymptotique de la limite permet de conclure que $u$ et $|u|$ ont la même limite. \\
			Donc $\lim u = |\lim u| \geqslant 0$
			\item $\exists N \in \N : \forall n \in \N, n \geqslant N \Rightarrow u_{n} \leqslant v_{n} \Rightarrow v_{n} - u_{n} \geqslant 0$ \\
			$u$ et $v$ convergent $\Rightarrow v-u$ converge vers $\lim v - \lim u$. \\
			On applique $(i)$ pour $u \leftarrow v - u$, autorisé car $u \text{ et }v$ convergent. \\
			On obtient $\lim v - \lim u \geqslant 0$ d'où $\lim u \leqslant \lim v$.
		\end{enumerate}
	\end{question_kholle}

	\begin{question_kholle}
	    [Soient $u$ et $v$ deux suites réelles adjacentes. Alors $u$ et $v$ convergent et ont la même limite.]
	    {Théorème des suites adjacentes}
	
	    Soient $u$ et $v$ de telles suites. Quitte à inverser les rôles desdites suites, prenons $u$ croissante et $v$ décroissante. \\
	    On a donc : 
	    \[
	    \forall n \in \N, \ (u_n \leq v_n \leq \underset{\in \R}{\underbrace{v_0}}) \wedge (\underset{\in \R}{\underbrace{u_0}}\leq  u_n \leq v_n),
	    \]
	    car la monotonie des suites induit ces inégalités. D'après le théorème de limite monotone, $u$ étant croissante et majorée elle converge, $v$ étant décroissante et minorée elle converge. \\
	    Il s'en suit que par définition des suites adjacentes : 
	    \[
	    0 \ = \lim_{n \to +\infty} (u_n - v_n) \ \underset{u,v \ \text{ convergent}}{\underbrace{=}} \ \lim_{n \to +\infty} u_n - \lim_{n \to +\infty} v_n.
	    \]
	    Ainsi, $\lim u = \lim v$.
	\end{question_kholle}

	\begin{question_kholle}
		[Toute suite bornée réelle admet une sous-suite convergente. \\
		L'ensemble des valeurs d'adhérence d'une suite réelle bornée est non vide.]
		{\emph{Facultative} Théorème de Bolzano-Weierstrass}

		Soit $u \in \R^\N$ fq bornée. \\
		Alors $\exists M \in \R_+ : \forall n \in \N, |u_n| \leqslant M$.

		Construisons une suite de segments dans $[-M;M]$ de plus en plus petits par dichotomie. \\
		Posons $a_0 = -M$, $b_0 = M$ et définissons les suites $c$ et $I$ pour tout $n$ dans \N par $c_n = \frac{a_n + b_n}{2}$ et $I_n = [a_n;b_n]$. \\
		
		\noindent Soit $n\in \N$ fq.
		Supposons $a_n \text{ et } b_n$ construits et $\{ k \in \N \;|\; u_k \in I_n \}$ infini.
		Construisons les termes d'indices $n+1$. \\
		Posons $\left| \begin{array}{lcr}
			I_n^- &=& \{ k \in \N \;|\; u_k \in [a_n;c_n] \} \\
			I_n^+ &=& \{ k \in \N \;|\; u_k \in [c_n;b_n] \} \\
		\end{array} \right.$ \\
		Nous avons $I_n^- \cup I_n^+ = \{ k \in \N \;|\; u_k \in I_n \}$ donc $I_n^-$ ou $I_n^+$ est infini.

		\begin{itemize}
			\item Si $I_n^-$ est infini, posons $\left| \begin{array}{lcl}
				a_{n+1} &=& a_n \\
				b_{n+1} &=& c_n
			\end{array} \right.$ \\
			Ainsi $\{ k \in \N \;|\; u_k \in I_{n+1} \} = I_n^-$ est infini.
			\item Si $I_n^+$ est infini, posons $\left| \begin{array}{lcl}
				a_{n+1} &=& c_n \\
				b_{n+1} &=& b_n
			\end{array} \right.$ \\
			Ainsi $\{ k \in \N \;|\; u_k \in I_{n+1} \} = I_n^+$ est infini.
		\end{itemize}
		\bigbreak

		\noindent Étudions la suite $\left(I_n\right)_{n\in\N}$.
		\begin{itemize}
			\item Nous avons toujours $a_n \leqslant b_n$ donc $\forall n \in \N, I_n \neq \emptyset$
			\item Par construction, $\forall n \in \N, I_{n+1} \subset I_n$
			\item $ |I_{n+1}| = |a_{n+1}-b_{n+1}| = \frac{1}{2} |a_n-b_n| = \frac{1}{2} |I_n| $
			donc la suite des cardinaux est une suite géométrique de raison $\nicefrac{1}{2}$.
			Donc $|I_n| \arrowlim{n}{+\infty} 0$.
		\end{itemize}
		Donc, d'après le théorème des segments emboîtés, $\exists ! l\ell \in \R : \underset{n\in\N}{\bigcap} I_n = \{\ell\}$. Fixons un tel $\ell$. \\

		Construisons maintenant une extractrice $\varphi$ de $u$. \\
		Posons $\varphi(n) = 0$. \\
		Soit $n \in \N$ fq. Supposons $\varphi(n)$ construite.
		\begin{equation*}
			\varphi(n+1) = \min\{ k \in \N | u_k \in I_{n+1} \wedge k > \varphi(n) \}
		\end{equation*}
		$\varphi(n+1)$ est bien définie car $\{ k \in \N | u_k \in I_{n+1} \}$ est une partie de \N non bornée (car infinie). \\
		
		\noindent Ainsi, nous avons construit $\varphi : \N \rightarrow \N$ strictement croissante. Nous pouvons extraire une sous-suite de $u$. Or $\forall n \in \N, u_{\varphi(n)} \in I_n$ donc
		\begin{equation*}
			\forall n \in \N, \quad
			\underbrace{a_n}_{ \arrowlim{n}{+\infty} \ell } \leqslant u_{\varphi(n)} \leqslant \underbrace{b_n}_{ \arrowlim{n}{+\infty} \ell }
		\end{equation*}
		Donc, d'après le théorème d'existence de limite par encadrement, $u_{\varphi(n)} \arrowlim{n}{+\infty} \ell$. \\
		Ainsi $\ell \in L_u$.
	\end{question_kholle}
	
	\begin{question_kholle}
	    [Soit $u$ une suite bornée. $u$ converge si et seulement si il existe $\ell \in \mathbb{K}$ tel que $L(u)$ est le singleton $\ell$ ]
	    {\emph{Facultative} Caractérisation de la convergence par l'unicité d'une valeur d'adhérence pour une suite bornée.}
	    Traitons le cas réel, celui sur \C est à adapter sans peine.\\
	    Supposons que $u$ converge et posons $\lim u =\ell \in \R  $. Toutes les sous-suites de $u$ convergent vers $\ell$ donc $L(u)=\{\ell \}$. \\
	    Supposons maintenant qu'il existe un unique $\ell \in \R$ tel que $L(u) = \{ \ell \}$. Par l'absurde, supposons que $u$ ne converge pas vers $\ell$, c'est-à-dire : 
	    \[
	    \exists \varepsilon \in \R ^* _+ \ : \ \forall N \in \N, \ \exists n \in \N \ : \ n\geq N \text{ et } |u_n - \ell | > \varepsilon.
	    \]
	    Fixons un tel $\varepsilon$. \\
	    %\textbf{Etape 1} : \textit{Construction d'une sous-suite de $u$ dont les termes sont $\varepsilon$-éloignés de $\ell$.} \\
	    Posons $\varphi (0) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon \} }$, ce qui a du sens car c'est une partie non-vide de $\N$. Posons ensuite $\varphi (1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(0) < k \} } $, ce qui a du sens pour les mêmes raisons. On construit en itérant ce procédé $\varphi (n)$ tel que : 
	    \[
	    \forall n \in \N, \ \varphi(n+1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(n) < k \} }.
	    \]
	    De cette manière, nous venons de construire une extractrice telle que : 
	    \[
	    \forall n \in \N, \ |u_{\varphi(n)} - \ell| > \varepsilon.
	    \]
	    Par hypothèse $u$ est bornée, donc il existe $M\in \R _+$ tel que : 
	    \[
	    \forall n \in \N, \ |u_n| \leq M,
	    \]
	    donc pour tout $n$ dans $\N$, $|u_{\varphi(n)}| \leq M$, donc $(u_{\varphi(n)})_{n\in \N}$ est bornée. \\
	    Par le théorème de Bolzano-Weierstrass, il existe $\psi$ une extractrice et $\ell ' \in \R$, avec $\varphi \circ \psi$ qui est aussi une extractrice par composition d'applications strictement croissantes, donc$(u_{\varphi \circ \psi (n)})_{n\in \N}$ est une sous-suite de $u$ et $\ell ' \in L(u) = \{ \ell \}$.\\
	    Par ailleurs, pour tout $n$ dans $\N$ :
	    \[
	    \underset{\xrightarrow[n\to +\infty]{}|\ell' -\ell|}{\underbrace{|u_{\varphi \circ \psi (n)} - \ell|}} > \varepsilon,
	    \]
	    donc en passant à la limite dans l'inégalité on a pour tout $n$ dans $\N$, $|\ell ' - \ell | \geq \varepsilon > 0$, ce qui n'est pas possible car $\ell$ est la seule valeur d'adhérence possible et ici la différence n'est pas nulle.
	\end{question_kholle}

\pagebreak\section{Semaine 12}

	\begin{question_kholle}
		[Soient $a \in \K$ et $v \in \K^\N$ où \K peut être \C ou \R.
		L'ensemble des solutions de l'équation $\forall n \in \mathbb{N}, u_{n+1} = au_{n} + v_{n}$
		est la droite affine :
		\begin{equation}
		\left\{ w + \lambda \left( a^n \right) _{n \in \mathbb{N}} \mid \lambda \in \mathbb{K} \right\}
		\end{equation}
		]
		{Résolution d'une relation de récurrence linéaire d'ordre 1 à coefficients constants et avec second membre}

		Posons $w$ la suite définie par $$
		\left\{ \begin{array}{ll}
		 w_{0} = 1 \\
		\forall n \in \mathbb{N}, w_{n+1} = aw_{n} + v_{n}
		\end{array}\right.
		$$
		$w$ est "évidemment solution de particulière de l'équation"
		
		Maintenant que nous disposons d'une solution particulière, et ayant observé que l'équation est linéaire, mettons en œuvre l'artillerie classique pour exprimer l'ensemble des solutions par l'habituelle technique.
		
		
		\begin{align*}
			\forall n \in \mathbb{N}, u_{n+1} = au_{n} + v_{n} & \iff \forall n \in \mathbb{N}, u_{n+1} - au_{n} = v_{n} \\
			& \iff \forall n \in \mathbb{N}, u_{n+1} - au_{n} = w_{n+1} - aw_{n} \\
			& \iff \forall n \in \mathbb{N}, (u-w)_{n+1} = a(u-w)_{n} \\
			& \iff u-w \in \text{Vect}\left\{ \left( a^n \right) _{n \in \mathbb{N}} \right\}  \\
			& \iff \exists \lambda \in \mathbb{K} : u-w = \lambda \left( a^{n} \right) _{n \in \mathbb{N}} \\
			& \iff \exists \lambda \in \mathbb{K} : \forall n \in \mathbb{N}, u_{n} = w_{n} + \lambda a^n \\
			& \iff  u \in \left\{ \left( w_{n} + \lambda a^n \right) _{n \in \mathbb{N}} \mid \lambda \in \mathbb{K} \right\}
		\end{align*}

	\end{question_kholle}

	\begin{question_kholle}
		[Soient $(a,b) \in \C^2$.
		L'ensemble des solutions $S_H$ de l'équation d'inconnue $u \in \C^\N$
		\begin{equation}
			\forall n \in \N, u_{n+2} = a u_{n+1} + b u_n
		\end{equation}
		est le plan vectoriel $\text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$ où $r_1$ et $r_2$ sont les racines de l'équation caractéristique ($r^2 = ar + b$) quand $\Delta \neq 0$.]
		{Résolution d'une relation de récurrence linéaire homogène d'ordre 2 à coefficients constants dans \C lorsque l'équation caractéristique possède un discriminant non nul}

		Soient $(a,b) \in \C^2$ fq.

		\underline{Lemme} Soit $r \in \C$. $\left(r^n\right)_{n\in\N}$ est solution de l'équation de récurrence si et seulement si $r^2 = ar + b$.
		\begin{equation*}
			\begin{aligned}
				(r^n)_{n\in\N} \text{ est solution }
				\iff \forall n \in \N, r^{n+2} &= a r^{n+1} + b r^n \\
				\iff \forall n \in \N, r^{n} \left( r^2 - a r - b \right) &= 0 \\
				\underbrace{\iff}_{\text{ En particularisant pour } n \leftarrow 0} r^2 - a r - b &= 0 \\
				\iff r^2 &= a r + b
			\end{aligned}
		\end{equation*}

		Considérons le cas où l'équation $r^2 = ar + b$ admet deux racines distinctes ($\Delta \neq 0$) $r_1$ et $r_2$.
		D'après le lemme, $\left(r_1^n\right)_{n\in\N}$ et $\left(r_2^n\right)_{n\in\N}$ sont solutions. Par linéarité de l'équation, toute combinaison linéaire est solution de l'équation homogène.
		Donc $\text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \} \subset S_H$.

		Réciproquement, soit $u \in \S_H$ fq.
		Étudions le système à deux inconnues $(\lambda, \mu) \in \C^2$ :
		\begin{equation*}
			\left\{ \begin{matrix}
				\lambda r_1^0 + \mu r_2^0 = u_0 \\
				\lambda r_1^1 + \mu r_2^1 = u_1
			\end{matrix} \right.
			\iff \left\{ \begin{matrix}
				\lambda + \mu = u_0 \\
				\lambda r_1 + \mu r_2 = u_1
			\end{matrix} \right.
		\end{equation*}
		$\begin{vmatrix}
			1 & 1 \\
			r_1 & r_2
		\end{vmatrix}
		= r_2 - r_1 \neq 0$
		Donc d'après les formules de Cramer, ce système admet une unique solution.

		Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ par :
		\begin{equation*}
			u_n = \lambda r_1^n + \mu r_2^n \text{ et } u_{n+1} = \lambda r_1^{n+1} + \mu r_2^{n+1}
		\end{equation*}
		\begin{itemize}
			\item $\mathcal{P}(0)$ est vrai par construction de $\lambda$ et $\mu$.
			\item Soit $n \in \N$ fq tq $\mathcal{P}(n)$ vrai.
			D'après $\mathcal{P}(n)$, $u_{n+1} = \lambda r_1^{n+1} + \mu r_2^{n+1}$.
			\begin{equation*}
				\begin{aligned}
					u_{n+2} &= a u_{n+1} + b u_n \\
					&= a \left( \lambda r_1^{n+1} + \mu r_2^{n+1} \right) + b \left( \lambda r_1^n + \mu r_2^n  \right) \quad \text{ d'après } \mathcal{P}(n) \\
					&= \lambda r_1^n \left(ar_1 + b\right) + \mu r_2^n \left(ar_2 + b\right) \\
					&= \lambda r_1^{n+2} + \mu r_2^{n+2} \quad \text{ car } r_1 \text{ et } r_2 \text{ sont racine de } r^2 = ar + b
				\end{aligned}
			\end{equation*}
		\end{itemize}
		Ainsi $S_H \subset \text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$.

		Par double inclusion, $S_H = \text{Vect}\{ \left(r_1^n\right)_{n\in\N}, \left(r_2^n\right)_{n\in\N} \}$.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $u$ une suite bornée. $u$ converge si et seulement si il existe $\ell \in \mathbb{K}$ tel que $L(u)$ est le singleton $\ell$ ]
		{Caractérisation de la convergence par l'unicité d'une valeur d'adhérence pour une suite bornée.}

		Traitons le cas réel, celui sur \C est à adapter sans peine.\\
		Supposons que $u$ converge et posons $\lim u =\ell \in \R  $. Toutes les sous-suites de $u$ convergent vers $\ell$ donc $L(u)=\{\ell \}$. \\
		Supposons maintenant qu'il existe un unique $\ell \in \R$ tel que $L(u) = \{ \ell \}$. Par l'absurde, supposons que $u$ ne converge pas vers $\ell$, c'est-à-dire :
		\[
			\exists \varepsilon \in \R ^* _+ \ : \ \forall N \in \N, \ \exists n \in \N \ : \ n\geq N \text{ et } |u_n - \ell | > \varepsilon.
		\]
		Fixons un tel $\varepsilon$. \\
		%\textbf{Etape 1} : \textit{Construction d'une sous-suite de $u$ dont les termes sont $\varepsilon$-éloignés de $\ell$.} \\
		Posons $\varphi (0) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon \} }$, ce qui a du sens car c'est une partie non-vide de $\N$. Posons ensuite $\varphi (1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(0) < k \} } $, ce qui a du sens pour les mêmes raisons. On construit en itérant ce procédé $\varphi (n)$ tel que :
		\[
			\forall n \in \N, \ \varphi(n+1) = \min{ \{ k\in \N \ | \ |u_k - \ell| > \varepsilon, \ \varphi(n) < k \} }.
		\]
		De cette manière, nous venons de construire une extractrice telle que :
		\[
			\forall n \in \N, \ |u_{\varphi(n)} - \ell| > \varepsilon.
		\]
		Par hypothèse $u$ est bornée, donc il existe $M\in \R _+$ tel que :
		\[
			\forall n \in \N, \ |u_n| \leq M,
		\]
		donc pour tout $n$ dans $\N$, $|u_{\varphi(n)}| \leq M$, donc $(u_{\varphi(n)})_{n\in \N}$ est bornée. \\
		Par le théorème de Bolzano-Weierstrass, il existe $\psi$ une extractrice et $\ell ' \in \R$, avec $\varphi \circ \psi$ qui est aussi une extractrice par composition d'applications strictement croissantes, donc$(u_{\varphi \circ \psi (n)})_{n\in \N}$ est une sous-suite de $u$ et $\ell ' \in L(u) = \{ \ell \}$.\\
		Par ailleurs, pour tout $n$ dans $\N$ :
		\[
			\underset{\xrightarrow[n\to +\infty]{}|\ell' -\ell|}{\underbrace{|u_{\varphi \circ \psi (n)} - \ell|}} > \varepsilon,
		\]
		donc en passant à la limite dans l'inégalité on a pour tout $n$ dans $\N$, $|\ell ' - \ell | \geq \varepsilon > 0$, ce qui n'est pas possible car $\ell$ est la seule valeur d'adhérence possible et ici la différence n'est pas nulle.
	\end{question_kholle}

	\begin{question_kholle}
		[Soient $f : \mathcal{D} \rightarrow \R$ et $I \subset \mathcal{D}_f$ une intervalle $f$-stable. \\
		Soit $\left(u_n\right)_{n\in\N} \in \R^\N$ la suite récurrente associée à la fonction $f$ c'est-à-dire $\forall n \in \N, u_{n+1} = f(u_n)$.
		\begin{itemize}
			\item Si $f$ est croissante sur $I$.
			\subitem Si $u_1 \geqslant u_0$ alors $u$ est croissante.
			\subitem Si $u_1 \leqslant u_0$ alors $u$ est décroissante.
			\item Si $f$ est décroissante sur $I$.
			\subitem Les sous-suites $\left(u_{2n}\right))_{n\in\N}$ et $\left(u_{2n+1}\right))_{n\in\N}$ sont monotone et ont une monotonie opposée (utiliser les premiers termes pour trouver leur monotonie respectives).
		\end{itemize}]
		{Monotonie de $u$ et des sous-suites des termes pairs et impairs de la suite$u_{n+1} = f(u_n)$ selon la monotonie de $f$}

		Soient de tels $f, I$ et $u$.
		\begin{itemize}
			\item Supposons que $f$ est croissante sur $I$. \\
			Supposons $u_1 \geqslant u_0$. Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ par
			\begin{equation*}
				\mathcal{P}(n) : " u_{n+1} \geqslant u_n "
			\end{equation*}
			\subitem Par hypothèse, $u_1 \geqslant u_0$ donc $\mathcal{P}(0)$ est vrai.
			\subitem Soit $n \in \N$ fq tq $\mathcal{P}(n)$ vrai. \\
			\begin{equation*}
				u_{n+1} \geqslant u_n
				\underbrace{\implies}_{f \text{ est croissante sur } I} f(u_{n+1}) \geqslant f(u_n)
				\implies u_{n+2} \geqslant u_{n+1}
			\end{equation*}
			Donc $\mathcal{P}(n+1)$ est vrai. \\
			Si $u_1 \leqslant u_0$, il suffit de changer $\geqslant$ par $\leqslant$ dans la récurrence ci-dessus.
			\item Supposons que $f$ est décroissante sur $I$. \\
			Donc $\forall n \in \N, u_{2(n+1)} = f \circ f(u_{2n}) \text{ et } u_{2(n+1)+1} = f \circ f(u_{2n+1})$. Or $f \circ f$ est croissante, donc $\left(u_{2n}\right)_{n\in\N}$ et $\left(u_{2n+1}\right)_{n\in\N}$ sont monotones. \\
			Supposons que $\left(u_{2n}\right)_{n\in\N}$ est croissante.
			Soit $n \in \N$ fq. Alors
			\begin{equation*}
				u_{2n} \leqslant u_{2(n+1)}
				\underbrace{\implies}_{f \text{ est décroissante sur } I} f(u_{2n}) \geqslant f(u_{2(n+1)})
				\implies u_{2n+1} \geqslant u_{2(n+1)+1}
			\end{equation*}
			Donc $\left(u_{2n+1}\right)_{n\in\N}$ est décroissante. \\
			De même, si $\left(u_{2n}\right)_{n\in\N}$ est décroissante alors $\left(u_{2n+1}\right)_{n\in\N}$ est croissante.
		\end{itemize}
	\end{question_kholle}

	\begin{question_kholle}
	    [Montrons que : $ \overset{\circ}{\Q} = \emptyset $]
	    {L'intérieur de l'ensemble des rationnels est vide.}
	    Par l'absurde, supposons que $\Q$ possède au moins un point intérieur. \\ Fixons $r_0 \in \overset{\circ}{\Q}$. Par définition d'un point intérieur, il existe $\varepsilon \in \R _+ ^* $ : $]r_0 - \varepsilon , \ r_0 + \varepsilon[ \subset \Q$. Or, par densité des irrationnels dans $\R$, il existe $\alpha \in \R \backslash \Q$ : $r_0 - \varepsilon < \alpha < r_0 + \varepsilon$. On en déduit que $\alpha \in ]r_0 - \varepsilon , \ r_0 + \varepsilon[$, or $]r_0 - \varepsilon , \ r_0 + \varepsilon[ \subset \Q$ donc $\alpha \in \Q$ ce qui contredit le choix de $\alpha \in \R \backslash \Q$. Ainsi, $\overset{\circ}{\Q} = \emptyset$
	\end{question_kholle}

	\begin{question_kholle}
	    [Soient $f,g \ : \ \mathcal{D} \to \R$, $\ell \in \R$ et $a \in \overline{\mathcal{D}}$ tels que $|f(x) - \ell| \leq g(x)$ au voisinage de $a$ et $g$ tend vers $0$ en $a$. Alors $f$ tend vers $\ell$ en $a$.]
	    {Théorème sans nom version continue au voisinage de $a$}

	    On traite le cas $a\in \R$. Par définition de $|f(x) - \ell| \leq g(x)$ au voisinage de $a$,
	    \[
	    	\exists \eta \in \R _+ ^* \ : \ \forall x \in \mathcal{D}, \ |x-a| \leq \eta \ \implies \ |f(x) - \ell| \leq g(x).
	    \]
	    Fixons un tel $\eta$. \\
	    Soit $\omega \in \R_+ ^*$. Appliquons la définition de $\lim_{x \to a} g(x) = 0$ pour $\varepsilon \gets \omega$ :
	    \[
	   		\exists \eta' \in \R_+ ^* \ : \ \forall x \in \mathcal{D}, \ |x-a| \leq \eta' \ \implies \ |g(x)| \leq \omega.
	    \]
	    Fixons un tel $\eta'$. \\
	    Posons $\Omega = \min{ \{\eta,  \eta' \} }$. \\
	    Soit $x\in \mathcal{D}$ tel que $|x-a| \leq \Omega$.
	    \[
		    |f(x) - \ell | \leq g(x) \leq \omega,
	    \]
	    car la définition de $\Omega$ permet de remplir les conditions des deux propriétés.
	\end{question_kholle}
\pagebreak\section{Semaine 13}
		
	\begin{question_kholle}
		[Soient $g$ une fonction définie sur $\mathcal{D}_g \subset \R$ et $f$ une fonction définie sur $\mathcal{D}_f \subset \R$ telle que $f(\mathcal{D}_f) \subset \mathcal{D}_g$.
			Si $\left. \begin{array}{cc}
			g \text{ admet une limite } \ell \in \overline{\R} \text{ en } b \in \overline{\mathcal{D}_g} \\
			f \text{ admet } b \text{ comme limite en } a \in \overline{\mathcal{D}_f}
			\end{array} \right\}$
		alors $g \circ f$ admet $\ell$ comme limite en $a$.]
		{Théorème de composition des limites}
				
		Traitons le cas où $\ell \in \R$, $a \in \R$ et $b \in \R$. \\
		Soit $\varepsilon \in \R_+^*$ fq. \\
		Appliquons la définition de $g(y) \arrowlim{y}{b} \ell$ pour cet $\varepsilon$ :
		\begin{equation*}
			\exists \eta_g \in \R_+^* : \forall y \in \mathcal{D}_g, | y - b | \leqslant \eta_g \implies | g(y) - \ell | \leqslant \varepsilon
		\end{equation*}
		Appliquons la définition de $f(x) \arrowlim{x}{a} b$ pour cet $\eta_g$ :
		\begin{equation*}
			\exists \eta_f \in \R_+^* : \forall x \in \mathcal{D}_f, | x - a | \leqslant \eta_f \implies | f(x) - b | \leqslant \eta_g
		\end{equation*}
		Posons $\eta = \eta_f$.
				
		Soit $x \in \mathcal{D}_{g \circ f}$ fq tq $ | x - a | \leqslant \eta $. Or $f(\mathcal{D}_f) \subset \mathcal{D}_g$ donc $\mathcal{D}_{g \circ f} = \mathcal{D}_f$. \\
		Ainsi, $x \in \mathcal{D}_f$ et $ | x - a | \leqslant \eta_f $ d'où $ | f(x) - b | \leqslant \eta_g $ d'où $ | g(f(x)) - \ell | \leqslant \varepsilon $. Donc
		\begin{equation*}
			g \circ f \arrowlim{x}{a} \ell
		\end{equation*}
	\end{question_kholle}
		
	\begin{question_kholle}
		[{ Soit une fonction continue $f : [a;b] \rightarrow \R$ avec $(a,b) \in \R^2$ et $a < b$. \\
		Si $f(a)f(b) \leqslant 0$ alors $\exists c \in [a;b] : f(c) = 0$. \\
		On rencontre aussi : \textit{Si $\mathit{f(a)f(b) < 0}$ alors $\mathit{\exists c \in ]a;b[ : f(c) = 0}$.} }]
		{Théorème des valeurs intermédiaires}
				
		La démonstration repose sur la technique de la dichotomie.
				
		\begin{figure}[!h]
	%					\centering
			\tikzmath{ \labTVI = 12; } % la longueur du segment [a;b] dans la démonstration du TVI
			\begin{tikzpicture}
				\draw (0,0) node[anchor=north] {a} -- (\labTVI,0) node[anchor=north] {b};
				\foreach \x in {0,...,\labTVI} {
					\draw (\x,0.1) -- (\x,-0.1);
				};
				\draw[red] (0,0) to[out angle=80, in angle=240, curve through={(\labTVI/5,2) (\labTVI/3,-1) (\labTVI*3/5,0.5)}] (\labTVI,0);
						
				\draw[green] (\labTVI/2,0.1) -- (\labTVI/2,-0.1);
				\draw (\labTVI/2,0) node[green, anchor=north] {$b_1$};
				\draw[green] (\labTVI/4,0.1) -- (\labTVI/4,-0.1);
				\draw (\labTVI/4,0) node[green, anchor=north] {$a_2$};
				\draw[green] (\labTVI*3/8,0.1) -- (\labTVI*3/8,-0.1);
				\draw (\labTVI*3/8,0) node[green, anchor=north] {$b_3$};
				\draw[green] (\labTVI*5/16,0.1) -- (\labTVI*5/16,-0.1);
				\draw (\labTVI*5/16,0) node[green, anchor=north] {$b_4$};
			\end{tikzpicture}
		\end{figure}
			
		\noindent Soient $a,b,f$ de tels objets. Procédons à la construction des suites $(a_n)_{n\in\N}, (b_n)_{n\in\N}, (c_n)_{n\in\N}$.
		
		Posons $a_0 = a$, $b_0 = b$ et $c_0 = \frac{a+b}{2}$ (le milieu du segment $[a;b]$). Nous avons, par hypothèse $f(a_0)f(b_0) \leqslant 0$.
		
		Soit $n \in \N$ fq. Supposons les trois suites construites au rang $n$ telles que $f(a_n)f(b_n) \leqslant 0$ et $c_n = \frac{a_n+b_n}{2}$ (milieu de $[a_n;b_n]$).
		\begin{itemize}
			\item Si $f(a_n)f(b_n) \leqslant 0$, posons $\left| \begin{array}{lcl}
	a_{n+1} &=& a_n \\
	b_{n+1} &=& c_n \\
	c_{n+1} &=& \frac{a_{n+1}+b_{n+1}}{2}
			\end{array} \right.$
			\item Sinon $f(a_n)f(b_n) > 0$. Or $f(a_n)f(b_n) \leqslant 0$, donc $f(a_n)^2 f(b_n) f(c_n) \leqslant 0$. Donc $f(b_n)f(c_n) \leqslant 0$. Posons $\left| \begin{array}{lcl}
	a_{n+1} &=& c_n \\
	b_{n+1} &=& b_n \\
	c_{n+1} &=& \frac{a_{n+1}+b_{n+1}}{2}
			\end{array} \right.$
		\end{itemize}
		Ainsi, nous avons bien construits $a_{n+1}, b_{n+1}, c_{n+1}$ telles que $f(a_{n+1})f(b_{n+1}) \leqslant 0$ et ${ c_{n+1} = \frac{a_{n+1}+b_{n+1}}{2} }$ (milieu de $[a_{n+1};b_{n+1}]$).
				
		Par récurrence immédiate, $(a_n)_{n\in\N}$ est croissante, $(b_n)_{n\in\N}$ est décroissante et ${ \forall n \in \N, b_n - a_n = \frac{b-a}{2^n} }$ d'où $b_n - a_n \arrowlim{n}{+\infty} 0$.
		Donc les suites $a$ et $b$ sont adjacentes.
		D'après le théorème des suites adjacentes, elles convergent vers la même limite. Notons la $c$.
		
		D'après le bonus de ce même théorème, $\forall n \in \N, a_n \leqslant c \leqslant b_n$ donc pour $n = 0$, $a \leqslant c \leqslant b$. Ainsi,
		\begin{equation*}
			c \in [a;b]
		\end{equation*}
				
		Par ailleurs, $\forall n \in \N, f(a_n)f(b_n) \leqslant 0$. Par continuité de $f$ sur $[a;b]$ donc en $c$, $f(a_n) \arrowlim{n}{+\infty} f(c)$ et $f(b_n) \arrowlim{n}{+\infty} f(c)$. Par passage à limite dans l'inégalité,
		\begin{equation*}
			f(c) \times f(c) \leqslant 0
		\end{equation*}
		Or $f(c)^2 \geqslant 0$, d'où $f(c)^2 = 0$. Ainsi,
		\begin{equation*}
			f(c) = 0
		\end{equation*}
		Donc $c$ est un point fixe.
				
	\end{question_kholle}
	
	\begin{question_kholle}
		[{L'image d'un segment par une fonction continue sur ce segment est un segment : soient $(a, b) \in \mathbb{R}^2$ tels que $a < b$ et $f: [a, b] \to \mathbb{R}$. Si $f \in \mathcal{C}^0([a, b], \mathbb{R})$ alors $\exists (x_{1}, x_{2}) \in \mathbb{R}^2 : f([a, b]) = [f(x_{1}), f(x_{2})]$}]
		{Théorème de Weierstraß}
        \begin{itemize}

		\item \emph{Étape 1} Montrons que $f([a, b])$ est majoré.

		Par l'absurde, supposons que $f([a, b])$ n'est pas majoré

		Alors \begin{equation}\label{eq:1}
  \forall A \in \mathbb{R}, \exists x \in [a, b] : f(x) > A
		\end{equation}
		
		Soit $n \in \mathbb{N}$ fq.
		Appliquons (\ref{eq:1}) pour $A \leftarrow n$:
		$\exists x \in [a, b] : f(x) > n$, et fixons un tel $x$ que l'on note $x_{n}$
		Nous venons de créer la suite $(x_{n})_{n \in \mathbb{N}} \in [a, b]^{\mathbb{N}}$ qui vérifie:
		
		
		$$
		\left. 
		\begin{array}{ll}
			\forall n \in \mathbb{N}, f(x_{n}) \geqslant n \\
			\lim_{ n \to \infty } n =  +\infty         
		\end{array}
		\right\} \underbrace{ \implies }_{ \text{théorème de divergence par minoration} } f(x_{n}) \xrightarrow[n \to +\infty]{} + \infty
		$$
		
		
		$(x_{n})_{n \in \mathbb{N}}$ est bornée (à valeurs dans $[a, b]$) donc, selon le théorème de Bolzanno-Weierstraß:
		$$
		\exists \ell \in \mathbb{R} : \exists \varphi : \mathbb{N} \to \mathbb{N} : \text{strict. croissante tel que } (x_{\varphi(n)})_{n \in \mathbb{N}} \text{ tend vers } \ell
		$$
		Donc, en passant à la limite : $\forall n \in \mathbb{N}, a \leqslant x_{\varphi(n)} \leqslant b \implies a \leqslant \ell \leqslant b \implies \ell \in [a, b]$
		
		Par continuité de $f$ sur $[a, b]$, donc en $\ell$, $(f(x_{\varphi(n)}))_{n \in \mathbb{N}}$ converge vers $f(\ell)$.
		
		Or $$
		\left\{ \begin{array}{ll}
		(f(x_{\varphi(n)}))_{n \in \mathbb{N}} \text{ est une sous suite de } (f(x_{n}))_{n \in \mathbb{N}} \\
		f(x_{n}) \xrightarrow[n \to + \infty]{} + \infty
		\end{array}\right.$$
		
		donc $(f(x_{\varphi(n)}))_{n \in \mathbb{N}}$, tend vers $+ \infty$, ce qui est absurde, donc $f$ est majorée.
		
		On fait de même pour la minoration.
		
		\item  \emph{Étape 2:} Montrons que $f([a, b])$ admet un pge et un ppe.
		
		Montrons donc que $f([a, b])$ admet une borne sup, qui, puisque c'est une valeur atteinte, deviendra un max.
		
		$$
		f([a, b]) \text{ est } \left\{ \begin{array}{ll}
		\text{ une partie de } \mathbb{R} \\
		\text{ non vide car contient } f(a) \\
		\text{majorée d'après l'étape 1}
		\end{array}\right.
		$$
		
		$f([a, b])$ admet donc une borne supérieure $\sigma$.
		
		Appliquons la caractérisation séquentielle de la borne supérieure:
		$$
		\exists (y_{n})_{n \in \mathbb{N}}, \in f([a, b])^{\mathbb{N}} : (y_{n}) \text{ converge vers } \sigma
		$$
		$$
		\forall n \in \mathbb{N}, y_{n} \in f([a, b]) \implies \exists x_{n} \in [a, b] : f(x_{n} ) = y_{n}
		$$
		Fixons un tel $x_{n}$ pour tout $y_{n}$.
		On a donc construit $(x_{n})_{n \in \mathbb{N}} \in [a, b]^{\mathbb{N}} : f(x_{n}) \xrightarrow[n \to +\infty]{} \sigma$
		
		De plus, $(x_{n})$ est bornée (à valeurs dans $[a, b]$) donc, selon le théorème de Bolzanno-Weierstraß:
		$$
		\exists \ell \in \mathbb{R} : \exists \varphi : \mathbb{N} \to \mathbb{N} : \text{strict. croissante tel que } (x_{\varphi(n)})_{n \in \mathbb{N}} \text{ tend vers } \ell
		$$
		Donc, en passant à la limite : $\forall n \in \mathbb{N}, a \leqslant x_{\varphi(n)} \leqslant b \implies a \leqslant \ell \leqslant b \implies \ell \in [a, b]$


		Par continuité de $f$ sur $[a, b]$, donc en $\ell$, $(f(x_{\varphi(n)}))_{n \in \mathbb{N}}$ converge vers $f(\ell)$.
		
		Or,
		$$
		\left\{ \begin{array}{ll}
		(f(x_{\varphi(n)}))_{n \in \mathbb{N}} \text{ est une sous suite de } (f(x_{n}))_{n \in \mathbb{N}} \\
		f(x_{n}) \xrightarrow[n \to + \infty]{} \sigma
		\end{array}\right.$$
		
		Par unicité de la limite, $\sigma = f(\ell)$.
		
		On montre de même qu'il existe $\ell' \in [a, b]: f(\ell') = \inf f([a, b])$
		
		Ainsi, $f(\ell) = \max f([a, b])$ et $f(\ell') = \min f([a, b])$



		\item \emph{Étape 3:}
		Montrons que $f([a, b]) = [f(\ell'), f(\ell)]$.
		
		Par la construction précédente, $\forall y \in f([a, b]), y \in [f(\ell'), f(\ell)]$.
		
		Ainsi, $f([a, b]) \subset [f(\ell'), f(\ell)]$.
		
		Réciproquement, l'image par la fonction continue $f$ du segment $[a, b]$ qui est un intervalle est un intervalle:
		
		$$
		\left. 
		\begin{array}{ll}
			f([a,b]) \text{ est un intevalle} \\
			f(\ell) \in f([a, b])             \\
			f(\ell') \in f([a, b])            
		\end{array}
		\right\} 
		\implies
		[f(\ell'), f(\ell)] \subset f([a,b])
		$$
		
		D'où $[f(\ell'), f(\ell)] = f([a,b])$

        \end{itemize}
	\end{question_kholle}
\pagebreak\section{Semaine 14}

	\begin{question_kholle}
	    [Soit $f \left| \begin{matrix}
	    	\R_+^* \rightarrow \R \\
	    	x \mapsto \frac{\ln x}{x}
	    \end{matrix} \right. $. \\
	    Exprimer $f^{(n)}$ pour tout $n \in \N$.]
	    {Expression de dérivées successives}
	    Soit $x\in \mathcal{D}_f$. \\
	    Considérons le prédicat $P(\cdot)$ définit pour $n\in \N$ par :
	    $$ P(n) \ : \ \text{\textquotedblleft} \ f^{(n)}(x) = \frac{(-1)^n n!}{x^{n+1}}\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right] \ " $$
	    Initialisation : \\
	    Pour $n = 0$, $$f^{(0)}(x) = f(x) = \frac{\ln (x)}{x} = \frac{(-1)^0 0!}{x^{0+1}}\left[ \ln (x) - \sum_{k=1}^0 \frac{1}{k} \right],$$ donc $P(0)$ est vrai. \\
	    Hérédité :
	    \\
	    Soit $n\in \N$ tel que $P(n)$. On a,
	    $$f^{(n+1)}(x) = (f^{(n)}(x))' = \left( \frac{(-1)^n n!}{x^{n+1}}\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right] \right)'$$
	    par véracité de $P(n)$. Ainsi,
		$$\begin{array}{rcl}
		f^{(n+1)}(x) & = & \frac{(-1)^nn!x^n - (-1)^n(n+1)!x^n\left[ \ln (x) - \sum_{k=1}^n \frac{1}{k} \right]}{x^{2(n+1)}} \\ [1ex]
		 & = & \frac{(-1)^{n+1}(n+1)!\ln (x) - (-1)^{n+1}(n+1)!\sum_{k=1}^{n+1} \frac{1}{k} }{x^{n+2}} \\ [1ex]
		 & = & \frac{(-1)^{n+1} (n+1)!}{x^{n+2}}\left[ \ln (x) - \sum_{k=1}^{n+1} \frac{1}{k} \right]
		\end{array}$$
	    c'est l'expression recherchée, donc $P(n+1)$ est vrai. \\
	    Par théorème de récurrence sur $\N$, $P(n)$ est vraie pour tout $n\in \N$.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $f : I \rightarrow f(I) \subset \R$ continue, strictement monotone sur $I$ et dérivable en $a \in I$.
		Si $f'(a) \neq 0$ alors $f$ est bijective, $f^{-1}$ est dérivable en $f(a)$ et $f^{-1}(f(a)) = \frac{1}{f'(a)}$.]
		{Dérivé d'une bijection réciproque}
		Soient de tels objets. \\
		Rappelons le lemme inattendu. Soit $g : J \rightarrow \R$ monotone (où $J$ est un intervalle). Nous avons l'équivalence suivante :
		\begin{equation*}
			f(J) \text{ est un intervalle } \iff f \text{ est continue sur } J
		\end{equation*}
		Par définition, $f$ est surjective. Comme elle est strictement monotone, $f$ est injective. Ainsi $f$ est bijective. \\
		D'après le lemme inattendu, $f(I)$ est un intervalle. Nous avons $f^{-1} : f(I) \rightarrow I$ avec $f(I)$ et $I$ des intervalles donc $f^{-1}$ est continue sur $f(I)$. \\
		Calculons la limite du taux d'accroissement de $f^{-1}$ en $f(a)$ :
		\begin{equation*}
			\forall x \in f(I), \tau_{f^{-1},f(a)} = \frac{f^{-1}(x) - f^{-1}(f(a))}{x - f(a)}
		\end{equation*}
		Posons $u = f^{-1}(x)$. D'où :
		\begin{equation*}
			\tau_{f^{-1},f(a)} = \frac{u - a}{f(u) - f(a)}
		\end{equation*}
		De plus, par continuité de $f^{-1}$, $u \arrowlim{x}{f(a)} f^{-1}(f(a)) = a$.
		Par dérivabilité en a et par continuité de $x \mapsto x^{-1}$ en $f(a) \neq 0$, $\frac{u - a}{f(u) - f(a)} \arrowlim{u}{a} \frac{1}{f('(a)}$. \\
		Ainsi, $f^{-1}$ est dérivable en $f(a)$ et $f^{-1}(f(a)) = \frac{1}{f'(a)}$.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $f : I \rightarrow \R$. Si $f$ admet un extremum local en $a \in \overset{\circ}{I}$ et si $f$ est dérivable en $a$ alors $f'(a) = 0$
		{\begin{figure}[!h]
				\centering
				\tikzmath{ integer \m; real \fm; \m = 3; \fm = 0.8; }
				\begin{tikzpicture}
					\draw[-stealth] (0,0) -- (6,0) node[anchor=west] {$x$};
					\draw[-stealth] (0,0) -- (0,3) node[anchor=south] {$y$};
					
					\draw[red] plot [smooth] coordinates {(0,2.5) (\m,\fm) (6,1.7)};
					\draw[blue, dashed] (\m,\fm) -- (\m,0) node[anchor=north] {minimum local};
					
					\draw[stealth-stealth, teal] (\m-1,\fm) -- (\m+1,\fm);
					\draw (\m,\fm) node[anchor=north east, teal] {$m$} node[anchor=south, teal] {$f'(m)=0$};
				\end{tikzpicture}
		\end{figure}}]
		{Dérivée d'un extremum local intérieur au domaine de définition}
		Soient de tels objets. \\
		$a \in \overset{\circ}{I} \implies \exists \eta_1 \in \R_+^* : [a-\eta_1;a+\eta_1] \subset I$ Fixons un tel $\eta_1$. \\
		Calculons le taux d'accroissement en $a$.
		\begin{equation*}
			\forall x \in [a-\eta_1;a+\eta_1], \tau_{f,a}(x) = \frac{f(x) - f(a)}{x - a}`
		\end{equation*}
		Or $f$ est dérivable en $a$ donc $\tau_{f,a}(x)$ admet une limite lorsque $x \rightarrow a$.
		Traitons le cas où $a$ est maximum local. Par définition :
		\begin{equation*}
			\exists \eta_2 \in \R_+^* : \forall x \in [a-\eta_2;a+\eta_2], f(x) \leqslant f(a)
		\end{equation*}
		Fixons un tel $\eta_2$. Soit $x \in [a-\eta_2;a+\eta_2] \setminus \{a\}$ fq. \\
		Alors $f(x) - f(a) \leqslant 0$. \\
		Si $x > a$, $x - a > 0$. Alors $\frac{f(x) - f(a)}{x - a} \leqslant 0$. Donc $\textlim{x}{a} \tau_{f,a}(x) \leqslant 0$. \\
		Sinon $x < a$, $x - a < 0$. Alors $\frac{f(x) - f(a)}{x - a} \geqslant 0$. Donc $\textlim{x}{a} \tau_{f,a}(x) \geqslant 0$. \\
		Ainsi $0 \leqslant \textlim{x}{a} \tau_{f,a}(x) \leqslant 0$. Donc $f'a) = 0$.
	\end{question_kholle}

	\begin{question_kholle}
	    [Soient $(a,b)\in \R ^2$ tels que $a<b$. Soit $I$ le segment $a,b$.
	    \\
	    Soit $f \ : \ I \ \to \ \R $ continue sur ledit segment et dérivable sur l'ouvert associé.\\
	    {\begin{enumerate}[label=($\roman*$)]
	    	\item Théroème de Rolle : \\
	    	Si $f(a) = f(b)$, alors $\exists \ c \in \overset{\circ}{I}$ tel que $f'(c) =0$
	    	\begin{figure}[!h]
	    		\centering
	    		\tikzmath{ integer \a \b \f \c \fc; \a = 1; \b = 9; \f = 2; \c = \a + 3; \fc = \f + 2; }
	    		\begin{tikzpicture}
	    			\draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
	    			\draw[-stealth] (0,0) -- (0,5) node[anchor=south] {$y$};

	    			\coordinate (A) at (\a,\f);
	    			\coordinate (B) at (\b,\f);
	    			\coordinate (C) at (\c,\fc);

	    			%\draw[red] (A) to[out angle=80, in angle=240, curve through={(C) (6,1.5)}] (B);
	    			\draw [red] plot [smooth] coordinates {(A) (C) (6,1.5) (B)};
	    			\draw[blue, thick] (A) -- (B);
	    			\draw[blue, dashed] (A) -- (\a,0) node[anchor=north] {$a$};
	    			\draw[blue, dashed] (B) -- (\b,0) node[anchor=north] {$b$};
	    			\draw[blue, dashed] (A) -- (0,\f) node[anchor=east] {$f(a)=f(b)$};

	    			\draw[stealth-stealth, teal] (\c-2,\fc) -- (\c+2,\fc);
	    			\draw (C) node[anchor=north, teal] {$c$} node[anchor=south, teal] {$f'(c)=0$};
	    		\end{tikzpicture}
    			\caption{Théorème de Rolle}
	    	\end{figure}

	    	\item Formule des accroissements finis : \\
	    	$\exists \ c \in \overset{\circ}{I} \ : \ f'(c) = \frac{f(b)-f(a)}{b-a}.$
	    	\begin{figure}[!h]
	    		\centering
	    		\tikzmath{integer \a \fa \b \fb \c \fc; \a = 1; \fa = 1; let \b = 9; \fb = 5; \c = \a + 5; \fc = \fa + 1; }
	    		\begin{tikzpicture}
	    			\draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
	    			\draw[-stealth] (0,0) -- (0,6) node[anchor=south] {$y$};

	    			\coordinate (A) at (\a,\fa);
	    			\coordinate (B) at (\b,\fb);
	    			\coordinate (C) at (\c,\fc);

	    			%\draw[red] (A) to[out angle=80, in angle=240, curve through={(C) (6,1.5)}] (B);
	    			\draw [red] plot [smooth] coordinates {(A) (3,4) (C) (7,4.5) (B)};
	    			\draw[blue, thick] (A) -- (B);
	    			\draw[blue, dashed] (A) -- (\a,0) node[anchor=north] {$a$};
	    			\draw[blue, dashed] (A) -- (0,\fa) node[anchor=east] {$f(a)$};
	    			\draw[blue, dashed] (B) -- (\b,0) node[anchor=north] {$b$};
	    			\draw[blue, dashed] (B) -- (0,\fb) node[anchor=east] {$f(b)$};

	    			\draw[stealth-stealth, teal] (\c-2,\fc-1) -- (\c+2,\fc+1);
	    			\draw (C) node[anchor=south, teal] {$c$} node[anchor=north west, teal] {$f'(c)=\frac{f(b)-f(a)}{b-a}$};
	    		\end{tikzpicture}
    			\caption{Formule des accroissements finis}
	    	\end{figure}
	    \end{enumerate}}
	    ]
	    {Théorème de Rolle et formule des accroissements finis}
	    Soient de tels objets. \\
	    Prouvons $(i)$, donc supposons $f(a) = f(b)$. \\
	    $f$ est continue sur $I$ donc par le théorème de Weierstraß, elle est bornée et atteint ses bornes sur ce segment :
	    \\
	    $$\exists \ (x_m, x_M)\in I^2 \ : \ (f(x_m) = \min f(I)) \wedge (f(x_M) = \max f(I))$$
	    donc, si $(x_m, x_M)\in \{a,b\}^2$, alors,
	    $$\forall x \in I, \ f(a)=f(x_m) \leq f(x) \leq f(x_M)=f(a)$$
	    donc $\forall x \in I, f(x) = f(a)$ c'est-à-dire que $f$ est constante et donc tous les points intermédiaires à $I$ sont des $c$ valides.\\
	    Sinon, $(x_m \notin \{a,b\}) \vee (x_M \notin \{a,b\}) $, quitte à prendre l'autre valeur, supposons que $x_M \notin \{a,b\}$, ainsi, $x_M \in \overset{\circ}{I}$ et $f(x_M)$ est un maximum global donc, $f$ étant dérivable sur $\overset{\circ}{I}$ elle est dérivable en $x_M$ donc $f'(x_M)=0$, on pose $c = x_M$, ce qui conclut. \\ \\
	    Prouvons $(ii)$.\\
	    Posons $d \ :\ I \ \to \ \R, \ x \ \mapsto \ f(x) - \left( \frac{f(b)-f(a)}{b-a}(x-a) +f(a) \right) $. $d$ est continue sur $I$ et dérivable sur $\overset{\circ}{I}$ comme combinaison linéaire de telles fonctions. On a $d(a) = 0$ et $d(b) = 0$ donc $d(a) = 0 = d(b)$. On peut alors appliquer le Théorème de Rolle pour $f \gets d, a \gets a $ et $ b \gets b$ : il existe $c\in \overset{\circ}{I}$ tel que $d'(c) = 0$, c'est le résultat.
	\end{question_kholle}

	\begin{question_kholle}
	    [Soit $f\in \mathcal{C}^0(I, \R) \cap \mathcal{D}^1(\overset{\circ}{I}, \R)$ et $x_0 \in I$, posons {$X_- = ]-\infty;x_0]$} la demi-droite fermée en $x_0$ et vers $-\infty$, de même {$X_+ = [x_0;+\infty[$} la demi-droite fermée en $x_0$ et vers $+ \infty$. \\ \\
	    {\begin{enumerate}[label=$(\roman*)$]
	    	\item \begin{itemize}[label=$\star$, leftmargin=0.4cm]
	    		\item Si $\exists \ m \in \R \ : \ \forall x \in \overset{\circ}{I}, \ m\leq f'(x)$, alors, $$\forall x \in I \cap X_+, \ f(x_0) + m(x-x_0) \leq f(x)$$ et $$\forall x \in I \cap X_-, \ f(x) \leq f(x_0) + m(x-x_0)$$
	    		\item Si $\exists \ M \in \R \ : \ \forall x \in \overset{\circ}{I}, \ f'(x) \leq M $, alors, $$\forall x \in I \cap X_+, \ f(x) \leq f(x_0) + M(x-x_0)$$ et $$\forall x \in I \cap X_-, \ f(x_0) + M(x-x_0) \leq f(x) $$
	    		\item Si $\exists \ (m,M) \in \R^2 \ : \ \forall x \in \overset{\circ}{I}, \ m\leq f'(x) \leq M$, alors, $$\forall x \in I \cap X_+, \ f(x_0) + m(x-x_0) \leq f(x) \leq f(x_0) +M(x-x_0)$$ et $$\forall x \in I \cap X_-, \ f(x_0) + M(x-x_0) \leq f(x) \leq f(x_0) +m(x-x_0)$$
	    	\end{itemize}
    		\item Si $\exists M \in \R \ : \ \forall x \in \overset{\circ}{I}, \ |f'(x)|\leq M$, alors, $$\forall (x,y) \in I^2, \ |f(y) -f(x)| \leq M|y-x|$$
	    \end{enumerate}}

		{\begin{figure}[!h]
			\centering
			\tikzmath{ integer \x0 \fx0 \m \M; \x0 = 5;\fx0 = 2.5; \m = 0.2; \M = 0.7; }
			\begin{tikzpicture}
				\draw[-stealth] (0,0) -- (10,0) node[anchor=west] {$x$};
				\draw[-stealth] (0,0) -- (0,5) node[anchor=south] {$y$};

				\coordinate (A) at (1,\fx0-\m*\x0+\m*1);
				\coordinate (B) at (9,\fx0-\m*\x0+\m*9);
				\coordinate (C) at (1,\fx0-\M*\x0+\M*1);
				\coordinate (D) at (9,\fx0-\M*\x0+\M*9);
				\draw[ultra thick, black] (A) -- (B) node[anchor=north west] {$y = f(x_0) + m(x -x_0)$};
				\draw[ultra thick, black] (C) -- (D) node[anchor=south east] {$y = f(x_0) + M(x -x_0)$};
				\filldraw[magenta!30] (A) -- (B) -- (D) -- (C) -- cycle;

				\draw (\x0,\fx0) node[cross out, minimum size=8*\pgflinewidth, inner sep=0pt, outer sep=0pt, draw=blue, rotate=45, thick] {};
				\draw[blue, dashed] (\x0,\fx0) -- (0,\fx0) node[anchor=east] {$f(x_0)$};
				\draw[blue, dashed] (\x0,\fx0) -- (\x0,0) node[anchor=north] {$x_0$};
			\end{tikzpicture}
			\caption{Interprétation géométrique des accroissements finis}
		\end{figure}}]
	    {Inégalité des accroissements finis}
	    $(i)$ Soit $x\in I$ et posons $S$ le segment d'extrémités $x$ et $x_0$. \\
	    $\star$ Si $x\neq x_0$, $f$ est continue sur $S$ et dérivable sur $\overset{\circ}{S}$, la formule des accroissements finis donne alors l'existence d'un $c$ appartenant à $\overset{\circ}{S}$ tel que $$f(x) - f(x_0) = (x-x_0)f'(c)$$ Si $x> x_0, \ x-x_0 > 0$, or $m \leq f'(c) \leq M$ donc $$m(x-x_0) \leq (x-x_0)f'(c) \leq M(x-x_0)$$ si bien que $$m(x-x_0) \leq f(x) - f(x_0) \leq M(x-x_0) $$ d'où $$f(x_0) + m(x-x_0) \leq f(x) \leq f(x_0) + M(x-x_0). $$ Si $x<x_0$, il suffit de retourner l'inégalité lors de la première multiplication et $(i)$ est prouvé.\\ \\
	    $(ii)$ Soit $y \in I.$\\
	    L'hypothèse $\forall x\in \overset{\circ}{I}, \ |f'(x)|\leq M$ équivaut à $\forall x \in \overset{\circ}{I}, \ -M \leq f'(x) \leq M$, donc on peut appliquer $(i)$ pour $x_0 \gets y, \ M \gets M$ et $m\gets -M$ : $$\forall x \in I\cap [y, +\infty [,  \ f(y) -M(x-y) \leq f(x) \leq f(y) + M(x-y)$$
	    Or $x-y >0$ donc $|f(x) -f(y) | \leq M|x-y|$.
	    Et $$\forall x \in I\cap ]-\infty, y ],  \ f(y) +M(x-y) \leq f(x) \leq f(y) - M(x-y)$$
	    Or $x-y < 0$ donc $|f(x) -f(y) | \leq M|x-y|$. \\
	    Par conséquent, $\forall (x,y)\in I^2, \ |f(y) -f(x)| \leq M|y-x|.$
	\end{question_kholle}

	\begin{question_kholle}
	    [Soit $f\in \mathcal{C}^1(I,\R)$, $I$ le segment $a,b$. Alors $f$ est $||f'||_{\infty,I}$-lipschitzienne sur I.]
	    {Caractère lipschitzien d'une fonction $\mathcal{C}^1$ sur un segment}
	    Soient de tels objets. \\
	    $\star$ $f\in \mathcal{C}^1(I, \R)$ donc $f\in \mathcal{C}^0(I, \R)$. \\
	    $\star$ $f\in \mathcal{C}^1(I,\R)$ donc $f\in \mathcal{D}^1(\overset{\circ}{I}, \R)$.\\
	    $\star$ $f\in \mathcal{C}^1(I,\R)$ donc $f'$ est continue sur $I$ donc le réel $||f'||_{\infty,I}$ est bien défini et $$\forall  x \in \overset{\circ}{I}, \ |f'(x)| \leq ||f'||_{\infty,I}.$$ Ces propriétés permettent d'appliquer le corollaire du TAF qui conclut que $f$ est $||f'||_{\infty,I}$-lipschitzienne.
	\end{question_kholle}

	\begin{question_kholle}
	    [Soit $f\in \mathcal{F}(I, \R)$ et $a\in I$.\\
	    \newline
	    \textit{Lemme} : \\
	    Si
	        $\left\{ \begin{array}{cl}
	        f \text{ est dérivable sur } I\backslash \{a\}  \\
	        f \text{ est continue en }a \\
	        f'_{|I\backslash \{a\}} \text{ admet une limite $\ell \in \overline{\R}$ en }a
	        \end{array}
	        \right.$, alors $\lim_{x \to a} \frac{f(x) -f(a)}{x-a} = \ell$\\
	    \newline
	    \textit{Théorème} : \\
	    Si
	        $\left\{ \begin{array}{cl}
	        f \text{ est dérivable sur } I\backslash \{a\}  \\
	        f \text{ est continue en }a \\
	        f'_{|I\backslash \{a\}} \text{ admet une limite \textbf{finie} $\ell \in \R$ en }a
	        \end{array}
	        \right.$, alors $\left\{ \begin{array}{cl}
	        f \text{ est dérivable en } a  \\
	        f'(a) = \ell \ (\textbf{donc } f' \textbf{ est continue en } a)
	        \end{array}
	        \right.$ ]
	    {Théorème du prolongement de la propriété de la dérivabilité}
	    Prouvons le lemme pour $\ell\in \R$, c'est le cas qui nous intéresse. \\
	    Soient de tels objets. Soit $\varepsilon\in \R_+^*$. Appliquons la définition de $\lim_{\substack{x\to a \\ x\neq a}}  f'_{|I\backslash \{a\}}(x) =\ell $ pour $\varepsilon \gets \varepsilon$ :
	    $$\exists \ \eta \in \R_+^* \ : \ \forall x\in I\backslash \{a\}, \ |x-a| \leq \eta \ \implies \ | f'_{|I\backslash \{a\}}(x) - \ell |\leq \varepsilon.$$ Fixons un tel $\eta$.\\
	    Soit $x\in I\backslash \{a\}$ tel que $|x-a|\leq \eta$.\\ La fonction $f$ est continue sur $I$ donc $f$ est continue sur le segment d'extrémités $a$ et $x$ qui est par ailleurs inclus dans $I$ par convexité d'un intervalle.\\ La fonction $f$ est dérivable sur $I$ donc $f$ est dérivable sur l'intervalle ouvert $a$, $x$ qui est aussi inclus dans $\overset{\circ}{I}$ par convexité.\\ L'égalité des accroissements finis s'applique à $f$ sur l'intervalle $a$ et $x$ : $$\exists \ c_x \in ]a,x[ \cup ]x,a[ \ : \ \frac{f(x) -f(a)}{x-a} = f'(c_x)$$
	    Or $|c_x-a| \leq |x-a| \leq \eta $ donc ladite définition de la limite s'applique pour $x\gets c_x$ : $|f'(c_x) - \ell|\leq \varepsilon $ si bien que $$|\frac{f(x)-f(a)}{x-a}- \ell |\leq \varepsilon.$$ D'où le lemme. \\
	    \newline
	    Prouvons alors le théorème.\\
	    Sous ces hypothèses, le lemme s'applique donc $\lim_{x\to a} \frac{f(x) -f(a)}{x-a} = \ell$, or $\ell \in \R$, donc le taux d'accroissement de $f$ en $a$ admet une limite finie en $a$ ce qui prouve la dérivabilité de $f$ en $a$ et $f'(a) = \ell$. Ce qui suffit.
	\end{question_kholle}

	\begin{question_kholle}
		[Posons $\zeta \left| \begin{matrix}
			\R \longrightarrow \R \\
			x \mapsto \left\{ \begin{array}{cc}
				0 &\text{si } x \leqslant 0\\
				\e^{-\frac{1}{x}} &\text{si } x > 0
			\end{array} \right.
		\end{matrix} \right.$.
		Montrons que $\zeta \in \Cont{\infty}{\R}{\R}$.
		{\begin{figure}[!h]
			\centering
			\begin{tikzpicture}
				\begin{axis}[
					axis lines = center,
					xlabel = $x$,
					ylabel = {$f(x)$},
					width=15cm,
					height=5cm,
					ymax=1.5
					]
					\addplot[
						domain=0.01:20,
						samples=200,
						color=red,
						]
						{exp(-1/x)};
					\addplot[
						domain=-10:0,
						samples=100,
						color=red,
						]
						{0};
					\addlegendentry{$\zeta$}

					\addplot[
						domain=0:20,
						samples=100,
						color=blue,
						dashed
						]
						{1};
				\end{axis}
			\end{tikzpicture}
		\end{figure}}]
		{La fonction $\zeta$ (pas celle-là une autre) est de classe \Cont{\infty}{}{} sur \R}
		~ \\

		\begin{itemize}[label=$\star$]
			\item $\zeta_{]-\infty;0[}$ est constante donc $\zeta \in \Cont{\infty}{]-\infty;0[}{}$.
			\item $x \mapsto - \frac{1}{x} \in \mathcal{C}^\infty(]0;+\infty[,]-\infty;0[)$ et $\exp \in \Cont{\infty}{]-\infty;0[}{}$ donc, par stabilité de \Cont{\infty}{}{} par composition, $\zeta \in \Cont{\infty}{]0;+\infty[}{}$.
		\end{itemize}

		Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N$ :
		\begin{equation}
			\mathcal{P} : \text{\textquotedblleft} \ \exists P_n \in \R[x] : \forall x \in \R^*, \ \zeta^{(n)} = \left\{ \begin{array}{lc}
				0 &\text{si } x < 0 \\
				\frac{P_n(x)}{x^{2n}} \e^{-\frac{1}{x}} &\text{si } x > 0
			\end{array} \right. \ \text{\textquotedblright}
		\end{equation}
		\begin{itemize}[label=$\star$]
			\item $\mathcal{P}(0)$ est vrai par définition de $\zeta$ en posant $P_0(x) = 1$
			\item Soit $n \in \N^*$ \fq \ tel que $\mathcal{P}$ est vrai.
			D'une part, $\forall x \in ]-\infty;0[, \zeta^{(n)}(x) = 0$ donc
			\begin{equation*}
				\forall x \in ]-\infty;0[, \zeta^{(n+1)}(x) = 0
			\end{equation*}
			D'autre part, $\forall x \in ]0;+\infty[, \zeta^{(n)}(x) = \frac{P_n(x)}{x^{2n}} \e^{-\frac{1}{x}}$ ce qui est un produit de trois expressions dérivables. D'où :
			\begin{equation*}
				\begin{aligned}
					\forall x \in ]-\infty;0[,
					\zeta^{(n+1)}(x)
					&= \left( P_n'(x) \frac{1}{x^{2n}} + P_n(x) \frac{-2n}{x^{2n+1}} + \frac{P_n(x)}{x^{2n}} \frac{1}{x^2} \right) \e^{-\frac{1}{x}} \\
					&= \frac{x^2 P_n'(x) - 2nxP_n(x) + P_n(x)}{x^{2(n+1)}} \e^{-\frac{1}{x}}
				\end{aligned}
			\end{equation*}
			Si bien qu'en posant $P_{n+1}(x) = x^2 P_n'(x) - 2 n x P_n(x) + P_n(x) \in \R[x]$, on obtient :
			\begin{equation*}
				\forall x \in ]0;+\infty[, \zeta^{(n+1)}(x) = \frac{P_{n+1}(x)}{x^{2(n+1)}} \e^{-\frac{1}{x}}
			\end{equation*}
			Par conséquent, $\mathcal{P}(x)$ est vrai.

			Appliquons maintenant le théorème de prolongement du caractère \Cont{\infty}{}{}.
			\begin{itemize}[label=$\star$]
				\item Nous avons montré que $\zeta \in  \Cont{\infty}{\R \!\setminus\! \{0\}}{}$.
				\item Calculons les limites à gauche et à droite de 0. Soit $k \in \N$ \fq.
				\begin{itemize}[label=$\star\star$]
					\item $\zeta^{(k)}$ est nulle sur $]-\infty;0[$, $\zeta^{(k)} \arrowlim{x}{0^-} 0$.
					\item De plus, $\exists P_n \in \R[x] : \ \forall x \in ]0;+\infty[, \zeta^{(k)}(x) = \frac{P_k(x)}{x^{2k}} \e^{-\frac{1}{x}}$. Posons $u = \frac{1}{x}$, ainsi $\zeta^{(k)}(x) = u^{2k} P_k(\frac{1}{u}) \e^{-\frac{1}{x}}$ et $u \arrowlim{x}{0^+} +\infty$. \\
					Le théorème des croissances comparées donne $u^{2k} P_k(\frac{1}{u}) \e^{-u} \arrowlim{u}{+\infty} 0$ donc $\zeta^{(k)}(x) \arrowlim{x}{0^+} 0$.
				\end{itemize}
			\end{itemize}
			Donc $\zeta \in \Cont{\infty}{\R}{\R}$.
		\end{itemize}
	\end{question_kholle}

\pagebreak\section{Semaine 15}
	
	\begin{question_kholle}
		[Soit $f : I \rightarrow \R$ convexe sur $I$. \\
		Soit $n \in \N^*$. Soient $x \in I^n$, $\lambda \in {[0;1]}^n$ telle que $\displaystyle \sum_{k=1}^{n} \lambda_k = 1$. \\
		\begin{equation}
			\sum_{k=1}^{n} \lambda_k x_k \in I \wedge
			f\left( \sum_{k=1}^{n} \lambda_k x_k \right)
			\leqslant \sum_{k=1}^{n} \lambda_k f\left( x_k \right)
		\end{equation}]
		{Inégalité de Jensen}
		Considérons le prédicat $\mathcal{P}(\cdot)$ défini pour tout $n \in \N^*$ par :
		\begin{equation*}
			\mathcal{P}(n) : \text{\textquotedblleft}
			\forall x \in I^n,
			\forall \lambda \in [0;1]^n,
			\sum_{k=1}^{n} \lambda_k = 1 \implies
			\sum_{k=1}^{n} \lambda_k x_k \in I \wedge
			f\left( \sum_{k=1}^{n} \lambda_k x_k \right)
			\leqslant \sum_{k=1}^{n} \lambda_k f\left( x_k \right)
			\text{\textquotedblright}
		\end{equation*}
	
		\begin{itemize}[label=*, leftmargin=0.5cm]
			\item Soient $x \in I^1$ et $\lambda \in [0;1]^1$ tel que $\sum_{k=1}^{1} \lambda_k = 1$. \\
			Alors $\lambda_1 = 1$. Trivialement, $\sum_{k=1}^{1} \lambda_k x_k = \lambda_1 x_1 = x_1 \in I$. \\
			De plus, $f\left( \sum_{k=1}^{1} \lambda_k x_k \right)
				= f\left( \lambda_1 x_1 \right)
				= f\left( x_1 \right)
				= \lambda_1 f\left( x_1 \right)
				= \sum_{k=1}^{1} \lambda_k f\left( x_k \right)$. \\
			Donc $\mathcal{P}(1)$ vrai.

			\item  Soit $n \in \N^*$  tel que $\mathcal{P}(n)$ vrai. \\
			Soient $x \in I^{n+1}$ et $\lambda \in [0;1]^{n+1}$ tel que $\sum_{k=1}^{n+1} \lambda_k = 1$. \\
			$\{ x_k \;|_; k \in [\![1;n+1]\!] \}$ est une partie non vide ($n \geqslant 1$) d'un ensemble totalement ordonnée $\left(\R,\leqslant\right)$.
			Posons $a = \min\{ x_k \;|_; k \in [\![1;n+1]\!] \}$ et $b = \max\{ x_k \;|_; k \in [\![1;n+1]\!] \}$. D'où
			\begin{equation*}
				a
				\underbrace{=}_{\displaystyle \sum_{k=1}^{n+1} \lambda_k = 1} \sum_{k=1}^{n+1} \lambda_k a
				\underbrace{\leqslant}_{a \leqslant x_k} \sum_{k=1}^{n+1} \lambda_k x_k
				\underbrace{\leqslant}_{x_k \leqslant b} \sum_{k=1}^{n+1} \lambda_k b
				\underbrace{=}_{\displaystyle \sum_{k=1}^{n+1} \lambda_k = 1} b
			\end{equation*}
			Or $\{ x_k \;|_; k \in [\![1;n]\!] \} \subset I$ (car $x \in I^n$) donc $a \in I \wedge b \in I$. Donc
			\begin{equation*}
				\sum_{k=1}^{n+1} \lambda_k x_k
				\in [a;b]
				\underbrace{\subset}_{\begin{array}{c} \text{par convexité} \\ \text{de l'intervalle } I \end{array}} I
			\end{equation*}
			
			$\sum_{k=1}^{n+1} \lambda_k = 1$ donc $\exists i_0 \in [\![1;n+1]\!] : \lambda_{i_0} \neq 1$ (sinon $\sum_{k=1}^{n+1} \lambda_k = n+1 \neq 1$ car $n \neq 0$). \\
			Fixons un tel $i_0$.
			\begin{equation*}
				\begin{aligned}
					f\left( \sum_{k=1}^{n+1} \lambda_k x_k \right)
					&= f\left( \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k x_k + \lambda_{i_0} x_{i_0} \right) \\
					&= f\left( \lambda_{i_0} x_{i_0} + \left( 1 - \lambda_{i_0} \right) \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} x_k \right) \\
					\underbrace{\leqslant}_{\begin{array}{c} \text{Par convexité} \\ \text{de } f \end{array}}& \lambda_{i_0} f(x_{i_0}) + \left( 1 - \lambda_{i_0} \right) f\left( \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} x_k \right)
				\end{aligned}
			\end{equation*}
			Or $\displaystyle \forall i \in [\![1;n+1]\!] \lambda_i \leqslant \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k = 1 - \lambda_{i_0}$ Donc $\displaystyle \frac{\lambda_i}{1 - \lambda_{i_0}} \in [0;1]$ et $\displaystyle \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} = 1$. Nous pouvons appliquer $\mathcal{P}(n)$ pour $\lambda_i \rightarrow \frac{\lambda_i}{1 - \lambda_{i_0}}$ :
			\begin{equation*}
				\begin{aligned}
					f\left( \sum_{k=1}^{n+1} \lambda_k x_k \right)
					&\leqslant \lambda_{i_0} f(x_{i_0}) + \left( 1 - \lambda_{i_0} \right) \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \frac{\lambda_k}{1 - \lambda_{i_0}} f\left( x_k \right) \\
					&\leqslant \lambda_{i_0} f(x_{i_0}) + \sum_{\begin{array}{c} k = 1 \\ k \neq i_0 \end{array}}^{n+1} \lambda_k f\left( x_k \right) \\
					&\leqslant \sum_{k = 1}^{n+1} \lambda_k f\left( x_k \right) \\
				\end{aligned}
			\end{equation*}
			Donc $\mathcal{P}(n+1)$ vrai.
		\end{itemize}
	\end{question_kholle}
	
	\begin{question_kholle}
		[Soit $n \in \N^*$. Soit $x \in \R_+^{*n}$.
		\begin{equation}
				\left( \prod_{k=1}^{n} x_k \right)^{\nicefrac{1}{n}}
				\leqslant \frac{1}{n} \sum_{k=1}^{n} x_k
		\end{equation}]
		{Inégalité arithmético-géométrique}
		Soit de tels objets. Posons $\forall k \in [\![1;n]\!], \lambda_k = \nicefrac{1}{n}$. \\
		Sachant que l'exponentielle est convexe, appliquons l'inégalité de Jensen pour $x_k \leftarrow ln(x_k)$ (autorisé car $x_k \in \R_+^*$) :
		\begin{equation*}
			\exp \left( \sum_{k=1}^{n} \frac{1}{n} \ln \left( x_k \right) \right)
			\leqslant \sum_{k=1}^{n} \frac{1}{n} \exp \left( \ln \left( x_k \right) \right)
		\end{equation*}
		L'exponentielle est la bijection réciproque du logarithme népérien et est un morphisme additif. Nous obtenons ainsi l'inégalité recherchée.
	\end{question_kholle}
\pagebreak\section{Semaine 16}
	
	\begin{question_kholle}
		{Unicité de la partie régulière d'un développement limité}
		
		Soit $f$ une fonction admettant un $DL_n(x_0)$ avec $n \in \N$ et $x_0 \in \mathcal{D}_f$. \\
		Supposons que $f$ admette deux développements limités. C'est-à-dire qu'il existe $a \in \C^{n+1}$ et $b \in \C^{n+1}$ \tqs :
		\begin{equation*}
			\begin{aligned}
				f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} a_k (x - x_0)^k + o\left((x-x_0)^n\right) \\
				f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} b_k (x - x_0)^k + o\left((x-x_0)^n\right)
			\end{aligned}
		\end{equation*}
		
		Posons $u = x - x_0$ et  $\tilde{f}(u) = f(x_0+u)$ de sorte que les hypothèses sur $f$ se traduise par l'existence d'un $DL_n(0)$ pour $\tilde{f}$ :
		\begin{equation*}
			f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} a_k u^k + o\left(u^n\right)
			\text{ et }
			f(x) \underset{x \rightarrow x_0}{=} \sum_{k=0}^{n} b_k u^k + o\left(u^n\right)
		\end{equation*}
		Appliquons la définition d'un $DL_n(0)$. Il existe deux fonctions $\varepsilon_1$ et $\varepsilon_2$ définies sur $\mathcal{D}_{\tilde{f}}$ \tqs
		\begin{equation*}
			\begin{aligned}
				\forall u \in \mathcal{D}_{\tilde{f}}, \ \tilde{f}(u) = \sum_{k=0}^{n} a_k u^k + u^n \varepsilon_1 \\
				\forall u \in \mathcal{D}_{\tilde{f}}, \ \tilde{f}(u) = \sum_{k=0}^{n} b_k u^k + u^n \varepsilon_2 \\
				\textlim{u}{0} \varepsilon_1(u) = 0 \text{ et } \textlim{u}{0} \varepsilon_2(u) = 0
			\end{aligned}
		\end{equation*}
		Donc
		\begin{equation*}
			\forall u \in \mathcal{D}_{\tilde{f}}, \
			\sum_{k=0}^{n} (a_k - b_k) u^k = u^n \left( \varepsilon_2(u) - \varepsilon_1(u) \right)
		\end{equation*}
		
		Par l'absurde, supposons que $\exists k_0 \in \lient 0; n \rient : a_{k_0} \neq b_{k_0}$. Posons $k_1$ le plus petit entier dont les coefficients $a$ et $b$ sont différents :
		\begin{equation*}
			k_1 = \min \left\{ k \in \lient 0;n \rient \;|\; a_k \neq b_k \right\}
		\end{equation*}
		Nous obtenons alors
		\begin{equation*}
			\forall u \in \mathcal{D}_{\tilde{f}}, \
			\sum_{k=0}^{k_1-1} \underbrace{(a_k - b_k)}_{=0} u^k + (a_{k_1} - b_{k_1}) u^{k_1} + \sum_{k=k_1+1}^{n} (a_k - b_k) u^k = u^n \left( \varepsilon_2(u) - \varepsilon_1(u) \right)
		\end{equation*}
		Multiplions par $u^{-k_1}$ puis calculons la limite en $u \rightarrow 0$.
		D'un coté, pour $k > k_1$, nous avons $k - k_1 \leqslant 1$ donc $(a_k - b_k) u^{k-k_1} \arrowlim{u}{0} 0$.
		De l'autre coté, $u^{n-k_1}$ tend vers $0$ ou $1$ selon si $k_1 < n$ ou $k_1 = n$. Et, par hypothèse, $\varepsilon_2(u) - \varepsilon_1(u) \arrowlim{u}{0} 0$.
		Par unicité de la limite, $a_{k_1} - b_{k_1} = 0$. Ce qui contredit la définition de $k_1$.
		
		Par conséquent $\forall k \in \lient 0;n \rient, \ a_k = b_k$. Ainsi, la partie régulière d'un $DL$ est unique.
	\end{question_kholle}
	
	\setnbquestion{6}
	
	\begin{question_kholle}
		{Deux fonctions équivalentes au voisinage de $a$ ont le même signe sur un voisinage de $a$}
		
		Soient $f : \mathcal{D} \rightarrow \R$ et $g : \mathcal{D} \rightarrow \R$ telles que $f(x) \underset{x \rightarrow a}{\sim} g(x)$ avec $a \in \mathcal{D}$. \\
		Appliquons la définition de l'équivalence pour $\varepsilon \leftarrow \frac{1}{2}$, il existe un voisinage $V$ de $a$ tel que :
		\begin{equation*}
			\forall x \in V \cap \mathcal{D},
			| f(x) - g(x) | \leqslant \frac{1}{2} | g(x) |
		\end{equation*}
	
		Fixons un tel voisinage $V$.
		Nous obtenons :
		\begin{equation*}
			\forall x \in V \cap \mathcal{D},
			\underbrace{g(x) - \frac{1}{2} | g(x) |}_{\text{du signe de }g(x)}
			\leqslant f(x) \leqslant
			\underbrace{g(x) + \frac{1}{2} | g(x) |}_{\text{du signe de }g(x)}
		\end{equation*}
	
		Ainsi $f(x)$ et $g(x)$ ont le même signe sur $V \cap \mathcal{D}$.
	\end{question_kholle}

	\begin{question_kholle}
		[ Soient $f \in \Cont{\infty}{\mathcal{D}}{}$ et $a \in \overset{\circ}{\mathcal{D}}$. Supposons que $E_0 = \left\{ p \in \N^* \setminus \{1\} \;|\; f^{(p)}(a) \neq 0 \right\}$ est non vide. \\
		Posons $p_0 = \min E_0$. \\
		$f$ admet un extremum local en $a$ si et seulement si $f'(a) = 0$ et $p_0$ est pair. \\
		$f$ admet un point d'inflexion en $a$ si et seulement si $p_0$ est impair. ]
		{Condition nécessaire et suffisante pour qu'une fonction \Cont{\infty}{}{} admette un extremum local ou un point d'inflexion}
		
		Soient de tels objets. Traitons le cas de l'extremum local.
		
		\noindent $f \in \Cont{\infty}{}{}$ donc, la formule Taylor-Young donne un $DL_{p_0}(a)$ de $f$ :
		\begin{equation*}
			f(x) \underset{x \rightarrow a}{=}
			\sum_{k=0}^{p_0} \frac{f^{(k)}(a)}{k!} (x-a)^k + o \left( (x-a)^{p_0} \right)
		\end{equation*}
		
		En développant :
		\begin{equation*}
			f(x) \underset{x \rightarrow a}{=}
			f(a) + \underbrace{f'(a)(x-a)}_{= 0} + \underbrace{\ldots + \frac{f^{(p_0-1)}(a)}{(p_0-1)!} (x-a)^{p_0-1}}_{= 0 \text{ par défintion de }p_0} + \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0} + o \left( (x-a)^{p_0} \right)
		\end{equation*}
		
		Ainsi (car $f^{(p_0)}(a) \neq 0$)
		\begin{equation}
			f(x) - f(a) \underset{x \rightarrow a}{\sim} \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}
		\end{equation}
		Au voisinage de $a$, $f(x) - f(a)$ et $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ ont le même signe.
		\\
		
		Supposons que $f$ admette un extremum local en $a$.
		Or $a \in \overset{\circ}{\mathcal{D}}$ et $f$ est dérivable en 0, donc $f'(a) = 0$.
		Comme $f$ admette un extremum local en $a$, $f(x) - f(a)$ est de signe constant au voisinage de $a$.
		Donc $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ est de signe constant au voisinage de $a$.
		Par conséquent, $p_0$ est pair.
		\\
		
		Réciproquement, supposons que $f'(a) = 0$ et que $p_0$ est pair. $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$ est de signe constant au voisinage de $a$. Donc $f(x) - f(a)$ est de signe constant au voisinage de $a$. Ainsi, $a$ est un extremum local de $f$.
		\\
		
		Traitons le cas du point d'inflexion. La formule de Taylor-Young donne :
		\begin{equation}
			f(x) - \underbrace{\left( f(a) + (x-a)f'(a) \right)}_{\text{tangente en } (a,f(a))}
			\underset{x \rightarrow a}{\sim} \frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}
		\end{equation}
		Le signe de l'écart courbe/tangente en $a$ est donc celui de $\frac{f^{(p_0)}(a)}{p_0!} (x-a)^{p_0}$. Ce qui conclut de la même manière que l'extremum local.
	\end{question_kholle}
	
\pagebreak\section{Semaine 17}

	\setnbquestion{4}
	 \begin{question_kholle}
	    [Soient $a,b\in \N ^*$ et $c\in \Z$. Il existe des entiers $x,y\in \Z$ tels que $ax + yb = c$ si et seulement si $c$ est multiple du pgcd de $a$ et $b$.]
	    {Théorème de Bézout}
	    Soient $a,b\in \N^*$. On suppose l'algorithme d'Euclide réalisé pour $a,b$, ainsi à la fin de ce dernier on a un entier naturel $r_n$ tel que $r_n = a\wedge b$. Comme l'algorithme est terminé, on peut remonter chaque ligne de proche en proche, on aurait, à titre d'exemple, pour une première itération, $r_n =r_{n-2} - q_{n} \cdot r_{n-1}$. En réalisant toutes les étapes nécessaires, on obtient une relation entre $r_n$ et $a,b$, cette relation s'écrit : $$\exists \ (x_0, y_0)\in (\Z ^*)^2 \ : \ a\wedge b = r_n = ax_0 + y_0b.$$
	    Si $c$ est un multiple de $a\wedge b $, alors il existe $k\in \Z^*$ tel que $c = k(a\wedge b)$, donc en multipliant le résultat montré au dessus par $k$, on a le sens indirect. Si pour $c\in \Z$, il existe des entiers $x,y\in \Z$ tels que $ax + yb = c$, alors le pgcd de $a$ et $b$ divise le membre de gauche et donc par égalite le membre de droite aussi donc $c$ est multiple de $a\wedge b$, ce qui suffit. 
	\end{question_kholle}
	
	\setnbquestion{6}
	\begin{question_kholle}
	    [Soient $a,b,c$ trois entiers naturels non nuls. Si $c$ est premier avec $a$ et divise le produit $ab$, alors il divise $b$.]
	    {Théorème de Gauss}
	    Soient $a,b,c$ des entiers naturels vérifiant les hypothèses. \\
	    Comme $c$ est premier avec $a$ on écrit une relation de Bézout pour $1$, leur pgcd et on multiplie le tout par $b$ : 
	    $$\exists \ (u,v) \in (\N ^*)^2 \ : \ au + vc = 1 \ \implies \ abu + vbc = b, $$
	    or $c$ divise $ab$ et lui-même donc aussi le membre de gauche donc par égalité, le membre droite, c'est le théorème.
	\end{question_kholle}
	
	\setnbquestion{8}
	\begin{question_kholle}
	    [Soient $a,b,c\in \Z$. Résoudre l'équation $$ax + yb = c,$$ d'inconnues $x$ et $y$ dans $\Z$.]
	    {Résoudre une équation du type $ax + yb = c$}
	    Soient $a,b,c\in \Z$ et une telle équation, notée $(i)$, en lesdites inconnues. \\
	    Si $a\wedge b \not| \ c$, alors le théorème de Bézout, affirme que l'équation n'a pas de solution. \\
	    Supposons le contraire. Posons $d = a\wedge b$. Le lemme technique affirme l'existence de $a'$ et $b'$ dans $\Z$, tels que $a'd= a$, $b'd =b$ et $a' \wedge b' = 1$. Donc, comme $d$ divise $c$, il existe $c'$ tel que $c = c'd$. On réécrit l'équation, notée $(ii)$ : $$ a'x +yb'  = c'.$$ On sait d'après le théorème de Bézout qu'il existe des solutions, en particulier grâce à l'algorithme d'Euclide on construit $(x_0,y_0)$, une solution de la nouvelle équation, puis on l'injecte et on raisonne par équivalence, on note $\omega$ l'ensemble des solutions de $(ii)$ et $\Omega$ celui de $(i)$ :
	    \begin{center}
	    $
	    \begin{array}{ccc}
	      (x,y) \in \Omega  & \iff &  (x,y)\in \omega \\
	         &\iff & a'x +yb' = c' \\
	         &\iff & a'x + yb' = a'x_0 + y_0b' \\
	         &\iff & a'(x-x_0) = b'(y_0 -y) \\[1ex]
	         &\iff & \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
	                                                a'(x-x_0) & = & b'(y_0 -y)  \\
	                                                y_0 - y & = & a'k 
	                                                \end{array} \right. \\[2ex]
	         &\iff &  \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
	                                                a'(x-x_0) & = & b'(y_0 -y)  \\
	                                                 y & = & y_0 - a'k 
	                                                \end{array} \right. \\[2ex]
	         &\iff &  \exists \ k \in \Z \ :\left\{  \begin{array}{ccc}
	                                                 x & = & x_0 + b'k  \\
	                                                 y & = & y_0 - a'k 
	                                                \end{array} \right. \\[2ex]
	         &\iff & (x,y) \in \{ (x_0 + b'k,\  y_0 - a'k) \ | \ k\in \Z \}
	    \end{array}
	    $ \\
	    \end{center}
	    La première ligne découle de la divisibilité des coefficients par $d$, la deuxième est la définition d'appartenance à $\omega$, la troisième est une réécriture du fait que $(x_0,y_0)$ soit solution de $(ii)$, la quatrième est une factorisation banale, la cinquième une utilisation du théorème de Gauss pour le sens direct et le sens indirect ne pose pas de problème, la sixième est une réécriture de la deuxième relation, la septième découle de l'expression de $y$ pour le sens direct et le sens indirect s'obtient en multipliant avec parcimonie l'équation, la huitième est une réécriture de la septième qui ne pose pas de problème. C'est $\Omega$, par équivalence.
	    
	\end{question_kholle}
\pagebreak\section{Semaine 18}

	\begin{question_kholle}
		{L'ensemble des nombres premiers est infini}

		Notons l'ensemble des nombres premiers $\PRIME = \left\{ n \in \N \;|\; \left| \mathcal{D}(n) \cup \N \right| = 2 \right\}$
		Par l'absurde, supposons que \PRIME est fini. \\
		Posons $\displaystyle m = 1 + \prod_{p \in \PRIME} p \in \N$. \\
		Comme $2 \in \PRIME$, $m \geqslant 2$. Donc $m$ admet un diviseur premier, $\exists q \in \PRIME : q \;|\; m$. Donc $q \wedge m = q$. \\
		Par ailleurs, $\displaystyle m = 1 + q \left( \prod_{\tiny \begin{matrix} p \in \PRIME \\ p \neq q \end{matrix}} p \right)$. Donc $\displaystyle m - q \left( \prod_{\tiny \begin{matrix} p \in \PRIME \\ p \neq q \end{matrix}} p \right) = 1$. D'après le théorème de Bézout, $q \wedge m = 1$. \\
		Donc $q = 1$ ce qui est une contradiction avec $q \in \PRIME$.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $n \in \N^*, p \in \PRIME, k_0 \in \N$.
		\begin{equation}
			\nu_p(n) = k_0 \iff
			\exists m \in \Z : \left\{ \begin{matrix}
				n = p^{k_0} m \\
				m \wedge p = 1
			\end{matrix} \right.
		\end{equation}
		]
		{Caractérisation de la valuation \textit{p}-adique}

		$\implies$ Supposons que $\nu_p(n) = k_0$. \\
		Par définition de la valuation \textit{p}-adique, $p^{\nu_p(n)} \;|\; n$ donc $p^{k_0} \;|\; n$.
		Notons $m \in \Z$ le quotient de la division euclidienne de $n$ par $p^{k_0}$. Nous avons $n = p^{k_0} m$. \\
		Comme $m \wedge p \in \mathcal{D}(p) \cap \N$, $m \wedge p \in \left\{1,p\right\}$.
		Par l'absurde, supposons que $m \wedge p = p$.
		\begin{equation*}
			\begin{aligned}
				p \;|\; m
				&\implies \exists m' \in \Z : m = p m' \\
				&\implies \exists m' \in \Z : n = p p^{k_0} m' = p^{k_0+1} m' \\
				&\implies k_0 + 1 \in \left\{ k \in \N \;|\; p^k | n \right\} \\
				&\implies k_0 + 1 \leqslant \max \left\{ k \in \N \;|\; p^k | n \right\} = \nu_p(n) = k_0
			\end{aligned}
		\end{equation*}
		Ce qui est une contradiction donc $m \wedge p = 1$.

		$\impliedby$ Supposons $\exists m \in \Z : \left\{ \begin{matrix}
			n = p^{k_0} m \\
			m \wedge p = 1
		\end{matrix} \right.$ \\
		Par définition de la valuation \textit{p}-adique, $p^{\nu_p(n)} \;|\; n$ donc $p^{\nu_p(n)} \;|\; p^{k_0} m$. Or $m \wedge p = 1$ donc $m \wedge p^{\nu_p(n)} = 1$.
		D'après le théorème de Gauss, $p_{\nu_p(n)} \;|\; p^{k_0}$. Donc $\exists \alpha \in \Z : \alpha p_{\nu_p(n)} = p^{k_0}$
		\begin{equation*}
			\begin{aligned}
				\alpha p_{\nu_p(n)} = p^{k_0}
				&\implies p^{k_0} - \alpha p_{\nu_p(n)} = 0 \\
				&\implies p^{k_0} \left( 1 - \alpha p^{\nu_p(n) - k_0} \right) = 0 \text{ car } k_0 \leqslant \nu_p(n) \\
				&\implies \alpha p^{\nu_p(n) - k_0} = 1 \text{ car \Z est intègre} \\
				&\implies p^{\nu_p(n) - k_0} \in \mathcal{D}(1) \cap \N \\
				&\implies p^{\nu_p(n) - k_0} = 1 \\
				&\implies \nu_p(n) - k_0 = 0 \\
				&\implies \nu_p(n) = k_0 \\
			\end{aligned}
		\end{equation*}
	\end{question_kholle}

    \begin{question_kholle}
    	[\begin{equation}
    		\forall (a, b) \in \Z^2, \
    		a|b \ \iff \ \forall p \in \PRIME, \ \nu _p (a) \leq \nu _p (b)
    	\end{equation}]
        {Caractérisation de $a | b$ par les valuations $p$-adiques et preuve de leur propriété de morphisme.}
        Premièrement, montrons que la valuation $p$-adique est un morphisme de $(\Z ^* , \times)$ dans $(\N ,+)$. \\
        Soient de tels entiers relatifs $a,b$. \\
        \[
            \exists \ m,n \in (\Z ^*)^2 \ : \ \left(\left(a = p^{\nu _p (a)}m\right) \ \wedge \ (m\wedge p = 1)\right) \ \wedge \ \left(\left(b = p^{\nu _p (b)}n\right) \ \wedge \ (n\wedge p = 1)\right),
        \]
        donc $ab = p^{\nu _p (a) + \nu _p (b)}mn$ et $mn \wedge p = 1$, par la réciproque de la caractérisation des valuations $p$-adiques :
        \[
            \nu _p (ab) = \nu _p (a) + \nu _p (b).
        \]
        Prouvons le sens réciproque de la susdite caractérisation.  Supposons le membre de droite. \\
        D'après le théorème de décomposition en facteurs premiers,
        \[
            |b| = \prod_{p\in \PRIME} p^{\nu _p (b)} = \prod_{p\in \PRIME} p^{\nu _p (a)}(p^{\nu _p (b) - \nu _p (a)}) = \prod_{p\in \PRIME}  p^{\nu _p (a)} \prod_{p\in \PRIME} p^{\nu _p (b) - \nu _p (a)}= |a|\prod_{p\in \PRIME} p^{\nu _p (b) - \nu _p (a)},
        \]
        la première manipulation se justifie par hypothèse et la seconde peut se justifier par le calcul.\\
        Ainsi, $|a| | |b|$ donc $a|b$. \\
        Prouvons le sens direct. Supposons le membre de gauche.  \\
        Soit $p \in \PRIME$. Il existe $k\in \Z$ tel que $ak = b $ car $a |b$. Ainsi,
        \[
            \nu _p (b) = \nu _p (ak) = \nu _p (a) + \nu _p (k) \geq \nu _p (a).
        \]
        Ce qui suffit.
    \end{question_kholle}

    \begin{question_kholle}
        [Le pgcd comme produit des $p$ à la puissance du minimum des $\nu _p$ et le ppcm comme le produit des  $p$ à la puissance du maximum des $\nu _p$.
        \begin{equation}
        	\begin{aligned}
        		a \wedge b &= \prod_{p\in \PRIME} p^{\min (\nu _p (a),\nu_p (b))} \\
        		a \vee b &= \prod_{p\in \PRIME} p^{\max (\nu _p (a),\nu_p (b))}
        	\end{aligned}
        \end{equation}]
        {Expression du pgcd et du ppcm à partir des décomposition en facteurs premiers de $a$ et $b$.}
        Prouvons la formule du pgcd et déduisons-en la formule du ppcm. \\
        Soient $(a,b)\in (\Z ^*)^2$. Soit $p \in \PRIME $. Il faut et il suffit de montrer que $\nu_p (a \wedge b) = \min (\nu_p(a), \nu_p(b))$ pour obtenir le résultat. On a $a\wedge b | a$ et $a \wedge b | b$ donc d'après la caractérisation de la divisibilité par les valuations $p$-adiques, $\nu_p (a \wedge b) \leq \nu _p (a)$ et $\nu _p (a\wedge b) \leq \nu _p (b)$ donc $\nu _p (a\wedge b) \leq \min (\nu _p(a), \nu_p (b)).$ \\
        Posons $m = \min (\nu _p(a), \nu_p (b))$. On a
        \[
            |a| = \prod_{q \in \PRIME} q ^{\nu _q (a)} = p^m \left( (p^{\nu_p (a)- m})\prod_{q \in \PRIME \backslash \{p\}} q ^{\nu _q (a)} \right),
        \]
        car par définition, $m \leq \nu _p (a)$, donc $p^m | a$, on montrerait de même que $p^m |b$, donc par définition, $p^m | a\wedge b$, donc une nouvelle fois en appliquant la caractérisation de la divisibilité par les valuations $p$-adiques, $m \leq \nu_p ( a\wedge b)$. Finalement, $\nu_p (a\wedge b) = m$. \\
        On en déduit la formule du ppcm :
        \[
            |a||b| = (a \wedge b) (a \vee b) \ \implies \ a\vee b = \prod_{p\in \PRIME} p^{\nu _p(a) + \nu _p(b) - \min (\nu _p (a),\nu_p (b))} =  \prod_{p\in \PRIME} p^{\max (\nu _p (a),\nu_p (b))}
        \]

    \end{question_kholle}

    \begin{question_kholle}
        [Petit Th. de Fermat :
        {\begin{enumerate}[label=($\roman*$)]
        	\item $\forall a \in \Z , \ a^p \equiv a \mod p$ \\
	        $\ \forall x \in \Z / p\Z, \ x^p = x $
	        \item $\forall a \in \Z, \ p\not | a, \ \implies \ a^{p-1} \equiv 1 \mod p$ \\
        	$\ \forall x \in \Z / p\Z, \ x^{p-1} = 1 $
        \end{enumerate}}]
        {Pour $p$ premier, $(a+b)^p \equiv a^p + b^p \mod p$, en déduire le petit Th. de Fermat (2 versions), expression du résultat dans $\Z / p\Z$.}
        Soient $a,b$ de tels entiers relatifs et soit $p$ un nombre premier. Calculons,
        \[
            (a + b )^p  = \sum_{k = 0}^p \binom{p}{k}a^{p-k}b^k  = a^p + b^p + \sum_{k = 1}^{p-1} \binom{p}{k}a^{p-k}b^k \equiv a^p + b^p \mod p,
        \]
        car $\forall k \in [\![1,p-1 ]\!], \ p | \binom{p}{k}$ (élémentaire), d'où le résultat. \\
        Dans $\Z /p\Z$, ce résultat s'énonce comme suit :
        \[
            \forall (x,y) \in \Z /p\Z ^2, \ (x+y)^p = x^p + y^p.
        \]
        En guise d'application, démontrons le petit Th. de Fermat énoncé plus haut. \\
        Démonstration du $(i)$. Considérons le prédicat $\PRIME ( \cdot)$ défini sur $\N$  par :
        \[
            \PRIME (a) : "a^p \equiv a \mod p".
        \]
        Initialisation : Pour $a = 0$, rien à faire, donc $\PRIME (0)$ est vrai. \\
        Hérédité : Soit $a\in \N$ \tq $\ \PRIME (a)$. Calculons,
        \[
            (a+1)^p  \equiv a^p + 1 \mod p \overset{\PRIME (a)}{\equiv} a + 1 \mod p,
        \]
        donc $\PRIME (a+1)$ vrai. \\
        Par Th. de récurrence sur $\N$, $\PRIME(a)$ est vrai pour tout $a\in \N$. \\
        Il faut maintenant étendre le résultat à $\Z$. Soit $p\in \PRIME \backslash \{2\}$, ainsi $p$ est impair. Soit $a\in \Z \backslash \N$. Calculons,
        \[
            a^p\equiv (-|a|)^p \mod p \equiv - |a|^p \mod p \overset{\underset{\text{pour }a \gets |a|}{\text{Th. de Fermat}}}{\equiv} - |a| \mod p \equiv a \mod p  .
        \]
        Si $p =2$, $\ a^2 \equiv |a|^2 \mod 2 \equiv |a| \mod 2 \equiv -|a| \mod 2 \equiv a \mod 2$. \\
        Le $(ii)$, soit $a\in \Z$ tel que $p \not | a$.
        \[
            (p\not | a)\wedge (p\in \PRIME) \implies p\wedge a = 1,
        \]
        d'après le $(i)$, $\ p | a^p -a \ \implies \ p| a(a^{p-1} -1) \ \overset{\text{Th. de Gauss}}{\implies}  \ p| a^{p-1} -1 \ \implies \ a^{p-1} \equiv 1 \mod p$. \\
        Les écritures dans $\Z /p \Z$ ne posent pas de problème.s, ce qui conclut.
    \end{question_kholle}

    \begin{question_kholle}
        []
        {$\Z /n\Z$ est un corps si et seulement si $n$ est premier.}
        Montrons le sens réciproque, supposons $n\in \PRIME$. \\
        Soit $x\in \Z/n\Z$ tel que $x \neq \overline{0}$. \\
        $\exists \ a \in [\![0,p-1]\!]\ : \ c = \overline{a}$, $\ I =[\![0, p-1]\!]$ étant un système de représentant des classes. \\
        Comme $a\in I, \ n\not | a$, or $n \in \PRIME$, donc $n \wedge a = 1$. Par Bezout, il existe $u,v \in \Z^2$ tels que $au +nv =1$, donc $u$ est l'inverse de $a$ modulo $n$ donc $a\in \Z/n\Z ^\times$, dès lors, tout élément non nul de $\Z/n\Z$ est inversible, or c'est un anneau commutatif, donc c'est un corps. \\ \\
        Montrons le sens direct en raisonnant par contraposition, supposons $n\not \in \PRIME$. \\
        Comme $n$ n'est pas premier et est plus grand que $2$, il admet un diviseur, $d$, dans $I\backslash \{0,1\} = J$. Notons $d'$ le quotient de la division euclidienne de $n$ par $d$, on a alors $a = dd'$ et $d'\in J$. Donc $\overline{d}\overline{d'}=\overline{0}$ et comme $d,d' \in J$, on a $d,d' \neq 0$, donc $\overline{d}$ est un diviseur de zéro de $\Z/n\Z$, donc $\overline{d}$ est un élément non nul de $\Z/n\Z$ non inversible, donc $\Z/n\Z$ n'est pas un corps. En contraposant ce que nous venons de démontrer on a le résulat. Ce qui conclut.
    \end{question_kholle}

	\begin{question_kholle}
		{Les éléments inversibles d'un anneau $A$ forment un groupe multiplicatif noté $\left( A^\times, \times \right)$}

		Soit $(A, +, \times)$ un anneau. \\
		Un élément inversible (ou unité) est un élément de $A$ symétrisable pour la loi $\times$. Posons l'ensemble des éléments inversibles $A^\times = \left\{ a \in A \;|\; \exists b \in A : a \times b = b \times a = 1_A \right\}$.

		\begin{itemize}[label=$\star$, leftmargin=.5cm]
			\item Montrons que la LCI $\times$ se restreint bien à $A^\times$ en un LCI $\times_{A^\times}$. \\
			Soient $(a_1, a_2) \in {A^\times}^2$.
			Par défintion de $A^\times$, $\exists (b_1, b_2) \in A^2 : a_1 \times b_1 = b_1 \times a_1 = 1_A \text{ et } a_2 \times b_2 = b_2 \times a_2 = 1_A$.
			\begin{equation*}
				\left( a_1 \times a_2 \right) \times \left( b_2 \times b_1 \right)
				\underbrace{=}_{\text{loi associative}} a_1 \times \underbrace{a_2 \times b_2}_{= ~ 1_A} \times b_1
				= a_1 \times b_1
				= 1_A
			\end{equation*}
			\begin{equation*}
				\left( b_2 \times b_1 \right) \times \left( a_1 \times a_2 \right)
				\underbrace{=}_{\text{loi associative}} b_2 \times \underbrace{b_1 \times a_1}_{= ~ 1_A} \times a_2
				= b_2 \times a_2
				= 1_A
			\end{equation*}
			Donc $\left( a_1 \times a_2 \right) \in A^\times$.

			\item La loi $\times$ est associative donc la loi $\times_{A^\times}$ l'est aussi.

			\item $1_A$ vérifie $1_A \times 1_A = 1_A$ donc $1_A \in A^\times$. \\
			De plus, $\forall a \in A^\times, 1_A \times_{A^\times} a = a \times_{A^\times} 1_A = a$ donc $\times_{A^\times}$ admet $1_A$ comme élément neutre.

			\item Soit $a \in A^\times$. Par définition de $A^\times$, $\exists b \in A : a \times b = b \times a = 1_A$. \\
			D'où $b \in A^\times$. En pensant les égalités ci-dessus dans $A^\times$,
			\begin{equation*}
				a \times_{A^\times} b = b \times_{A^\times} a = 1_A
			\end{equation*}
			Donc $a$ est inversible dans $A^\times$.
		\end{itemize}

		Ainsi, $\left( A^\times, \times_{A^\times} \right)$ est un groupe.
	\end{question_kholle}

	\begin{question_kholle}
		{L'image directe par un morphisme d'anneau d'un sous-anneau de l'anneau de départ est un sous anneau de l'anneau d'arrivée. De même pour l'image réciproque.}

		Soient $\left(A,+,\times\right)$ et $\left(B,+,\times\right)$ deux anneaux et $f : A \rightarrow B$ un morphisme d'anneau.

		\noindent Soit $A'$ un sous-anneau de $A$. Montrons que $f(A')$ est un sous-anneau de $B$.
		\begin{itemize}[label=$\star$, leftmargin=.5cm]
			\item Par définition de $f$, $f(A') \subset B$ et $(B,+,\times)$ est un anneau.
			\item Soient $(u,v) \in f(A')^2$. Alors $\exists (a,b) \in A'^2 : f(a) = u \text{ et } f(b) = v$. $f$ est un morphisme d'anneau donc un morphisme de groupe de $(A,+)$ dans $(B,+)$ donc
			\begin{equation*}
				u - v = f(a) - f(b) = f(a - b)
			\end{equation*}
			Comme $A'$ est un sous-anneau, $a - b \in A'$. Donc $u - v \in f(A')$. \\
			De même, $f$ est un morphisme d'anneau donc un morphisme de monoïde de $(A,\times)$ dans $(B,\times)$ donc
			\begin{equation*}
				u \times v = f(a) \times f(b) = f(a \times b)
			\end{equation*}
			Comme $A'$ est un sous-anneau, $a \times b \in A'$. Donc $u \times v \in f(A')$.
			\item $f$ est un morphisme d'anneau donc $1_B = f(1_A)$. Or $A'$ est un sous-anneau donc $1_A \in A'$. D'où $1_B \in f(A')$.
		\end{itemize}

		\noindent Soit $B'$ un sous-anneau de $B$. Montrons que $f^{-1}(B')$ est un sous-anneau de $A$.
		\begin{itemize}[label=$\star$, leftmargin=.5cm]
			\item Par définition de $f$, $f^{-1}(B') \subset A$ et $(A,+,\times)$ est un anneau.
			\item Soient $(a,b) \in f^{-1}(B')^2$. $f$ est un morphisme d'anneau donc un morphisme de groupe de $(A,+)$ dans $(B,+)$ donc
			\begin{equation*}
				f(a - b) = \underbrace{f(a)}_{\in B'} - \underbrace{f(b)}_{\in B'} \in B'
			\end{equation*}
			Donc $a - b \in f^{-1}(B')$. \\
			De même, $f$ est un morphisme d'anneau donc un morphisme de monoïde de $(A,\times)$ dans $(B,\times)$ donc
			\begin{equation*}
				f(a b) = \underbrace{f(a)}_{\in B'} \underbrace{f(b)}_{\in B'} \in B'
			\end{equation*}
			Donc $a b \in f^{-1}(B')$.
			\item $f$ est un morphisme d'anneau donc $1_B = f(1_A)$. Or $B'$ est un sous-anneau donc $1_B \in B'$. D'où $1_A \in f^{-1}(B')$.
		\end{itemize}
	\end{question_kholle}
\pagebreak\section{Semaine 19}
	
	\begin{question_kholle}
		[Pour une matrice $A \in \mathcal{M}_{(n,p)}(\K)$, la matrice transposée est définie :
		{\begin{equation*}
			\forall (k, l) \in [\![1,p]\!] \!\times\! [\![1,n]\!], \ \left[A^T\right]_{kl} = A_{lk}
		\end{equation*}}
		Formellement, la transposition est une application de $\mathcal{M}_{(n,p)}(\K)$ dans $\mathcal{M}_{(p,n)}(\K)$.]
		{$\left(A \times B\right)^T = B^T \times A^T$}
		
		Soit $(A, B) \in \mathcal{M}_{(n,p)}(\K) \times \mathcal{M}_{(p,q)}(\K)$. \\
		$\left(A \times B\right)^T \in \mathcal{M}_{(q,n)}(\K)$. Soit $(i, j) \in [\![1,q]\!] \!\times\! [\![1,n]\!]$.
		\begin{equation*}
			\begin{aligned}
				\left[ \left(A \times B\right)^T \right]_{i,j}
				&= \left[A \times B\right]_{j,i} \\
				&= \sum_{k=1}^{p} A_{j,k} \times_\K B_{k,i} \\
				&= \sum_{k=1}^{p} B_{k,i} \times_\K A_{j,k} \\
				&= \sum_{k=1}^{p} \left[B^T\right]_{i,k} \times_\K \left[A^T\right]_{k,j} \\
				&= \left[ \left(B^T\right) \times \left(A^T\right) \right]_{i,j}
			\end{aligned}
		\end{equation*}
	\end{question_kholle}
	
	\begin{question_kholle}
		[Le symbole de Kronecker est défini de la manière suivante :
		\begin{equation*}
			\forall (x, y) \in \R^2, \delta_{xy} = \left\{ \begin{matrix}
				0 \text{ si } x \neq y \\
				1 \text{ si } x = y
			\end{matrix} \right.
		\end{equation*}
		La matrice $E^{i,j} \in \mathcal{M}_{(n, p)}(\K)$ avec {$(i, j) \in [\![ 1, n ]\!] \!\times\! [\![ 1, p ]\!]$} ne possède que des coefficients nuls sauf le coefficient de la $i^{e}$ ligne et $j^{e}$ colonne qui vaut 1. Formellement :
		{\begin{equation*}
			\forall (r, s) \in [\![ 1, n ]\!] \times [\![ 1, p ]\!], \
			\left[E^{i,j}\right]_{rs} = \delta_{ir} \delta_{js}
		\end{equation*}}]
		{Calculer $E^{i,j} \times E^{k,l}$ en fonction de $i$, $j$, $k$, $l$ et des symboles de Kronecker}
				
		Calculons $E^{i,j}(n,p) \times E^{k,l}(p,q)$.
	 
		Soient $(r, s) \in [ \! [ 1, n] \!] \times [ \! [ 1, q ] \!]$ fq
		
		\begin{align*}
			\left[ E^{i,j} \times E^{k,l} \right] _{rs}
			& = \sum_{t = 1}^{n}E^{i,j}_{r,t} E^{k,l}_{t,s} \\
		 	& =\sum_{t = 1}^{n} \delta_{ir} \delta_{jt} \delta_{kt} \delta_{ls} \\
		 	& = \delta_{jk} \delta_{ir} \delta_{ls} \\
		 	& = \delta_{jk} \left[ E^{i,l} \right] _{rs}
		\end{align*}
		
		Donc $E^{i,j} \times E^{k,l} = \delta_{jk} E^{i,l}$.
		
		
		Ainsi, pour le calcul de $(E^{i,j})^{2}$, $q \leftarrow n$, $k \leftarrow i$, $l \leftarrow j$.
		
		\begin{align*}
			(E^{i,j})^{2} = \delta_{ji} E^{i,j} = \left\{ 
			\begin{array}{ll}
			  E^{i,j} \text{ si } i = j  \\ 
			  0_{n,p} \text{ si } i \neq j 
			\end{array}
			\right.
		\end{align*}
	
	\end{question_kholle}
	
	\begin{question_kholle}
		{Les matrices triangulaires supérieures forment un sous-anneau de $\mathcal{M}_n(\K)$}
		
		$\mathcal{T}_n^+(\K) \subset \mathcal(M)_n(\K)$ et $(\mathcal{M}_n(\K), +, \times)$ est un anneau. \\
		$\mathcal{T}_n^+(\K) \neq \emptyset$ car $I_n \in \mathcal{T}_n^+(\K)$ ($I_n$ est le neutre multiplicatif de $\mathcal{M}_n(\K)$). \\
		Soient $(A, B) \in \mathcal{T}_n^+(\K)^2$ . \\
		Soient $(i, j) \in [\![1,n]\!]^2$ $\text{ tels que }$ $i > j$.
		\begin{equation*}
			(A - B)_{i,j}
			= \underbrace{A_{i,j}}_{=0 \text{ car } A \in \mathcal{T}_n^+(\K)} - \underbrace{B_{i,j}}_{=0 \text{ car } B \in \mathcal{T}_n^+(\K)}
			= 0
		\end{equation*}
		Donc, $A - B \in \mathcal{T}_n^+(\K)$.
		
		\begin{equation*}
			\begin{aligned}
				(A \times B)_{i,j}
				&= \sum_{k=1}^{n} A_{i,k} \times_\K B_{k,j} \\
				&= \sum_{k=1}^{j} \underbrace{A_{i,k}}_{=0 \text{ car } i > j \geqslant k \text{ et } A \in \mathcal{T}_n^+(\K)} \times_\K B_{k,j}
				+ \sum_{k=j+1}^{n} A_{i,k} \times_\K \underbrace{B_{k,j}}_{=0 \text{ car } k > j \text{ et } B \in \mathcal{T}_n^+(\K)} \\
				&= 0
			\end{aligned}
		\end{equation*}
		Donc, $A \times B \in \mathcal{T}_n^+(\K)$.
	\end{question_kholle}
 
	\begin{question_kholle}
		{Si $A$ est une matrice d'ordre $n$ et $\lambda$ un scalaire non nul d'un corps, alors la transposée de $A$ et $\lambda A$ sont inversibles aussi.}
		Soient $A,\lambda \in \mathcal{GL}_n(\mathbb{K})\times \mathbb{K}^*$, avec $\mathbb{K}$ un corps. \\
		Par définition, il existe $B\in \mathcal{GL}_n(\mathbb{K})$ tel que $AB=BA=I_n$. Ainsi : 
		\[
		(AB)^T = I_n^T \ \iff \ B^TA^T = I_n,
		\]
		donc $A^T$ admet un inverse à gauche, $B^T$, donc un inverse tout court et donc $A^T$ est inversible (on notera que $A^T$ reste dans les matrices d'ordre $n$). De même, 
		\[
		\lambda AB = \lambda I_n \ \iff \ (\lambda A)B = \lambda I_n \ \iff \ (\lambda A) \left(\frac{1}{\lambda}B \right) = I_n, 
		\]
		car les scalaires commutent avec toutes les matrices. Ainsi, $\lambda A$ admet un inverse à droite, donc un inverse tout court, donc est inversible, d'inverse $\frac{1}{\lambda}B$. Concluant la preuve.    
	\end{question_kholle}
	
	\begin{question_kholle}
		{Si $N$ est une matrice d'ordre $n$ nilpotente, alors $I_n + \lambda N$ est inversible pour tout $\lambda$, scalaire d'un corps.}
		Soient $N$ une matrice d'ordre $n$ à coefficient dans $\mathbb{K}$, un corps, nilpotente, d'indice de nilpotence $k$ (un entier naturel donc) et $\lambda \in \mathbb{K}$. Calculons : 
		\[
		I_n^{2k+1} + (\lambda N)^{2k+1} = I_n^{2k+1} - (- \lambda N)^{2k+1} = (I_n + \lambda N)\sum_{i=0}^{2k}(-\lambda N)^i =  (I_n + \lambda N)\sum_{i=0}^{k-1}(-\lambda N)^i,
		\]
		car $\lambda N$ commute avec $I_n$, or le membre de gauche est égal à $I_n$ car $2k+1 > k$, donc $I_n + \lambda N$ est inversible à droite, donc inversible tout court, d'inverse $\sum_{i=0}^{k-1}(-\lambda N)^i$. Ce qui conclut la preuve.
	\end{question_kholle}
	
	\begin{question_kholle}
		[$A \in \mathcal{M}_n(\K)$ est inversible si et seulement si pour tout $Y \in \mathcal{M}_{n,1}(\K)$, l'équation $AX = Y$ d'inconnue $X \in \mathcal{M}_{n,1}$ admet une unique solution.
		\begin{equation}
			\forall A \mathcal{M}_n(\K),
			A \in \mathcal{GL}_n(\K) \iff
			\forall Y \in \mathcal{M}_{n,1}(\K), \exists ! X \in \mathcal{M}_{n,1} : AX = Y
		\end{equation}]
		{Caractérisation de l'inversibilité pour les matrices}
		
		Supposons que $A \in \mathcal{GL}_n(\K)$.
		Soit $Y \in \mathcal{M}_{n,1}(\K)$ \fq. \\
		$AX = Y \iff A^{-1}AX = A^{-1}Y \iff X = A^{-1}Y$ donc l'équation $AX = Y$ d'inconnue $X \in \mathcal{M}_{n,1}$ admet une unique solution.
		
		Supposons maintenant que $\forall Y \in \mathcal{M}_{n,1}(\K), \exists ! X \in \mathcal{M}_{n,1} : AX = Y$. \\
		Pour $i \in [\![1;n]\!]$, notons  $X_i$ la solution de $AX = E^{i,1}$. \\
		Posons $\displaystyle B = \left[ \begin{array}{c|c|c|c}
			&&&\\
			X_1 & X_2 & \ldots & X_n \\
			&&&\\
		\end{array} \right]$. \\
		Calculons $\displaystyle AB
		= \left[ \begin{array}{c|c|c|c}
			&&&\\
			AX_1 & AX_2 & \ldots & AX_n \\
			&&&\\
		\end{array} \right]
		= \left[ \begin{array}{c|c|c|c}
			&&&\\
			E^{1,1} & E^{2,1} & \ldots & E^{n,1} \\
			&&&\\
		\end{array} \right]
		= I_n$. \\
		Ainsi A est inversible à droite donc $A \in \mathcal{GL}_n(\K)$ et $A^{-1} = B$.
	\end{question_kholle}
	
	\begin{question_kholle}
		[Une matrice diagonale est inversible si et seulement si tous ses coefficients diagonaux sont non nuls.
		\begin{equation}
			\forall D = diag(d_1, d_2, \ldots, d_n) \in \mathcal{D}_n(\K),
			D \in \mathcal{GL}_n(\K) \iff \prod_{i=1}^{n} d_i \neq 0
		\end{equation}]
		{Caractérisation des matrices diagonales inversibles}
		
		Soit $D \in \mathcal{D}_n(\K)$ de coefficients diagonaux $d_1, d_2, \ldots, d_n \in \K^n$. \\
		Soit $Y = \begin{bmatrix} y_1 \\ \ldots \\ y_n \end{bmatrix} \in \mathcal{M}_{n,1}(\K)$.
		\'Etudions l'équation $DX = Y$ d'inconnue $X = \begin{bmatrix} x_1 \\ \ldots \\ x_n \end{bmatrix} \in \mathcal{M}_{n,1}(\K)$.
		\begin{equation*}
			DX = Y \iff
			\left\{ \begin{array}{cccccccccc}
				d_1 x_1 & & & & = & y_1 & & & \\
				& d_2 x_2 & & & = & & y_2 & & & \\
				& & \ddots & & = & & & \ddots & \\
				& & & d_n x_n & = & & & & y_n \\
			\end{array} \right.
		\end{equation*}

		\begin{itemize}
			\item Si $\exists i_0 \in [\![1;n]\!] : d_{i_0} = 0$, la $i_0$-ème ligne du système ci-dessus deviens une condition de compatibilité $0 = y_{i_0}$ qui ne sera pas respecté pour $Y = E^{i_0,1}$. Donc $D \notin \mathcal{GL}_{n}(\K)$.
			\item Sinon $\forall i \in [\![1;n]\!] : d_i \neq 0$, le système est donc triangulaire à coefficients diagonaux non nuls. Il admet donc une unique solution. Ainsi $D \in \mathcal{GL}_{n}(\K)$.
			\begin{equation*}
				DX = Y \iff
				\left\{ \begin{array}{cccccccccc}
					x_1 & & & & = & d_1^{-1} y_1 & & & \\
					& x_2 & & & = & & d_2^{-1} y_2 & & & \\
					& & \ddots & & = & & & \ddots & \\
					& & & x_n & = & & & & d_n^{-1} y_n \\
				\end{array} \right.
			\end{equation*}
			Ainsi $D^{-1} = diag\left(d_1^{-1}, d_2^{-1}, \ldots, d_n^{-1}\right)$.
		\end{itemize}
	\end{question_kholle}
\pagebreak\section{Semaine 20}

	\begin{question_kholle}
		[{\begin{equation}
				\K[X] ^\times = \left\{ \lambda X^0, \lambda \in \K^* \right\}
		\end{equation}}]
		{Éléments inversibles de l'anneau $\K[X]$}

		Soit P un élément inversible de $\K[X]$.
		Alors $\exists Q \in \K[X] : P \cdot Q = Q \cdot P = 1_{\K[X]}$.
		En prenant les degrés des polynômes, $\text{deg } P + \text{deg } Q = 0$. \\
		Or $\text{deg } : \K[X] \rightarrow \N$ donc $\text{deg } P = \text{deg } Q = 0$.
		Donc $\exists \lambda \in \K^* : P = \lambda$. \\
		Ainsi $\K[X]^\times \subset \left\{ \lambda X^0, \lambda \in \K^* \right\}$.

		Soit $\lambda \in \K^*$. Considérons $P = \lambda$.
		Posons $Q = \lambda^{-1}$ (car \K est un corps). $P \cdot Q = \lambda \lambda^{-1} = 1$ et $Q \cdot P = \lambda^{-1} \lambda = 1$ donc $P$ est inversible. Ainsi $\left\{ \lambda X^0, \lambda \in \K^* \right\} \subset \K[X]^\times$.
	\end{question_kholle}

	\begin{question_kholle}
		[Le problème d'interpolation de Lagrange est, pour $n \in \N$ avec $a \in \K^{n+1}$ et $b \in \K^{n+1}$, l'ensemble des polynômes passant par tous les points de coordonnée $(a_i, b_i)$. C'est-à-dire l'ensemble des {$P \in \K[X]$} vérifiant :
		{\begin{equation}
			\forall i \in [\![0;n]\!], P(a_i) = b_i
		\end{equation}}
		Il existe une unique solution $P$ de degré $\leqslant n$ au problème d'interpolation de lagrange, et elle s'exprime de la manière suivante en posant
		\begin{equation}
			L_{i} = \prod_{\substack{j=0\\j \neq i}}^{n} \frac{X - a_{j}}{a_{i} - a_{j}}
		\end{equation}
		\begin{equation}
			P = \sum_{i=0}^{n}b_{i}L_{i}
		\end{equation}]
		{Théorème d'interpolation de lagrange}

		\begin{itemize}
			\item Unicité

			Supposons qu'il existe $(P, Q) \in \mathbb{K}_n[X]^{2}$ solutions du problème d'interpolation.

			Alors $\forall i \in [ \! [ 0, n ] \!], \tilde{P}(a_{i}) = \tilde{Q}(a_{i}) = b_{i}$

			Posons $H = P - Q$, alors, $\forall i \in [ \! [ 0, n ] \!], \tilde{H}(a_{i}) = \tilde{P}(a_{i}) - \tilde{Q}(a_{i}) = 0$.

			De plus, $\deg H = \deg(P-Q) \leqslant \max \left\{ \deg P, \deg Q \right\}$

			Donc $H$ est un polynôme de degré $\leqslant n$ avec $\lvert [ \! [ 0, n ] \!] \rvert = n+1$ racines.

			Donc $H$ est le polynôme nul.
			\item Existence
			Soit $i \in [ \! [ 0, n ] \!]$ fq
			Notons $L_{i}$ une solution de degré $\leqslant n$ au problème $Pb_{i}$ suivant:
			$$
			(Pb_i)
			\left\{ \begin{array}{ll}
				\tilde{P}(a_{0}) = 0 \\
				\vdots \\
				\tilde{P}(a_{i-1}) = 0 \\
				\tilde{P}(a_{i}) = 1 \\
				\tilde{P}(a_{n}) = 0 \\
				\vdots \\
				\tilde{P}(a_{n}) = 0
			\end{array} \right.
			$$
			On remarque que $(a_{0},\dots ,a_{i-1}, a_{i+1},\dots, a_{n})$ sont $n$ racines deux à deux distinctes de $L_{i}$. Or $L_{i}$ est de degré $\leqslant n$ et n'est pas le polynôme nul (car $\tilde{L_{i}}(a_{i}) = 0$) donc $(a_{0},\dots ,a_{i-1}, a_{i+1},\dots, a_{n})$ sont les \emph{seules} racines de $L_{i}$, toutes simples.

			Dès lors,
			$$
			\exists c \in \mathbb{K}^{*} : L_{i} = c\prod_{\substack{j=0 \\ j\neq i}} ^{n}(X-a_{j})
			$$
			Pour trouver le $c$, remarquons que

			\begin{align*}
				\tilde{L_{i}}(a_{i}) = 1 &\iff c\prod_{\substack{j=0 \\ j\neq i}} ^{n}(a_{i}-a_{j}) = 1\\
				&\iff c = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{1}{a_{i}-a_{j}} \right)
			\end{align*}

			Ainsi, s'il existe une solution au problème $Pb_{i}$ c'est nécéssairement
			$$L_{i} = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{X-a_{j}}}{a_{i}-a_{j}} \right)$$

			Réciproquement, cette solution est correcte puisque
			$$\forall k \in [ \! [ 0, n ] \!], k \neq i,  \tilde{L_{i}}(a_{k}) = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{\overbrace{ a_{k}-a_{j} }^{ =0 }}}{a_{i}-a_{j}} \right) = 0$$
			Et
			$$\tilde{L_{i}}(a_{i}) = \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{a_{i}-a_{j}}}{a_{i}-a_{j}} \right) = \prod_{\substack{j=0 \\ j\neq i}} ^{n} 1 = 1$$

			Posons donc $P = \sum_{i=0} ^{n} b_{i}Li$.

			Alors, par construction,
			$$
			\forall k \in [ \! [ 0, n ] \!], \tilde{P}(a_{k}) = \sum_{i=0} ^{n}\left(  b_{i} \prod_{\substack{j=0 \\ j\neq i}} ^{n}\left( \frac{{a_{k}-a_{j}}}{a_{i}-a_{j}} \right) \right) = \sum_{i=0} ^{n}\left(  b_{i} \delta_{ki} \right) = b_{k} \delta_{kk} = b_{k}
			$$
			Nous avons donc construit une solution unique au problème d'interpolation de Lagrange
		\end{itemize}
	\end{question_kholle}

    \begin{question_kholle}
        [Soient $P$ à coefficients dans $\K$ et $a\in \K$. On a : 
        \begin{equation}
            P = \sum_{n\in \N}\frac{\widetilde{P^{(n)}}(a)}{n!}(X-a)^n
        \end{equation}
        ]
        {Formule de Taylor dans $\K [X]$ (caractéristique nulle)}
        
        Considérons le prédicat $\PRIME (\cdot)$ défini sur $\N$ par : 
        \[
        \PRIME (n ) \ :  \text{\textquotedblleft} \ \forall P \in \K _n[X], \ P = \sum_{k = 0}^n\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^k \ \text{\textquotedblright}
        \]
        Initialisation : pour $n = 0 $, soit $P\in \K _0 [X]$. \\
        $\exists \ p_0 \in \K \ : \ P = p_0 X^0$ et $\sum_{k = 0}^0\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^k = \frac{\widetilde{P^{(0)}}(a)}{1}X^0 = p_0 X^0$, donc $\PRIME (0)$ vrai. \\ \\
        Hérédité : Soit $n\in \N$ tel que $\PRIME (n)$. Soit $P\in \K _{n+1} [X]$. On a donc $\deg P' = \deg P -1 \leq n$ donc $\PRIME (n)$ s'applique à $P'$ : 
        \[
        P' = \sum_{k = 0}^n\frac{\widetilde{P'^{(k)}}(a)}{k!}(X-a)^k = \left( \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} \right) ',
        \]
        donc : 
        \[
        \left( P - \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} \right) ' = 0 \ \implies \ \exists \ \mu \in \K \ : \ P - \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{k!}\frac{(X-a)^{k+1}}{k+1} = \mu ,
        \]
        ainsi : 
        \[
        P = \sum_{k = 0}^n\frac{\widetilde{P^{(k+1)}}(a)}{(k+1)!}(X-a)^{k+1} + \mu = \sum_{k = 1}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k} + \mu,
        \] 
        donc en $a$ par $\ph _ a$ : 
        \[
        \widetilde{P}(a) = \mu \ \implies \ P = \sum_{k = 1}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k} + \widetilde{P}(a) = \sum_{k = 0}^{n+1}\frac{\widetilde{P^{(k)}}(a)}{k!}(X-a)^{k},
        \]
        donc $\PRIME (n+1) $ vrai. Ainsi par théorème de récurrence sur $\N$, $\PRIME (n)$ est vrai pour tout $n\in \N$.
    \end{question_kholle}

	\begin{question_kholle}
		[Soit {$P \in \K[X]$}. Soit $a \in \K$.
		\begin{equation}
			a \text{ est une racine de } P \text{ de multiplicité au moins } m
			\iff \left\{ \begin{matrix}
				P(a) = 0 \\
				P'(a) = 0 \\
				\ldots \\
				P^{(m-1)}(a) = 0
			\end{matrix} \right.
		\end{equation}
		\begin{equation}
			a \text{ est une racine de } P \text{ de multiplicité d'exactement } m
			\iff \left\{ \begin{matrix}
				P(a) = 0 \\
				P'(a) = 0 \\
				\ldots \\
				P^{(m-1)}(a) = 0 \\
				P^{m}(a) \neq 0
			\end{matrix} \right.
		\end{equation}]
		{Caractérisation de la multiplicité d'une racine}

		\begin{enumerate}[label=$\bullet$]
			\item Supposons que $a$ est une racine de $P$ de multiplicité au moins $m$. \\
			Alors $\exists Q \in \K[X] : P = (X-a)^m Q$. D'après la formule de Leibniz, pour tout $k \in [\![0;m-1]\!]$,
			\begin{equation*}
				\begin{aligned}
					P^{(k)}
					&= \sum_{i=0}^{k} \binom{k}{i} \left( (X-a)^m \right) ^{(k-i)} Q^{(i)} \\
					&= \sum_{i=0}^{k} \binom{k}{i} \frac{m!}{(m-(k-i))!} (X-a)^{m-(k-i)} Q^{(i)} \\
					&= \underbrace{ (X-a)^{(m-k)} }_{\substack{\text{c'est un bien un polynôme} \\ \text{non constant car } k < m}} \sum_{i=0}^{k} \binom{k}{i} \frac{m!}{(m-(k-i))!} (X-a)^{i} Q^{(i)}
				\end{aligned}
			\end{equation*}
			Donc $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$.

			\item Supposons que $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$. \\
			Appliquons la formule de Taylor a.
			\begin{equation*}
				\begin{aligned}
					P &= \sum_{n \in \N} \frac{P^{(n)}(a)}{n!} (X-a)^n \\
					&= \sum_{n = 0}^{m-1} \underbrace{ \frac{P^{(n)}(a)}{n!} }_{=0} (X-a)^n + \sum_{\substack{n \in \N \\ n \geqslant m}} \frac{P^{(n)}(a)}{n!} (X-a)^n \\
					&= (X-a)^m \sum_{\substack{n \in \N \\ n \geqslant m}} \frac{P^{(n)}(a)}{n!} \underbrace{(X-a)^{n-m}}_{\in \K[X] \textit{ car } n - m \in \N}
				\end{aligned}
			\end{equation*}
			Donc $(X-a)^m | P$. Donc $a$ est racine de $P$ de multiplicité au moins $m$.

			\item Supposons que $a$ est une racine de $P$ de multiplicité exactement $m$. \\
			Nous pouvons appliquer le point précédent car la multiplicité est supérieur à $m$ : $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$. \\
			Par l'absurde, si $P^{(m)}(a) = 0$ alors le point précédent donne que $a$ a une multiplicité supérieur à $m + 1$ donc $m \geqslant m + 1$ ce qui est une contradiction. \\
			Par conséquent, $P^{(m)}(a) \neq 0$.

			\item Supposons $\forall k \in [\![0;m-1]\!], P^{(k)}(a) = 0$ et $P^{(m)}(a) \neq 0$. \\
			En reprenant le calcul précédent, pour $k = m$, en sachant que $(X-a)^{(m-k)} = X^0$,
			\begin{equation*}
				P^{(m)} = \binom{m}{0} \frac{m!}{0!} (X-a)^{0} P + \sum_{i=1}^{m} \binom{m}{i} \frac{m!}{i!} (X-a)^{i} Q^{(i)}
			\end{equation*}
			D'où $P^{(m)}(a) = m! \ Q(a)$ donc $Q(a) = \frac{P^{(m)}(a)}{m!}$. Donc $Q(a) \neq 0$. \\
			Par l'absurde, supposons que $(X-a)^{m+1} | P$. Alors $\exists R \in \K[X] : P = (X-a)^{m+1} R$. Donc $(X-a)^{m+1} R = (X-a)^m Q$ d'où $Q = (X-a) R$. Nous obtenons $Q(a) = 0$ ce qui est une contradiction avec $Q(a) = 0$. \\
			Donc $a$ est une racine de $P$ de multiplicité strictement inférieur à $m + 1$ et, d'après le point précédent, supérieur à $m$. Donc $a$ est une racine de $P$ de multiplicité exactement $m$.
		\end{enumerate}
	\end{question_kholle}

    \begin{question_kholle}
        []
        {Identification de $\K[X]$ à $\K [x]$, par l'injectivité de $\Phi$}
        Montrons que l'application $\Phi$ définie comme suit est injective : 
        \[
        \Phi : \left|   \begin{array}{ccc}
                            \K[X] & \longrightarrow & \mathcal{F}(\K, \K) \\
                            P & \longmapsto & \widetilde{P}
                        \end{array} \right..
        \] 
        Soit donc $P\in \ker \Phi $, on a : 
        \[
        \Phi (P)  = \widetilde{0} \ \implies \ \widetilde{P} = \widetilde{0} \text{ sur } \K \ \implies \ P = 0_{\K [X]},
        \]
        donc $\ker \Phi \subset \{ 0_{\K [X]} \}$. \\
        Réciproquement, on calcule l'image du polynôme nul par $\Phi$ : 
        \[
        \Phi (0 _{\K [X]} ) = \widetilde{0},
        \]
        donc $0_{\K [X]} \in \ker \Phi$, ainsi on a l'égalité ensembliste et donc cela suffit.
    \end{question_kholle}

	\begin{question_kholle}
		[Les fonctions symétriques élémentaires $\displaystyle \left( \sigma_k \right)_{k \in [\![0;n]\!]}$ pour une famille $\displaystyle \left( x_k \right)_{k \in [\![1;n]\!]}$ sont définies par
		\begin{equation}
			\sigma_ k = \sum_{1 \leqslant i_1 < \ldots < i_k \leqslant n} \ \prod_{j=1}^{k} x_{i_j}
		\end{equation}]
		{Pour $P = (X-x_1)(X-x_2)(X-x_3)$, exprimer $x_1^3 + x_2^3 + x_3^3$ en fonction des fonctions symétriques élémentaires}

		Sous forme développée, $P = X^3 - (x1 + x_2 + x_3) X^2 + (x_1 x_2 + x_1 x_3 + x_2 x_3) X - x_1 x_2 x_3 = X^3 - \sigma_1 X^2 + \sigma_2 X - \sigma_3$. Comme $x_1, x_2, x_3$ sont racines de $P$, nous avons les trois égalité suivantes :
		\begin{equation*}
			\begin{aligned}
				0 = P(x_1) &= x_1^3 - \sigma_1 x_1^2 + \sigma_2 x_1 - \sigma_3 \\
				0 = P(x_1) &= x_2^3 - \sigma_1 x_2^2 + \sigma_2 x_2 - \sigma_3 \\
				0 = P(x_1) &= x_3^3 - \sigma_1 x_3^2 + \sigma_2 x_3 - \sigma_3
			\end{aligned}
		\end{equation*}
		En sommant ces trois équation,
		\begin{equation*}
			0 = x_1^3 + x_2^3 + x_3^3 - \sigma_1 (x_1^2 + x_2^2 + x_3^2) + \sigma_2 (x_1 + x_2 + x_3) - 3 \sigma_3
		\end{equation*}
		Cherchons la somme des carrés.
		\begin{equation*}
			\begin{aligned}
				(x_1 + x_2 + x_3)^2 &= x_1^2 +  x_2^2 + x_3^2 + 2 x_1 x_2 + 2 x_1 x_3 + 2 x_2 x_3 \\
				\implies x_1^2 +  x_2^2 + x_3^2 + x_1 x_2 &= \sigma_1^2 - 2 \sigma_2
			\end{aligned}
		\end{equation*}
		Ainsi \begin{equation*}
			x_1^3 + x_2^3 + x_3^3 = \sigma_1^3 - 3 \sigma_1 \sigma_2 + 3 \sigma_3
		\end{equation*}
	\end{question_kholle}

	\begin{question_kholle}
		[Les sommes de Newton $(S_k)_{k\in\Z^*}$ pour une famille $(x_k)_{k \in \N^*}$ sont définies par (sous réserve d'existence pour $k<0$) :
		\begin{equation}
			S_k = \sum_{i=1}^{n} x_i^k
		\end{equation}]
		{Expression de $S_2$, $S_{-1}$ et $S_{-2}$ à l'aide des fonctions élémentaires symétriques.}

		\begin{equation*}
			\begin{aligned}
				\sigma_1^2 &= \left( \sum_{i=1}^{n} x_i \right)^2 \\
				&= \underbrace{ \sum_{i=1}^{n} x_i^2 }_{S_2} + \ 2 \ \underbrace{ \sum_{1 \leqslant i < j \leqslant n} x_i x_j }_{\sigma_2} \\
				\implies S_2 &= \sigma_1^2 - 2 \sigma_2
			\end{aligned}
		\end{equation*}
		\begin{equation*}
			S_{-1} = \sum_{i=1}^{n} \frac{1}{x_i}
			= \frac{\displaystyle \sum_{i=1}^{n} \prod_{\substack{ j = 1 \\ j \neq i }}^{n} x_j }{\displaystyle \prod_{i=1}^{n} x_i }
			= \frac{\sigma_{n-1}}{\sigma_n}
		\end{equation*}
		\begin{equation*}
			\begin{aligned}
				S_{-2} &= \sum_{i=1}^{n} \frac{1}{x_i^2} \\
				&= \left( \sum_{i=1}^{n} \frac{1}{x_i} \right)^2 - 2 \sum_{1 \leqslant i < j \leqslant n} \frac{1}{x_i} \frac{1}{x_j} \\
				&= \frac{\sigma_{n-1}^2}{\sigma_n^2} - 2 \frac{\displaystyle \sum_{1 \leqslant i < j \leqslant n} \prod_{\substack{ k = 1 \\ k \notin \{i,j\} }} \frac{1}{x_j} }{\sigma_n} \\
				&= \frac{\sigma_{n-1}^2 - 2 \sigma_{n-2}\sigma_n}{\sigma_n^2}
			\end{aligned}
		\end{equation*}
	\end{question_kholle}
\pagebreak\section{Semaine 21}
    \begin{question_kholle}
        [Tous les polynômes de degré 1 sont irréductibles, les polynômes irréductibles de degré $2$ ou $3$ sont les polynômes sans racine.s dans le corps de base.]
        {Caractérisation des polynômes irréductibles de degré 1, 2 et 3 dans \cx{\K}.}
        
        Un polynôme de degré $1$ ne peut s'écrire comme produit de 2 polynômes de \\ degré $\geq 1$ donc il est irréductible. \\
        Soit $P \in \cx{\K}$ un polynôme irréductible de degré $2$ ou $3$. \\
        Par définition, $P$ n'a pas de racine.s dans $\K$, donc la première inclusion. \\
        Soit $P\in \K [X]$ tel que $\deg P = 2$. \\
        Montrons que si $P$ n'a pas de racine dans \K alors $P$ est irréductible. Montrons la contraposée. Supposons $P$ non-irréductible. \\
        \[
            \exists \ A, \ B \in \K [X] \ : \ P = AB \ \text{et} \ \deg A, \ \deg B \geq 1,
        \]
        On a alors, $P=AB \ \implies \ 2 = \deg A + \deg B \ \implies \ \deg A, \ \deg B = 1$ donc :
        \[
            \exists \ \alpha, \ \gamma \in \K^* \times \K \ : \ A = \alpha X + \gamma, 
        \]
        ainsi, $P=( \alpha X + \gamma )B= \alpha \left( X + \frac{\gamma}{\alpha} \right)B $, donc $P$ admet $-\frac{\gamma}{\alpha}\in \K$ comme racine, ce qui montre la contraposée.\\
         Soit $P\in \K [X]$ tel que $\deg P = 3$.\\
         Montrons, de même, la contraposée. Supposons $P$ non-irréductible. De même, on a : 
         \[
            \exists \ A, \ B \in \K [X] \ : \ P = AB \ \text{et} \ \deg A, \ \deg B \geq 1,
         \]
         Puis encore, $P=AB \ \implies \ 3 = \deg A + \deg B \ \implies \ \deg A, \ \deg B \in \{2,1\}$ (l'un n'étant pas l'autre). Donc l'un des deux est de degré 1 donc $P$ admet une racine dans $\K$, donc encore une fois cela montre la contraposée, ce qui démontre l'inclusion réciproque.
    \end{question_kholle}

    \begin{question_kholle}
        [Les polynômes irréductibles de $\cx{\C}$ sont les polynômes de degré $1$ et ceux de $\cx{\R}$ sont les polynômes de degré $1$ et les polynômes de degré $2$ de discriminant strictement négatif. ]
        {Polynômes irréductibles de $\C [X]$ et de $\R [X]$.}
        
        Le premier point est immédiat, les polynômes irréductibles d'un corps contiennent les polynômes de degré $1$ et par le théorème de D'Alembert-Gauss, tout polynôme de $\C [X]$ ($\deg \geq 2$) est scindé dans $\C [X]$, donc non-irréductible. \\ \\
        Pour le second point, le cas du degré $1$ est réglé. Soit $P$ un polynôme irréductible de $\R [X]$. \\
        Supposons que $P$ soit de degré supérieur ou égal à $3$. Si son degré est impair, le TVI conclut quant à l'existence d'une racine, donc non-irréductible. Si son degré est pair, par D'Alembert-Gauss, on obtient $\deg P$ couples de racines possiblement égaux. \\
        Or, $P\in \cx{\R}$ donc $\forall z \in \C, \ P(z) = 0 \ \implies P(\overline{z}) = 0 $ donc les racines se rassemblent 2 à 2 pour former un polynôme scindé dans $\R$, donc non-irréductible. Ainsi, $\deg P = 2$, immédiatement, si le discriminant de $P$ est positif ou nul, $P$ admet une ou deux racines dans $\R$, donc non irréductible. Enfin, son discriminant est alors négatif, de cette manière $P$ n'admet pas de racine dans $\R$ et est donc irréductible. Ce qui achève la preuve.
    \end{question_kholle}

    \begin{question_kholle}
        [Il s'agit donc de montrer que racine cubique de $2$ n'est pas un rationnel.]
        {$X^3 -2$ est irréductible dans $\cx{\Q}$.}
        
        Supposons, par l'absurde, qu'il existe $r\in \Q$ tel que $r^3 - 2 = 0 $. Prenons $p,q \in \Z \times \N^*$ \textit{le} représentant irréductible de $r$ dans $\Q$. On a alors, $p^3 = 2q^3$ donc $2\ | \ p^3$ or $2\in \PRIME$ donc $2\ | \ p$, ainsi, il existe $k\in \Z$ tel que $p = 2k$. Par conséquent, $2(2k^3) = q^3$ donc $2 \ | \ q^3$ or $2 \in \PRIME$ donc $2\ |\ q $ donc ceci contredit $p$ et $q$ premiers entre eux, par définition d'un représentant irréductible. Ainsi, $P = X^3 -2$ n'admet pas de racine dans \Q, c'est donc un polynôme irréductible.
    \end{question_kholle}

    \begin{question_kholle}
        [Pour $\displaystyle P = \prod_{k=1}^p(X-z_k)^{m_k} \in \cx{\C} \setminus \{0_{\C[X]}\}$ avec $m_k \in \N^*$ pour tout $k\in \lient 1,p \rient$, on a
        \begin{equation}
        	P \wedge P' = \prod_{k=1}^p(X-z_k)^{m_k - 1}
        \end{equation}
        C'est une conséquence de la définition du pgcd de deux polynômes $P \wedge Q = \prod_{i\in I}P_i^{\min \{ m_i, p_i\} }$, où les $P_i$ sont les facteurs irréductibles de $P$ et $Q$ dans leur décomposition. ]
        {PGCD d'un polynôme de \cx{\C} et son polynôme dérivé}
        
        Soit $P$ un tel polynôme et $p$ un entier naturel non nul. Naturellement, $P'$ hérite de $P$, $\deg P - p$ racines, lesquelles sont les $z_k$ pour $k\in \lient 1 , p \rient $, de multiplicité $m_k -1$. Ainsi, 
        \[
            \exists \ B \in \C [X] \ : \ \left[ P' = \left( \prod_{k=1}^p(X-z_k)^{m_k -1} \right) B \right] \ \wedge \ \left[  \deg B = p  \right],
        \]
        de cette manière on peut écrire : 
        \[
            P' = \left( \left( \prod_{k=1}^p(X-z_k)^{m_k -1} \right) B \right) P^0 \ \text{et} \ P = \left( \prod_{k=1}^p(X-z_k)^{m_k} \right) (P')^0,
        \]
        de façon à faire apparaître dans les deux décompositions les mêmes facteurs, possiblement avec une puissance $0$, histoire de coller à la définition de manière explicite. Ceci fait, il ne reste plus qu'à appliquer la définition du pgcd et de remarquer que seuls les $(X-z_k)^{m_k -1}$ subsistent. \\ Notons $\mathfrak{I}$ l'ensemble des facteurs de leur décomposition, on a alors :  
        \[
            P \wedge P' = \prod_{D\in \mathfrak{I}}D ^{\min \{ \nu_D (P), \nu _D(P') \}} = \prod_{k=1}^p (X-z_k)^{m_k -1},
        \]
        où $\nu _D (\cdot ) $ est la valuation $D$-adique au sens des polynômes irréductibles. Ce qui conclut.
    \end{question_kholle}

    \begin{question_kholle}
        [Il s'agit là de vérifier que la définition que l'on souhaiterait le plus, c'est-à-dire la même que pour la dérivée d'une fraction de fonctions, s'applique effectivement aux fractions rationnelles, c'est-à-dire que cette définition ne dépend pas du représentant choisi.]
        {Justifier la bonne définition de la dérivée d'une fraction rationnelle.}
        Montrons que pour $A, \ B \in \K [X] \times \K [X] \backslash \{ 0 _{ \K[X]} \}$, on a : 
        \[
            \left( \frac{A}{B} \right) ' = \frac{A'B - B'A}{B^2}.
        \]
        Soient $A$ et $B$ de tels polynômes et $C, \ D \in \K [X] \times \K [X] \backslash \{ 0 _{ \K[X]} \}$ tels que $AD = BC$, en dérivant on obtient $A'D + D'A = B'C + C'B$. Calculons : 
        \begin{center}
        $
        \begin{array}{rcl}
            (A'B-B'A)D^2 & = & D(A'BD- (AD)B')  \\
             & = & D(A'BD- BCB') \\
             & = & BD(A'D- CB') \\
             & = & BD(C'B - D'A) \\
             & = & B(C'BD - (AD)D') \\
             & = & B(C'BD - BCD') \\
             & = & B^2(C'D - D'C),
        \end{array}
        $
        \end{center}
        ce qui prouve que le résultat ne dépend pas du représentant, par définition de $\K(X)$ comme structure quotient.
    \end{question_kholle}

    \begin{question_kholle}
        [Les racines du polynôme dérivée sont dans l'enveloppe convexe des racines du polynôme. \\
        Soit {$P\in \C[X]$} de degré au moins $2$ et notons $z_1, \dots, z_n$ ses racines répétées avec multiplicité. \\
        Soit $u$ une racine de $P'$.
        Alors : 
        \begin{equation}
        	\exists \ (c_1, \dots, c_n)\in \R ^*_+ \ : \ \sum_{k=1}^n c_k z_k = u \ \text{et} \ \sum_{k=1}^n c_k = 1.
        \end{equation}]
        {Théorème de Gauss-Lucas et interprétation graphique.}
        Pour ce qui est de l'interprétation graphique, elle n'est pas prévue à l'heure qu'il est dans ce pdf, pour la faire soi-même dessiner des points et les "clôturer" dans un polygone convexe, ou même faire ceci avec un cas concret. \\

        \noindent $\rightarrow$ Si $u$ est une racine de $P$ alors noter $k_0$ son indice et utiliser le symbole de Kronecker. \\
        $ \sum_{k=1}^n \delta_{k, k_0} z_k = u \ \text{et} \ \sum_{k=1}^n \delta_{k, k_0} = 1. $ \\
        $\rightarrow$ Sinon, $u$ n'appartient pas aux racines de $P$, donc $u$ n'est pas pôle de $\frac{P'}{P}$ ce qui permet de prendre l'image par le morphisme d'évaluation en $u$ de cette même fraction rationnelle : 
        \[
            0_\K = \frac{P'(u)}{P(u)} = \sum_{k = 1}^n \frac{1}{u - z_k} = \sum_{k = 1}^n \frac{\overline{u}- \overline{z_k}}{|u-z_k|^2} = \sum_{k = 1}^n \frac{\overline{u}}{|u-z_k|^2}- \sum_{k = 1}^n \frac{\overline{z_k}}{|u-z_k|^2}.
        \]
        Donc en passant la seconde somme à gauche et en prenant le conjugué : 
        \[
            \sum_{k = 1}^n\frac{u}{|u-z_k|^2} = \sum_{k = 1}^n \frac{z_k}{|u-z_k|^2} \ \implies \ u = \frac{\sum_{k = 1}^n \frac{z_k}{|u-z_k|^2}}{\sum_{k = 1}^n\frac{1}{|u-z_k|^2}}= \sum_{k = 1}^n\underset{= \ c_k}{\underbrace{\frac{\frac{1}{|u-z_k|^2}}{\sum_{i = 1}^n\frac{1}{|u-z_i|^2}}}} z_k = \sum_{k=1}^n c_k z_k ,
        \]
        ce qui démontre la première partie du résultat, il est immédiat de vérifier que $\sum_{k=1}^nc_k = 1$, vérification laissée aux lecteurs. Ce qui achève la preuve. \\
        
        \begin{figure}[H]
        	\centering
	        \begin{tikzpicture}
	        	\draw[->] (-5, 0) -- (5, 0);
	        	\draw[->] (0, -3) -- (0, 4);
				\draw[blue][pattern={Hatch[angle=45, distance=7pt]}, pattern color = blue] (-4,0) -- (-1, 3) -- (2, 2) -- (3, 1) -- (1, -1) -- (-1, -2) -- cycle;
	        \end{tikzpicture}
	        \newline
	        $P = (X + 4)^3 (X + 1 - 3i) (X - 2 - 2i)^2 (X -3 - i) (X - 1 + i) (X + 1 + 2i)^2$ \\
	        Les racines de $P'$ sont dans le polygone bleu.
        \end{figure}
    \end{question_kholle}

    \begin{question_kholle}
        {Deux expressions du coefficient associé à un pôle simple dans une décomposition en éléments simples.}
        
        Soient $(P, Q) \in \K [X] \times \left( \K [X] \backslash \{ 0_{\K[X] }\} \right) $ tels que la fraction rationnelle $\frac{P}{Q}$ soit \\ irréductible et en prenant $\deg P < \deg Q$. En appliquant le théorème de décomposition en éléments simples, on obtient un expression de la forme : 
        \[
            \exists \ R \in \K (X) \ : \ \frac{P}{Q} = \sum_{k=1}^n \frac{a_k}{X - z_k} + R, 
        \]
        où les $z_k$ pour $k\in \lient 1 ,n \rient$ sont racines de $Q$. Ainsi, en prenant $k_0 \in \lient 1, n \rient $ tel que $z_{k_0}$ soit racine simple, 
        \[
            \frac{P(X-z_{k_0})}{Q} = a _ {k_0} + \sum_{k=0 \ \text{et} \ k \neq k_0} \frac{a_k(X-z_{k_0})}{X - z_{k_0}} + R(X - z_{k_0}),
        \] 
        une première expression se trouvera en notant $\widetilde{Q} = \displaystyle \prod_{\substack{k=1 \\ k\neq k_0}}^n(X-z_k)^{\nu_{(X-z_k)}(Q)}$, on a alors : 
        \[
            \frac{P(z_{k_0})}{\widetilde{Q}(z_{k_0})} = a_{k_0}.
        \]
        Une autre expression est possible en explicitant $\widetilde{Q}$. Pour ce faire, remarquons plutôt : 
        \[
            Q' = \sum_{k=1}^n \nu_{(X-z_k)}(Q) (X- z_k)^{\nu_{(X-z_k)}(Q) -1}\prod _{\substack{i = 1 \\ i \neq k}}^n(X-z_i)^{\nu_{(X-z_i)}(Q)},
        \]
        donc en prenant l'image par le morphisme d'évaluation en $z_{k_0}$ on obtient : 
        \[
            Q'(z_{k_0}) = \prod _{\substack{i = 1 \\ i \neq k_0}}^n(z_{k_0}-z_i)^{\nu_{(X-z_i)}(Q)},
        \]
        il s'agit exactement de $\widetilde{Q}(z_{k_0})$. Ainsi, 
        \begin{equation}
             \frac{P(z_{k_0})}{Q'(z_{k_0})} =a_{k_0},
        \end{equation}
        ce qui suffit.
    \end{question_kholle}

    \begin{question_kholle}
        []
        {Expressions des deux coefficients associés à un pôle double dans une décomposition en éléments simples.}
        Soient $(P, Q) \in \K [X] \times \left( \K [X] \backslash \{ 0_{\K[X] }\} \right) $ tels que la fraction rationnelle $\frac{P}{Q}$ soit \\ irréductible et en prenant $\deg P < \deg Q$. En appliquant le théorème de décomposition en éléments simples on obtient un expression de la forme suivante en considérant $z_{k_0}$, une racine double de $Q$ : 
        \[
            \exists \ R \in \K (X) \ : \ \frac{P}{Q} = \frac{a_1}{X - z_{k_0}}  + \frac{a_2}{(X - z_{k_0})^2} + R \quad \quad (\star)  
        \]
        puis de même, 
        \[
            \frac{P(X - z_{k_0})^2}{Q} = a_2  + \left( \frac{a_1}{X - z_{k_0}} + R \right)(X - z_{k_0})^2,
        \]
        donc en notant $\widetilde{Q} = \displaystyle \prod_{\substack{k=1 \\ k\neq k_0}}^n(X-z_k)^{\nu_{(X-z_k)}(Q)}$, on a : 
        \[
            \frac{P(z_{k_0})}{\widetilde{Q}(z_{k_0})} = a_2, 
        \]
        c'est une première expression. Pour la suivante, encore une fois, explicitons $\widetilde{Q}$. Remarquons que :
        \[
            \exists \ A \in \K [X] \ : \ \left[ Q'' = 2\prod_{\substack{k=1 \\ k \neq k_0}}^n (X-z_k)^{\nu_{(X-z_k)}(Q)} + A \right] \ \wedge \ \left[ A(z_{k_0} ) = 0 \right],
        \]
        donc, en remarquant que : 
        \[
            2\widetilde{Q}(z_{k_0}) = Q''(z_{k_0}),
        \]
        on a finalement : 
        \[
            \frac{2P(z_{k_0})}{Q''(z_{k_0})} = a_2.
        \]
        Pour récupérer $a_1$, on multiplie $(\star )$ par $(X- z_{k_0})^2$ puis on dérive : 
        \[
            \left( \frac{P  ( X - z_{k_0})^2}{Q}\right)' =  a_1 + R'(X-z_{k_0})^2 + 2R(X - z_{k_0}),
        \]
        soit, 
        \[
            \frac{((P'(X - z_{k_0})^2 + 2P(X-z_{k_0}))Q - Q'P(X-z_{k_0})^2 }{Q^2}  = a_1 + R'(X-z_{k_0})^2 + 2R(X - z_{k_0})
        \]
    \end{question_kholle}
    
\pagebreak\section{Semaine 22}

	Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$, $I$ un ensemble quelconque non vide.

	\begin{question_kholle}
		[Une famille est liée si et seulement si l'un de ses vecteurs est une combinaison linéaires d'autres vecteurs de la famille.
		\begin{equation}
			(x_i)_{i \in I} \text{ est liée}
			\iff \exists i_0 \in I : \exists (\lambda_i)_{i \in I\setminus\{i_0\}} \in \K^{\left( I \setminus \{i_0\} \right)} :
			x_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i
		\end{equation}]
		{Caractérisation d'une famille liée}

		Supposons que $(x_i)_{i \in I}$ est liée. \\
		Par définition, $\displaystyle \exists (\mu_i) \K^{(I)} :
		\left\{ \begin{array}{ccc}
			\sum_{i \in I} \mu_i x_i &= 0_E \\
			(\mu_i)_{i \in I} &\neq (0_\K)_{i \in I}
		\end{array} \right.$ \\
		Donc $\exists i_0 \in I : \mu_{i_0} \neq 0_\K$. Fixons un tel $i_0$. \\
		$\displaystyle \mu_{i_0} x_{i_0} + \sum_{i \in I \setminus \{i_0\}} \mu_i x_i = 0_E$ \\
		Or $\mu_{i_0} \neq 0$, donc $\displaystyle x_{i_0} = \sum_{i \in I \setminus \{i_0\}} \left( \mu_{i_0}^{-1} \times (-\mu_i) \right) \ldotp x_i$. \\
		En posant $\lambda_i = \mu_{i_0}^{-1} \times (-\mu_i)$, on obtient $x_{i_0} = \displaystyle \sum_{i \in I \setminus \{i_0\}} \lambda_i \ldotp x_i$.

		Supposons maintenant que $\exists i_0 \in I : \exists (\lambda_i)_{i \in I\setminus\{i_0\}} \in \K^{\left( I \setminus \{i_0\} \right)} :
		x_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i$. \\
		Alors $-x_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp x_i = 0_E$.
		Posons $\mu_{i_0} = - 1_\K$ et $\forall i \in I \!\setminus\! \{i_0\}, \mu_i = \lambda_i$.
		Ainsi, $(\mu_i)_{i \in I} \in \K^{(I)}$ et $\sum_{i \in I} \mu_i \ldotp x_i = 0_\K$. Or $\mu_{i_0} \neq 0_\K$ donc $(\mu_i)_{i \in I} \neq (0_\K)_{i \in I}$. \\
		Donc $(\mu_i)_{i \in I}$ est liée.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $\mathcal{F}$ une famille de vecteurs de $E$. Les propositions suivantes sont équivalentes :
		\begin{propositions}
			\item $\mathcal{F}$ est une base.
			\item Tout vecteur de $E$ se décompose de manière unique dans $\mathcal{F}$.
			\item $\mathcal{F}$ est génératrice minimale (au sens de l'inclusion)
			\item $\mathcal{F}$ est libre maximale (au sens de l'inclusion)
		\end{propositions}]
		{Caractérisations d'une base}

		Notons $(e_i)_{i \in I}$ la famille $\mathcal{F}$.


		$(i) \implies (ii)$ Supposons que $\mathcal{F}$ est une base de $E$. \\
		Soit $x \in E$ \fq. Montrons que $x$ s'écrit de manière unique comme une combinaison linéaire des vecteurs de $\mathcal{F}$. \\
		$\mathcal{F}$ est une base donc elle est une famille génératrice et libre de $E$. La propriété génératrice donne, par définition, l'existence d'une telle écriture tandis que la propriété libre donne l'unicité d'une telle écriture.

		$(ii) \implies (iii)$ Supposons que tout vecteur de $E$ s'écrit de manière unique comme une combinaison linéaire de vecteurs de $\mathcal{F}$. \\
		L'existence d'un telle décomposition permet d'affirmer que $\mathcal{F}$ est génératrice. \\
		Supposons que $\mathcal{F}$ ne soit pas génératrice minimale c'est-à-dire qu'il existe une famille $\mathcal{F}'$ de vecteurs de $E$ telle que $\mathcal{F}' \subsetneq \mathcal{F}$ et $\mathcal{F}'$ engendre $E$. \\
		Alors $\exists i_0 \in I : e_{i_0} \notin \mathcal{F}'$. Comme $\mathcal{F}'$ est génératrice, $\exists (\lambda_i)_{i \in I \setminus \{i_0\}} \in \K^{(I \setminus \{i_0\})} : e_{i_0} = \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp e_i$.
		Donc \begin{equation*}
			\begin{aligned}
				e_{i_0} &= 0_\K \ldotp e_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} \lambda_i \ldotp e_i \\
				e_{i_0} &= 1_\K \ldotp e_{i_0} + \sum_{\substack{i \in I \\ i \neq i_0}} 0_\K \ldotp e_i
			\end{aligned}
		\end{equation*}
		$e_{i_0}$ peut donc s'écrire de deux manières différentes au moins comme combinaison linéaire de vecteurs de $\mathcal{F}$ ce qui contredit le caractère libre de $\mathcal{F}$. \\
		Par conséquent, $\mathcal{F}$ est génératrice et minimale parmi les familles génératrices.

		$(iii) \implies (iv)$ Supposons que $\mathcal{F}$ est une famille génératrice minimale.
		Par l'absurde, supposons que $\mathcal{F}$ est liée. Alors il existe un $i_0 \in I$ tel que $e_{i_0}$ s'écrit comme une combinaison linéaire d'autres vecteurs de $\mathcal{F}$ donc $(e_i)_{i \in I \setminus \{i_0\}}$ est génératrice de $E$ . Or cette famille est strictement incluse dans $\mathcal{F}$ ce qui contredit la propriété de génératrice minimale. \\
		Donc $\mathcal{F}$ est libre. \\
		Par l'absurde, supposons que $\mathcal{F}$ n'est pas libre maximale c'est-à-dire qu'il existe une famille $\mathcal{F}'$ de vecteurs de $E$ telle que $\mathcal{F} \subsetneq \mathcal{F}'$ et $\mathcal{F}'$ est libre. \\
		Alors $\exists x \in \mathcal{F}' : x \notin \mathcal{F}$. Or $\mathcal{F}$ est génératrice d'où :
		\begin{equation*}
			\exists \bdak :
			x = \sum_{i \in I} \lambda_i \ldotp x_i
			= 0_\K \ldotp x + \sum_{i \in I} \lambda_i \ldotp x_i + \sum_{\substack{y \in \mathcal{F}' \\ y \notin \mathcal{F} \\ y \neq x}} 0_\K \ldotp y
		\end{equation*}
		Puisque $x \in \mathcal{F}'$,
		\begin{equation*}
			\exists \bdak :
			x = 1_\K \ldotp x + \sum_{i \in I} 0_\K \ldotp x_i + \sum_{\substack{y \in \mathcal{F}' \\ y \notin \mathcal{F} \\ y \neq x}} 0_\K \ldotp y
		\end{equation*}
		Donc $x$ s'écrit de deux manières différentes au moins comme combinaison linéaire de vecteurs $\mathcal{F}'$, ce qui contredit la liberté de $\mathcal{F}'$. \\
		Par conséquent, $\mathcal{F}$ est libre maximale.

		$(iv) \implies (i)$ Supposons que $\mathcal{F}$ est une famille libre maximale. \\
		Par hypothèse même, $\mathcal{F}$ est libre.
		Par l'absurde, supposons que $\mathcal{F}$ n'est pas génératrice. Alors il existe $x \in E$ tel que $x \notin \Vect \mathcal{F}$. Donc $\mathcal{F} \wedge \{x\}$ est libre et contient strictement $\mathcal{F}$, ce qui contredit la propriété de liberté maximale. \\
		Par conséquent, $\mathcal{F}$ est aussi génératrice, donc une base.

		\begin{equation*}
			\begin{matrix}
				(i) &\!\!\!\implies\!\!\!& (ii) \\
				\Uparrow & & \Downarrow \\
				(iv) &\!\!\!\impliedby\!\!\!& (iii) \\
			\end{matrix}
		\end{equation*}
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $f \in \mathcal{L}_\K(E, F)$.
		\begin{equation}
			\begin{aligned}
				\ker f &= \left\{ x \in E \;|\; f(x) = 0_F \right\} = f^{-1} (\{0_F\}) \\
				\Im f &= \left\{ y \in F \;|\; \exists x \in E : f(x) = y \right\}
			\end{aligned}
		\end{equation}
		Nous démontrerons le résultat plus général suivant :
		\begin{propositions}
			\item $f(E')$ est un \sev de $F$.
			\item $f^{-1}(F')$ est un \sev de $E$.
		\end{propositions}
		]
		{Le noyau et l'image d'une application linéaire sont des \sevs}

		$(i)$ $0_E \in E'$ et $f(0_E) = 0_F$ donc $0_F \in f(E')$ d'où $f(E') \neq \emptyset$ \\
		Soit $(\alpha, \beta, y, y') \in \K^2 \times f(E')^2$ \fqs. \\
		Par définition, $\exists (x, x') \in E'^2 : f(x) = y \wedge f(x') = y$.
		\begin{equation*}
			\begin{aligned}
				\alpha y + \beta y'
				&= \alpha f(x) + \beta f(x') \\
				&= f( \alpha x + \beta x' ) \quad \text{ car } f \in \mathcal{L}_\K(E, F) \\
				&\in f(E') \quad \text{ car } \alpha x + \beta x' \in E' \text{ puisque } E' \text{ est un \sev}
			\end{aligned}
		\end{equation*}
		Donc $f(E')$ est un \sev.

		$(ii)$ $0_F \in F'$ et $f(0_E) = 0_F$ donc $0_E \in f^{-1}(F')$ d'où $f(F') \neq \emptyset$ \\
		Soit $(\alpha, \beta, x, x') \in \K^2 \times f^{-1}(F')^2$ \fqs. \\
		Par définition, $\exists (y, y') \in F'^2 : f(x) = y \wedge f(x') = y$. \\
		Or $F'$ est \sev donc $\alpha y + \beta y' \in F'$. $f \in \mathcal{L}_\K(E, F)$ d'où $f(\alpha x + \beta x') = \alpha y + \beta y'$. Donc $\alpha x + \beta x' \in f^{-1}(F')$. \\
		Ainsi, $f^{-1}(F')$ est un \sev.

		En appliquant pour $E' = E$ et $F' = \{0_F\}$, nous obtenons que $\ker f$ et $\Im f$ sont des \sevs.
	\end{question_kholle}


	\begin{question_kholle}
		[{Soient $(E,F)$ deux $\mathbb{K}$-espaces vectoriels $f \in \mathcal{L}_\K(E, F)$, $\mathcal{F}=(x_{i})_{i\in I}$ une base de $E$.

		Alors \begin{equation}
			\text{Vect} \!\!\!\! \underbrace{ f(\mathcal{F}) }_{ \left\{ f(x_{i}) \mid i \in I \right\}  } = f(\Vect\mathcal{ F})
		\end{equation}
		}]
		{L’image par une application linéaire d’une partie génératrice engendre l’image de l’application linéaire}
		Soit $y \in \text{Vect}f(\mathcal{F})$
		Alors $\exists (\lambda_{i})_{i \in I} \in \mathbb{K}^{(I)}$ tel que $y = \sum_{i \in I} \lambda_{i}f(x_{i})$
		Mais
		\begin{align*}
			y & =  \sum_{i \in I} \lambda_{i}f(x_{i}) \\
			& = f\left( \sum _{i \in I} \lambda _{i} x_{i} \right) \implies y \in f(\text{Vect}{\mathcal{F}})
		\end{align*}


		Réciproquement soit $y \in f(\text{Vect}{\mathcal{F}})$ fq.
		$$\exists x \in \text{Vect} \mathcal{F} : f(x) = y \implies \exists (x_{i})_{i \in I} : x = \sum_{i \in I} \lambda_{i}x_{i}$$
		Donc:

		\begin{align*}
			y = f(x)  & = f\left( \sum_{i \in I} \lambda_{i} x_{i} \right) \\
			& = \sum _{i \in I} \lambda_{i} f(x_{i}) \in \text{Vect} f ( \mathcal F)
		\end{align*}
	\end{question_kholle}

	\begin{question_kholle}
		[Nous donnerons les caractérisations au fur et à mesure de la démonstration.]
		{Caractérisation inj/surj/bij d'une application linéaire par l'image d'une base de l'espace de départ.}
		Soient donc pour la suite, $f \in \mathcal{L}_\K(E,F)$, $\mathcal{B} = (e_i)_{i\in I}$ une base de $E$, $\mathcal{B}' = (e'_i)\ii$ une base de $F$,$\mathcal{F} = (x_i)_{i\in I}$ une famille libre de $E$ et $\mathcal{G} = (y_i)_{i \in I}$ une famille génératrice de $E$, ces objets servent ici de notation et seront utilisés indépendamment lors de la preuve. \\ \\
		Montrons que l'image d'une base $\mathcal{B}$ par une application injective est une famille libre $\mathcal{F}$. \\
		Supposons $f$ injective, donc pour $\bdak$,
		\[
		0_F = \sum_{i\in  I}\bda_i f(e_i) = f \left( \sum_{i\in  I} \bda_i e_i \right) \ \overset{f \text{ inj}}{\implies} \ \sum_{i\in  I} \bda_i e_i = 0_E \ \overset{\mathcal{B} \text{ base donc libre}}{\implies} \ (\bda _i)_{i\in  I} = \widetilde{0_\K},
		\]
		donc $f(\mathcal{B})= \mathcal{F}$ libre. \\
		Supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})$ soit libre, montrons qu'alors $f$ est injective. \\
		Soit $x \in \ker f$ :
		\[
		\exists \ \bdak \ : \ 0_F = f(x) = f\left( \sum\ii \bda_i e_i \right) =\sum \ii \bda _i f(e_i) \  \overset{f(\mathcal{B}) \text{ libre}}{\implies} \ (\bda _i)_{i\in  I} = \widetilde{0_\K},
		\]
		donc $x = 0_E$ donc $\ker f = \{0_E\}$ et $f$ injective. \\ \\
		Montrons que l'image d'une base $\mathcal{B}$ par une application surjective est une famille génératrice $\mathcal{G}$. \\
		Supposons $f$ surjective. Ainsi, $\Im f = F$, or $\mathcal{B}$ est une base donc est génératrice donc :
		\[
		\Vect f(\mathcal{B}) = f\left( \Vect \mathcal{B} \right) = f( E) = \Im f = F,
		\]
		donc $f(\mathcal{B}) = \mathcal{G}$ est génératrice. \\
		Supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})$ soit génératrice, montrons que $f$ est surjective. \\
		On a ainsi,
		\[
		F = \Vect f(\mathcal{B}) = f \left( \Vect \mathcal{B} \right) = f(E) = \Im f,
		\]
		donc $f$ surjective. \\ \\
		Montrons que l'image d'une base $\mathcal{B}$ par un isomorphisme est une base $\mathcal{B}'$. \\
		Supposons que $f$ soit un isomorphisme. $f$ est injective et $\mathcal{B}$ est une base donc $f(\mathcal{B})$ est libre. $f$ est surjective et $\mathcal{B}$ est une base donc $f(\mathcal{B})$ est génératrice. Ainsi, $f(\mathcal{B})=\mathcal{B}'$ est une base. \\
		Réciproquement, supposons qu'il existe $\mathcal{B}$ telle que $f(\mathcal{B})=\mathcal{B}'$ soit une base, montrons que $f$ est un isomorphisme.\\
		$\mathcal{B}'$ est une base donc est libre donc $f$ est injective. $\mathcal{B}'$ est une base donc est génératrice donc $f$ est surjective.
		Ainsi, $f$ est un isomorphisme.
	\end{question_kholle}

	\begin{question_kholle}
		[Il existe une unique application linéaire de $E$ dans $F$ qui envoie une base donnée de $E$ sur une famille de $F$ imposée. \\
		Soient $(e_i)\ii$ une base de $E$ et $(y_i)\ii$ une famille de $F$.
		\begin{equation}
			\exists ! f \in \mathcal{L}_\K(E, F) : \forall i \in I, f(e_i) = y_i
		\end{equation}
		Nous pouvons expliciter une telle application :
		\begin{equation}
			f \left| \;\; \begin{matrix}
				E & \rightarrow & F \\
				\displaystyle \sum\ii \lambda_i \ldotp e_i & \mapsto & \displaystyle \sum\ii \lambda_i \ldotp y_i
			\end{matrix} \right.
		\end{equation}]
		{Caractérisation d'une application linéaire par l'image d'une base}
		~\newline
		\underline{Analyse} Supposons qu'il existe $f \in \mathcal{L}_\K(E, F)$ \tq $\forall i \in I, f(e_i) = y_i$. \\
		Tout vecteur de $E$ peut se décomposer de manière unique dans la base $(e_i)\ii$, ce qui détermine son image. Ainsi, $f$ est unique.
		\newline

		\noindent \underline{Synthèse} Posons une telle application $f$. \\
		\begin{itemize}
			\item $(e_i)\ii$ est une base donc $(\lambda_i)\ii$ est presque nulle et unique donc $\sum\ii \lambda_i \ldotp y_i$ existe et unique.
			Ainsi, $f$ est bien définie.
			\item Soient $(\alpha, \beta, x, x') \in \K^2 \times E^2$ \fqs. Notons $(\lambda_i)\ii$ et $(\lambda'_i)\ii$ les coordonnées de $x$ et $x'$ dans $(e_i)\ii$.
			\begin{equation*}
				\begin{aligned}
					f( \alpha x + \beta x' )
					&= f\left( \alpha \sum\ii \lambda_i \ldotp e_i + \beta \sum\ii \lambda'_i \ldotp e_i \right) \\
					&= f\left( \sum\ii \left( \alpha \lambda_i + \beta \lambda'_i \right) \ldotp e_i \right) \\
					&= \sum\ii \left( \alpha \lambda_i + \beta \lambda'_i \right) \ldotp y_i \quad \text{ par définiton de } f \\
					&= \alpha \sum\ii \lambda_i y_i + \beta \sum\ii \lambda'_i y_i \\
					&= \alpha f(x) + \beta f(x')
				\end{aligned}
			\end{equation*}
			Donc $f$ est linéaire.
			\item Soit $j \in I$ \fq.
			\begin{equation*}
				\begin{aligned}
					f(e_j)
					&= f \left(\sum\ii \delta_{i,j} \ldotp e_i \right) \\
					&= \sum\ii \delta_{i,j} \ldotp y_i \\
					&= y_j
				\end{aligned}
			\end{equation*}
		\end{itemize}
	\end{question_kholle}

\pagebreak\section{Semaine 23}

	Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$, $I$ un ensemble quelconque non vide.

	\begin{question_kholle}
		{L'ensemble des automorphisme d'un espace vectoriel muni de la loi de composition forme un groupe}

		Montrons que $(\mathcal{GL}_\K(E), \circ)$ est un sous-groupe de $(\mathcal{S}(E), \circ)$.
		\begin{itemize}
			\item $\mathcal{GL}_\K(E) \subset \mathcal{S}(E)$ et $(\mathcal{S}(E), \circ)$ est bien un groupe.
			\item $\mathcal{GL}_\K(E) \neq \emptyset$ puisque $Id_E \in \mathcal{GL}_\K$.
			\item Soit $(f, g) \in \mathcal{GL}(E)$. Montrons que $f \circ g^{-1} \in \mathcal{GL}(E)$. \\
			Soit $(\alpha, \beta, x, y) \in \K^2 \times E^2$ \fqs. \\
			\begin{equation*}
				\begin{aligned}
					\left(f \circ g^{-1}\right) \left(\alpha x + \beta y\right)
					&= f \left( g^{-1} \left(\alpha x + \beta y\right) \right) \\
					&= f \left( g^{-1} \left(\alpha g^{-1}(g(x)) + \beta g^{-1}(g(y))\right) \right) \\
					&= f \left( g^{-1} \left( \alpha g\left(g^{-1}(x)\right) + \beta g\left(g^{-1}(y)\right) \right) \right) \\
					&= f \left( g^{-1} \left( g \left( \alpha g^{-1}(x) + \beta g^{-1}(y) \right) \right) \right) \quad \text{car } g \text{ est linéaire} \\
					&= f \left( \alpha g^{-1}(x) + \beta g^{-1}(y) \right) \\
					&= \alpha f \left( g^{-1}(x) \right) + \beta f \left( g^{-1}(y) \right) \\
					&= \alpha \left(f \circ g^{-1}\right) (x) + \beta \left(f \circ g^{-1}\right) (y)
				\end{aligned}
			\end{equation*}
		\end{itemize}
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $\ffinie{E}{p}{E}$ $p$ \sev de E avec $p \in \N^*$ \fq. \\
		Par définition, cette famille est en somme directe si tout vecteur de $E_1 + E_2 + \ldots + E_p$ peut s'écrire comme une somme unique d'élément de $E_1 \times E_2 \times \ldots \times E_p$. Formellement :
		\begin{equation}
			\forall x \in \sum_{i=1}^{p} E_i,
			\exists ! x \in \! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i :
			x = \sum_{i=1}^{p} x_i
		\end{equation}
		Nous allons démontrer que $E_1, E_2, \ldots$ et $E_p$ sont en somme directe \ssi
		{\begin{equation}
			\forall x \in \! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i,
			\left( \sum_{i=1}^{p} x_i = 0_E \implies \forall i \in [\![1;p]\!], x_i = 0_E \right)
		\end{equation}}]
		{Caractérisation de la somme directe de $p$ \sevs}

		Supposons que $E_1, E_2, \ldots E_p$ sont en somme directe. \\
		Soient $x \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ \fqs \tqs $x_1 + x_2 + \ldots + x_p = 0_E$. \\
		Or $0_E = \underbrace{0_E}_{\in E_1} + \underbrace{0_E}_{\in E_2} + \ldots + \underbrace{0_E}_{\in E_p}$.
		Par unicité de l'écriture de x comme somme d'éléments de $\overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$, $\forall i \in [\![]1;p\!], x_i = 0_E$.

		Supposons maintenant l'équation de la caractérisation. \\
		Soit $x \in \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ \tq $x$ puisse s'écrire comme somme de $x' \!\! \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$ et somme de $x'' \!\! \in \!\! \overset{p}{\underset{i=1}{\mathlarger{\mathlarger{\times}}}} E_i$. Montrons que $x' = x''$.
		\begin{equation*}
			\sum_{i=1}^{p} x'_i = x = \sum_{i=1}^{p} x''_i
		\end{equation*}
		Donc
		\begin{equation*}
			\sum_{i=1}^{p} \left( x''_i -x''_i \right) = 0_E
		\end{equation*}
		D'après l'équation de la caractérisation, $\forall i \in [\![1;p]\!], x'_i - x''_i = 0_E$. \\
		Donc $\forall i \in [\![1;p]\!], x'_i = x''_i$
	\end{question_kholle}
\pagebreak\section{Semaine 24}
	
	Pour cette semaine, \K désigne un corps commutatif, $E$ et $F$ des \K\!\!-espaces vectoriels, $E'$ et $F'$ des sous-espaces vectoriels respectivement de $E$ et de $F$.
	
	Nous rappelons que $\dim \{0_E\} = 0$ et que $\{0_E\} = \Vect \emptyset$.
	
	\begin{question_kholle}
		[Pour tout \sev de $E$, il existe un \sev complémentaire.]
		{Existence d'un supplémentaire en dimension finie}
		
		~\newline
		\textit{Théorème de la base incomplète} (admis ici mais démontré dans le cours) : pour toute famille libre de E, nous pouvons y adjoindre une partie d'une famille quelconque génératrice de $E$ (généralement une base, la base canonique si elle a un sens) pour en faire une base de $E$. \\
		
		Posons $n = \dim E$ et $p = \dim E'$. Ainsi, il existe $(e_1, \ldots, e_p)$ base de $E'$.
		Appliquons le théorème de la base incomplète pour cette famille.
		Il existe $(e_{p+1}, \ldots, e_n)$ $n-p$ vecteurs de E \tq $(e_1, \ldots, e_n)$ est un base de $E$.
		Posons $E'' = \Vect \{ e_{p+1}, \ldots, e_n \}$ et vérifions qu'il est supplémentaire à $E'$.
		
		Par définition de \Vect\!\!, $E''$ est un \sev.
		Trivialement, $E' + E'' = E$.
		$\{0_E\} \subset E' \cap E''$ car $E'$ et $E''$ sont deux \sevs.
		Soit $x \in E' \cap E''$.
		$X \in E' \implies \exists (\lambda_1, \ldots, \lambda_p) \in \K^p : x = \sum_{i=1}^{p} \lambda_i e_i$ et 
		$X \in E'' \implies \exists (\lambda_{p+1}, \ldots, \lambda_n) \in \K^{n-p} : x = \sum_{i=p+1}^{n} \lambda_i e_i$.
		Par différence, $\sum_{i=1}^{p} \lambda_i e_i + \sum_{i=p+1}^{n} \left(-\lambda_i\right) e_i = 0_E$.
		Or $\famille{e}{\lient 1 ; n \rient}$ est une base de $E$ donc $\forall i \in \lient 1 ; p \rient, \lambda_i = 0_\K$.
		donc $x = 0_E$.
		Ainsi, $E' \cap E'' = \{0_E\}$.
	\end{question_kholle}
	
	\begin{question_kholle}
		[$\mathcal{L}_\K(E,F)$ est dimension finie et
		\begin{equation}
			\dim \mathcal{L}_\K(E,F) = \dim E \times \dim F
		\end{equation}]
		{Dimension de $\mathcal{L}_\K(E,F)$}
		
		Notons $n = \dim E$ et $\famille{e}{\lient 1 , n \rient}$ une base de $E$. Considérons
		\begin{equation*}
			\varphi
			\left| \begin{array}{ccl}
				\mathcal{L}_\K(E,F) &\to &F^n \\
				f &\mapsto &\Big( f(e_i) \Big)_{i \in \lient 1 , n \rient}
			\end{array} \right.
		\end{equation*}
		
		$\varphi$ est linéaire et, d'après le théorème de création des applications linéaires, bijective.
		Ainsi, $\mathcal{L}_\K(E,F)$ et $F^n$ sont isomorphes. $F^n$ est de dimension finie, ce qui conclut.
	\end{question_kholle}
	
	\begin{question_kholle}
		[Supposons $E$ de dimension finie. \\
		Soient $E_1$ et $E_2$ deux \sevs. Alors $E_1 + E_2$ est de dimension finie et
		\begin{equation}
			\dim E_1 + E_2 = \dim E_1 + \dim E_2 - \dim E_1 \cap E_2
		\end{equation}]
		{Formule de Grassman}
		
		Commençons par prouver une version simplifier de la somme directe. Supposons que $E_1$ et $E_2$ sont en somme directe.
		
		Fixons $\mathcal{B}_1$ et $\mathcal{B}_2$ deux bases de $E_1$ et $E_2$.
		Alors $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ engendre $E_1 + E_2$. Or $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est finie donc $E_1 + E_2$ est de dimension finie.
		
		Posons $n = \dim E_1$ et $p = \dim E_2$. Notons $\famille{e}{\lient 1 ; n \rient}$ la base $\mathcal{B}_1$ et $\famille{f}{\lient 1 ; n \rient}$ la base $\mathcal{B}_2$.
		Soient $\lambda_1, \ldots, \lambda_n, \mu_1, \ldots, \mu_p) \in \K^{n+p}$ \fqs \ \tqs \ $\displaystyle \sum_{i=1}^{n} \lambda_i e_i + \sum_{i=1}^{p} \mu_i f_i = 0_E$.
		Alors $\sum_{i=1}^{n} \lambda_i e_i = \sum_{i=1}^{p} (-\mu_i) f_i$.
		Or $\sum_{i=1}^{n} \lambda_i e_i \in E_1$ et $\sum_{i=1}^{n} (-\mu_i) e_i \in E_2$ donc $\sum_{i=1}^{n} \lambda_i e_i \in E_1 \cap E_2 = \{0_E\}$.
		Donc $\lambda = \widetilde{0}$. De même, $\mu = \widetilde{0}$.
		Donc $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est libre.
		
		Ainsi, $\left(\mathcal{B}_1, \mathcal{B}_2\right)$ est une base de $E_1 \oplus E_2$.
		Donc $ \dim E_1 \oplus E_2 = |(\mathcal{B}_1, \mathcal{B}_2)| = |\mathcal{B}_1| + |\mathcal{B}_2| = \dim E_1 + \dim E_2$. \\
		
		Enlevons l'hypothèse que $E_1$ et $E_2$ sont en somme directe.
		$E_1 \cap E_2$ est un \sev de $E_2$. Comme $E_2$ et un \K\!\!-espace vectoriel de dimension finie, il existe $E_2'$ \sev de $E_2$ \tq $E_2 = (E1 \cap E_2) \oplus E_2'$.
		
		Montrons que $E_1 + E_2 = E_1 \oplus E_2'$.
		\begin{equation*}
			\begin{aligned}
				E_1 \cap E_2' &= E_1 \cap \left( E_2' \cap E_2 \right) \text{ car } E_2' \subset E_2 \\
				&= \left( E_1 \cap E_2 \right) \cap E_2' \text{ car } \cap \text{ est associative et commutative} \\
				&= {0_E} \text{ car $E_1$ et $E_2$ sont en somme directe et $E_2'$ sev}
			\end{aligned}
		\end{equation*}
		Donc $E_1$ et $E_2'$ sont en somme directe.
		
		$E_2' \subset E_2$ donc $E_1 + E_2' \subset E_1 + E_2$.
		Soit $x \in E_1 + E_2$.
		Alors $\exists (x_1, x_2) \in E_1 \times E_2 : x = x_1 + x_2$. \\
		Or $E_2 = \left( E_1 \cap E_2 \right) \oplus E_2'$ donc $\exists (x_{21}, x_2') \times E_2' : x_2 = x_{21} + x_2'$.
		D'où $x = x_1 + x_{21} + x_2'$. Or $x_1 + x_{21} \in E_1$ et $x_2' \in E_2$ donc $x \in E_1 + E_2'$.
		
		Ainsi, $E_1$ et $E_2'$ étant des \sev de dimension finie, $\dim E_1 \oplus E_2' = \dim E_1 + \dim E_2'$.
		De plus, $\dim E_2 = \dim (E_1 \cap E_2) \oplus E_2' = \dim E_1 \cap E_2 + \dim E_2'$.
		Donc $\dim E_1 + E_2 = \dim E_1 + \dim E_2 - \dim E_1 \cap E_2$.
	\end{question_kholle}

	\begin{question_kholle}
		[Soit $f \in \mathcal{L}_{\mathbb{K}}(E, F)$.
		\begin{propositions}
			\item Si $E$ est de dimension finie 
			\begin{equation}
				f \text{ injective } \iff \mathrm{rg} f = \dim E
			\end{equation}
			\item Si $F$ est de dimension finie
			\begin{equation}
				f \text{ surjective } \iff \mathrm{rg} f = \dim F
			\end{equation}
			\item Si $E$ et $F$ sont de même dimension finie $$f \text{ bijective } \iff f \text{ injective } \iff f \text{ sujective }$$
			C'est \textit{l'accident de la dimension finie} !
		\end{propositions}]
		{Caractérisation injectivité/bijectivité/surjectivité par le rang}
		
		~
		\begin{propositions}
			\item Supposons $E$ de dimension finie, fixons $(e_{1}, \dots, e_{n})$ une base de $E$ (avec $n = \dim E$)
			Supposons $f$ injective :
			$$
			\mathrm{rg} f = \dim \mathrm{Im} f = \dim \text{Vect} \left\{ f(e_{1}) \dots f(e_{n}) \right\}  
			$$
			
			Donc $(f(e_{1}), \dots f(e_{n}))$ est génératrice.
			$(f(e_{1}), \dots f(e_{n}))$ est de plus libre car $f$ est injective.
			Donc c'est une base, donc
			$$
			\dim \text{Vect} \left\{ f(e_{1}) \dots f(e_{n}) \right\}  =n = \dim E
			$$
			donc $\mathrm{rg} f = \dim E$.
			Réciproquement, supposons que $\mathrm{rg} f = \dim E = n$.
			Alors $$n = \mathrm{rg} f = \dim \text{Vect} \left\{ f(e_{1}),\dots,f(e_{n}) \right\}$$
			Donc $(f(e_{1}), \dots f(e_{n}))$ est génératrice de cardinal $n$, égal à la dimension du sous-espace vectoriel engendré. C'est donc une base du sous-espace vectoriel engendré.
			Donc $(f(e_{1}),\dots , f(e_{n}))$ est libre, donc $f$ est injective.
			
			\item Supposons $F$ de dimension finie
			$$
			f \text{ surjective } \iff \mathrm{Im} f = F \iff \dim \mathrm{Im} f = \dim F
			$$
			
			\item Supposons $E$ et $F$ de même dimension finie
			$$
			f \text{ injective } \iff \mathrm{rg} f = \dim E \iff \mathrm{rg} f = \dim F \iff f \text{ surjective}
			$$
			D'où la bijectivité.
		\end{propositions}
	\end{question_kholle}
	
	\begin{question_kholle}
		[Si $E$ est de dimension finie alors pour toute $f \in \mathcal{L}_\K(E, F)$ application linéaire,
		\begin{equation}
			\dim E = \rg f + \dim \ker f
		\end{equation}]
		{Théorème du rang}
		
		Démontrons d'abord le lemme suivant.
		Soient $f \in \mathcal{L}_\K(E, F)$ et $H$ un supplémentaire de $\ker f$ dans $E$.
		Alors $f_{|H}^{|\Im f}$ est un isomorphisme de $H$ sur $\Im f$.
		
		Notons $\hat{f}$ un telle restriction et corestriction. Cette application est bien définie (car $f(H) \subset \Im f$) et $\hat{f} \in \mathcal{L}_\K(H, \Im f)$.
		
		Calculons son noyau. $\ker \hat{f} = \{ x \in H \;|\; \hat{f}(x) = 0_E \} = \{ x \in H | x \in \ker f \} = H \cap \ker f = \{0_E\}$  car $H$ et $\ker f$ sont complémentaire.
		Donc $\hat{f}$ est injective.
		
		Soit $y \in \Im f$. D'où $\exists x \in E: y = f(x)$.
		Décomposons $x$ dans $E = H \oplus \ker f$, $\exists (x_H, x_k) \in H \times \ker f : x = x_H + x_k$.
		Ainsi, $y = f(x) = f(x_H) + f(x_k)= f(x_H)$ car $x_k \in \ker f$.
		Donc $y$ admet un antécédent par $\hat{f}$ (qui est $x_H$).
		Donc $\hat{f}$ est surjective.
		
		Donc $f_{|H}^{|\Im f}$ est un isomorphisme de $H$ sur $\Im f$.
		\newline
		
		Supposons maintenant que $E$ est de dimension finie.
		Soit $f \in \mathcal{L}_\K(E, F)$.
		D'après le théorème d'existence d'un supplémentaire en dimension finie, $\ker f$, étant un \sev de $E$, admet un supplémentaire $H$ c'est-à-dire $E = H  \oplus \ker f$.
		En prenant la dimension sur cette égalité, $\dim E = \dim \ker f + \dim H$.
		D'après le lemme précédent, $\dim H = \dim \Im f = \rg f$.
		D'où $\dim E = \rg f + \dim \ker f$.
	\end{question_kholle}
	
	\begin{question_kholle}
		[Soit $G$ un \K-espace vectoriel et $(u,v) \in \mathcal{L}_\K(E, F) \times \mathcal{L}_\K(F, G)$. Si $E$ et $F$ sont de dimension finie alors
		\begin{equation}
			\rg u = \rg v \circ u + \dim \ker v \cap \Im u
		\end{equation}]
		{Rang d'une composition d'applications linéaires}
		
		Considérons que $E$ et $F$ sont de dimension finie.
		Soient de tels objets.
		Appliquons le théorème du rang à $v_{|\Im u}$ ce qui est autorisé puisque $v_{|\Im u}$ est une application linéaire et $\Im u$ est un \ev de dimension finie (car sev de $F$).
		\begin{equation*}
			\dim \Im u = \rg v_{|\Im u} + \dim \ker v_{|\Im u}
		\end{equation*}
		Ainsi, $\ker v_{|\Im u} = \left\{ y \in \Im u \;|\; v(y) = 0_G \right\} = \left\{ y \in \Im u \;|\; y \in \ker v \right\} = \Im u \cap \ker v$
		et $\Im v_{|\Im u} = v(Im u) = \Im v \circ u$ (cette égalité est vraie pour deux fonctions de $E$ dans $F$ et de $F$ dans $G$ quelconques, pas forcément linéaires).
		Ce qui conclut.
	\end{question_kholle}
	
	\begin{question_kholle}
		[Soit $H$ un \sev de $E$.
		Les conditions suivantes sont équivalentes :
		\begin{propositions}
			\item $H$ est un hyperplan de $E$ :
			$\exists \varphi \in E^* : H = \ker \varphi$
			\item $H$ admet une droite vectorielle comme supplémentaire :
			$\exists a \in E \setminus \{0_E\} : H \oplus \Vect{\{a\}} = E$
		\end{propositions}]
		{Caractérisation des hyperplans}
		
		$(i) \implies (ii)$ Supposons que $H$ est un hyperplan de $E$.
		Appliquons la définition de l'hyperplan, $\exists \varphi \in E^* : H = \ker \varphi$.
		Par l'absurde, supposons que $E \setminus H = \emptyset$. Or $H \subset E$ donc $E = H$. Donc $\varphi = 0_{E^*}$ ce qui est une contradiction.

		Ainsi fixons $a \in E \setminus H$ quelconque.
		Montrons que $E = H \oplus \Vect{\{a\}}$.
		Trivialement, $\{0_E\} \subset H \cap \Vect{\{a\}}$.
		Soit $x \in H \cap \Vect{\{a\}}$.
		$x \in \Vect{\{a\}}$ donc $\exists \lambda \in  \K : x = \lambda$. De plus, $x \in H = \ker \varphi$ donc $0_\K = \varphi(x) = \lambda \varphi(a)$.
		Si $\lambda \neq 0_\K$, alors $a \in \ker \varphi$ ce qui est impossible car $a \notin H$.
		Donc $\lambda = 0_\K$, d'où $x = 0_E$.
		Ainsi, $H \cap \Vect{\{a\}} = \{0_E\}$. $H$ et $\Vect{\{a\}}$ sont en somme directe.
		
		Trivialement, $H + \Vect{\{a\}} \subset E$.
		Soit $x \in E$ \fq.
		$a \notin H$ donc $\varphi(a) \neq 0_\K$. $\varphi(a)$ est inversible dans \K d'où :
		\begin{equation*}
			\varphi(x)
			= \frac{\varphi(x)}{\varphi(a)} \cdot \varphi(a)
			= \varphi\left( \frac{\varphi(x)}{\varphi(a)} \times a \right)
		\end{equation*}
		Donc $x - \frac{\varphi(x)}{\varphi(a)} \!\cdot\! a \in H$. D'où
		\begin{equation*}
			x =
			\underbrace{x - \frac{\varphi(x)}{\varphi(a)} \!\cdot\! a}_{\in H}
			+ \underbrace{\frac{\varphi(x)}{\varphi(a)} \!\cdot\! a}_{\in \Vect{\{a\}}}
		\end{equation*}
		Ainsi, $E = H + \Vect{\{a\}}$.
		
		$(ii) \implies (i)$ Supposons maintenant que $H$ soit un sous-espace vectoriel tel que $\exists a \in E \setminus \{0_E\} : E = H \oplus \Vect{\{a\}}$.
		Posons $\displaystyle \varphi : \begin{matrix}
			E &=& H \oplus \Vect{\{a\}} &\rightarrow& \K \\
			x &=& h_x + \lambda_x \cdot a &\mapsto& \lambda_x
		\end{matrix}$.
		Montrons que $\varphi$ est une forme linéaire non triviale dont $H$ est le noyau.
		\
		
		$\varphi$ est
		bien définie (car $h_x$ et $\lambda_x$ sont uniques),
		linéaire,
		à valeur dans le corps de base \K
		donc $\varphi$ est un forme linéaire.
		$\varphi \neq 0_{E^*}$ car $\varphi(a) = 1_\K \neq 0_\K$.
		Soit $x \in E$ \fq. Alors $\exists (h_x, \lambda_x) \in H \times \K : x = h_x + \lambda_x \cdot a$.
		\begin{equation*}
			x \in \ker \varphi
			\iff \varphi(x) = 0_\K
			\iff \lambda_x = 0_\K
			\iff x \in H
		\end{equation*}
		donc $\ker \varphi = H$.
		Donc $H$ est un hyperplan de $E$.
		\bigbreak
		
		\noindent Si $E$ est de dimension finie, alors les deux conditions sont équivalentes à
		\begin{enumerate}[label=$(\roman*)$, leftmargin=1.5cm]
			\setcounter{enumi}{2}
			\item $H$ est de codimension 1 c'est-à-dire de dimension $n - 1$.
		\end{enumerate}
		$(ii) \implies (iii)$ Il faut prendre la dimension de l'égalité $H \oplus \Vect{\{a\}}$. \\
		$(iii) \implies (ii)$ Supposons que $\dim H = n - 1$.
		Comme $E$ est de dimension finie, $H$ admet un supplémentaire $I$ dans $E$ : $H \oplus I = E$.
		En prenant la dimension, $\dim I = 1$. Donc $I$ est une droite vectorielle.
		D'où $\exists a \in E : I = \Vect{\{a\}}$.
		$a \notin H$ car sinon $I \subset H$ ce qui contredit $I \cap H = \{0_E\}$ ($I$ et $H$ sont en somme directe).
	\end{question_kholle}
	
	\begin{question_kholle}
		[\textit{Lemme fondamental dans l'étude des formes linéaires} \ \
		Soit $\varphi \in E^* \setminus \{0_{E^*}\}$. \\
		Tout vecteur de $E$ n'appartenant pas au noyau de $\varphi$ engendre une droite qui est supplémentaire au noyau de $\varphi$ dans $E$.
		\begin{equation}
			\forall a \in E \setminus \ker \varphi, \
			E = \ker \varphi \oplus \Vect{\{a\}}
		\end{equation}
		
		Deux formes linéaires non nulles $\varphi$ et $\psi$ ont le même noyau si est seulement si elles sont proportionnelles ce qui revient à dire que la famille $\left(\varphi,\psi\right)$ est liée.
		\begin{equation}
			\forall \left(\varphi,\psi\right) \in \left( E^* \setminus \{0_{E^*}\} \right) \!^2, \
			\ker \varphi = \ker \psi \iff \exists \lambda \in \K^* : \varphi = \lambda \cdot \psi
		\end{equation}]
		{Proportionnalité des formes linéaires ayant le même noyau}
		
		Commençons par prouver le lemme.
		Soit $a \in E \setminus \ker \varphi$. \\
		Soit $x \in E$ \fq.
		Exhibons la décomposition unique de $x$ dans $\ker \varphi + \Vect{\{a\}}$.
		
		\textit{Analyse} Supposons qu'il existe $(x_k, \lambda) \in \ker \varphi \times \K$ \tq $x = x_k + \lambda a$.
		Puisque $x_k \in \ker \varphi$, $\varphi(x) = \lambda \cdot \varphi(a)$. Or $\varphi(a) \neq 0_\K$ (car $a \notin \ker \varphi$) donc $\varphi(a)$ est inversible dans \K.
		D'où $\lambda = \nicefrac{\varphi(x)}{\varphi(a)}$ et $x_k = x - \nicefrac{\varphi(x)}{\varphi(a)} \cdot a$.
		
		Ainsi, sous réserve d'existence, $\lambda$ et $x_k$ sont uniques.
		
		\textit{Synthèse} Posons $\displaystyle \left\{ \begin{matrix}
			\lambda &=& \frac{\varphi(x)}{\varphi(a)} \\
			x_k &=& x - \frac{\varphi(x)}{\varphi(a)} a
		\end{matrix} \right.$
		Nous avons bien
		$x = x_k + \lambda \cdot a$,
		$\lambda \cdot a \in \Vect{\{a\}}$ (car $\lambda \in \K$)
		et $x_k \in \ker \varphi$ (car $\varphi(x_k) = \varphi(x) - \varphi\left( \frac{\varphi(x)}{\varphi(a)} a \right) = \varphi(x) - \frac{\varphi(x)}{\varphi(a)} \varphi(a) = 0_\K$).
		Ainsi $E = \ker \varphi \oplus \Vect{\{a\}}$.
		\newline \newline
		
		\noindent Soient $\left(\varphi,\psi\right) \in \left( E^* \setminus \{0_{E^*}\} \right) ^2$ \fqs.
		
		\textit{Sens direct
		} Supposons que $\ker \varphi = \ker \psi$.
		$\varphi \neq 0_{E^*}$ donc $\ker \varphi \neq E$ donc $\exists a \in E : a \notin \ker \varphi$. Appliquons la lemme ci-dessus :
		\begin{equation*}
			\begin{array}{ccccccccc}
				& & E &=& \substack{\ker \varphi \\ \shortparallel \\ \ker \psi} &\oplus& \Vect{\{a\}} & \rightarrow & \K \\
				& \varphi : & x &=& \left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) &+& \frac{\varphi(x)}{\varphi(a)} \cdot a & \mapsto & \varphi(x) \\
				& \psi : & x &=& \left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) &+& \frac{\varphi(x)}{\varphi(a)} \cdot a & \mapsto & \psi(x)
			\end{array}
		\end{equation*}
		Or $\left( x - \frac{\varphi(x)}{\varphi(a)} \cdot a \right) \in \ker \psi$ donc $\psi(x) = \frac{\psi(a)}{\varphi(a)} \varphi(x)$.
		Ainsi, $\psi = \frac{\psi(a)}{\varphi(a)} \varphi$. Donc $\varphi$ et $\psi$ sont proportionnelles.
		
		\textit{Sens réciproque} Supposons que $\varphi$ et $\psi$ sont proportionnelles. Alors $\exists \lambda \in \K^* : \varphi = \lambda \psi$.
		$\varphi = \lambda \psi \implies \ker \psi \subset \ker \varphi$ et
		$\psi = \lambda^{-1} \varphi \implies \ker \varphi \subset \ker \psi$.
		Ce qui donne l'égalité.
	\end{question_kholle}
	
	\begin{question_kholle}
		[\indent Soit $\varphi \in E^{*}$ une forme linéaire \emph{non nulle}. Soit $F$ un sous-espace vectoriel de $E$ de dimension finie $p \in \mathbb{N}$, alors
		\begin{equation}
			\dim_{\mathbb{K}}F \cap \ker \varphi =
			\left\{ \begin{array}{ll}
				p  & \text{ si } F \subset \ker \varphi \\
				p-1  & \text{ sinon}
			\end{array}\right.
		\end{equation}
		En particulier, on a toujours $\dim_{\mathbb{K}}F \cap \ker \varphi \geqslant p-1$
		\newline
		
		Supposons que  $E$ un \ev de dimension finie $n \in \mathbb{N}^{*}$.
		Soient $m \in \mathbb{N}^{*}$ et $(H_{i})_{n \in [ \! [ 1,m ] \!]}$, $m$ hyperplans de $E$.
		Alors
		\begin{equation}
			\dim_{\mathbb{K}} \bigcap_{i=1}^{m}H_{i} \geqslant n-m
		\end{equation}]
		{Intersection d'hyperplans}
		% {Lemme sur les formes linéaires non nulles} ???
		
		Si $F \subset \ker\varphi$, $F \cap \ker \varphi = F$ donc $\dim F \cap \ker \varphi = p$
		
		Sinon, il existe $a \in F$ tel que $a \not\in \ker \varphi$. Ainsi,
		$$
		\text{Vect}\left\{ a \right\}  \oplus \ker \varphi = E
		$$
		Montrons alors que $F = \text{Vect} \left\{ a \right\} \oplus (F \cap \ker \varphi)$.
		$$\text{Vect} \left\{ a \right\} \cap (F \cap \ker \varphi) = \underbrace{ \text{Vect} \left\{ a \right\} \cap F }_{ =\text{Vect}\left\{ a \right\}  } \cap \ker \varphi = \text{Vect}\left\{ a \right\}  \cap \ker \varphi = \left\{ 0_{E} \right\} $$ car les deux espaces sont supplémentaires donc en somme directe.
		
		Par double inclusion, montrons que $\text{Vect} \left\{ a \right\} + (F \cap \ker \varphi) = F$.
		Pour l'inclusion directe, remarquons que $a \in F$ donc $\text{Vect}\left\{ a \right\} \subset F$ or $F \cap \ker \varphi \subset F$ donc leur somme est bien incluse $\text{Vect} \left\{ a \right\} + (F \cap \ker \varphi) \subset F$.
		Réciproquement, soit $x \in F$ \fq.
		Puisque $\text{Vect} \left\{ a \right\} \oplus \ker \varphi = E$
		$$
		\exists (\lambda, x_{K}) \in \mathbb{K} \times \ker \varphi : x = \lambda .a+x_{K}
		$$
		De plus, $x_{K} = x - \lambda.a \in F$ car $(a, x) \in F^{2}$ donc
		$$
		x = \underbrace{ \lambda . a }_{ \in \text{Vect} \left\{ a \right\}  } + \underbrace{ x_{K} }_{ \in F \cap \ker \varphi } \in \text{Vect} \left\{ a \right\} + (F \cap \ker \varphi)
		$$
		D'où l'inclusion réciproque.

		Donc $F = \text{Vect} \left\{ a \right\} \oplus (F \cap \ker \varphi)$.
		En passant à la dimension :
		$$
		\underbrace{ \dim F }_{= p } = \underbrace{ \dim \text{Vect} \left\{ a \right\} }_{ =1 } + \dim (F \cap \ker \varphi)
		$$
		Donc $\dim (F \cap \ker \varphi) = p - 1$.
		\newline \newline
		
		Considérons la propriété $\mathcal{P}(\cdot)$ définie pour tout $m \in \mathbb{N}^{*}$ par :
		$$
		\mathcal{P}(m) : \text{\textquotedblleft} \text{ pour tous } H_{1},\dots, H_{m} \text{ hyperplans de } E, \dim_{\mathbb{K}} \bigcap_{i=1}^{m}H_{i} \geqslant n-m \text{\textquotedblright}
		$$
		Soit $H_{1}$ un hyperplan de $E$ fixé quelconque. D'après la caractérisation des hyperplans en dimension finie, 
		$$
		\dim_{\mathbb{K}} \bigcap_{i=1}^{1}H_{i} = \dim_{\mathbb{K}}H_{1}= n-1 \geqslant n-1
		$$
		Donc $\mathcal{P}(1)$ est vraie.
		
		Soit $m \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{P}(m)$ est vraie.
		Soient $H_{1},\dots,H_{m}$ et $H_{m+1}$ $m+1$ hyperplans de $E$.
		D'après la définition d'un hyperplan, il existe $\varphi \in E ^{*}$ non nulle telle que $H_{m+1} = \ker \varphi$.
		
		Appliquons donc le lemme précédent pour $F \leftarrow \bigcap_{i=1}^{m}H_{i}$ (autorisé car c'est un sous espace de l'espace $E$, qui est de dimension finie, donc ses sous espaces les sont aussi) et $\varphi \leftarrow \varphi$ (autorisé car c'est une forme linéaire non nulle) :
		
		$$
		\dim_{\mathbb{K}} \underbrace{ \left( \bigcap_{i=1}^{m}H_{i} \right) \cap \ker \varphi  }_{ =\left( \bigcap_{i=1}^{m}H_{i}  \right)\cap H_{m+1} }\geqslant \dim_{\mathbb{K}} \left( \bigcap_{i=1}^{m}H_{i} \right) - 1 \underbrace{ \geqslant n - m - 1 }_{ \text{ en appliquant } \mathcal{P}(m) \text{ pour } H_{1},\dots,H_{m} }
		$$
		
		Donc par associativité de l'intersection, $\dim_{\mathbb{K}} \bigcap_{i=1}^{m+1}H_{i} \geqslant n - (m+1)$.
		Donc $\mathcal{P}(m+1)$ est vraie.
	\end{question_kholle}

\pagebreak\section{Semaine 25}

\begin{question_kholle}
	[Soit $A \in \mathcal{M}_{n}(\mathbb{K})$
	\begin{itemize}
		\item S'il existe $B \in \mathcal{M}_{n}(\mathbb{K}):A\times B = I_{n}$, alors $A\in GL_{n}(\mathbb{K})$ et $A^{-1}=B$
		\item S'il existe $B \in \mathcal{M}_{n}(\mathbb{K}):B \times A = I_{n}$, alors $A\in GL_{n}(\mathbb{K})$ et $A^{-1}=B$
	\end{itemize}
	]
	{S'il existe un inverse à droite (ou à gauche) pour une matrice carrée, alors celle ci est inversible}


	Supposons $\exists B \in \mathcal{M}_{n}(\mathbb{K}):A\times B = I_{n}$. Notons $(\hat{a}, \hat{b}) \in \mathcal{L}(\mathbb{K}^{n})$ les endomorphismes canoniquement associés à $A$ et à $B$.


	\begin{align*}
		\Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}(\hat{a} \circ \hat{b}) &= \mathrm{mat}(\hat{a} \circ  \hat{b}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) \\
		&= \mathrm{mat}(\hat{a}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) \times_{\mathcal{M_{n}(\mathbb{K})}} \mathrm{mat}(\hat{b}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) \\
		&= A \times B \\
		&= I_{n} \\
		&= \mathrm{mat}(\mathrm{Id}_{\mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}, \mathcal{B}_{\text{can } \mathbb{K}^{n}}) = \Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}(\mathrm{Id}_{\mathbb{K}^{n}})
	\end{align*}


	D'où, par injectivité de $\Phi_{\mathcal{B}_{\text{can } \mathbb{K}^{n}}}$, $\hat{a} \circ \hat{b} = \mathrm{Id}_{\mathbb{K}^{n}}$.

	Ainsi, $\hat{a} \circ \hat{b}$ est surjective, donc $\hat{a}$ est surjective, mais par l'accident de la dimension finie, $\hat{a}$ est bijective, donc c'est un automorphisme, donc toutes ses matrices associées sont inversibles. On effectue un même raisonnement pour l'inversibilité à gauche, en utilisant cette fois l'injectivité.
\end{question_kholle}
\pagebreak\section{Semaine 26}
	
	Cette semaine avais trois jours fériés : les khôlles furent annulées.
\pagebreak\section{Semaine 27}
	
	\begin{question_kholle}
		[{Soit $f \in \ContM{[a;b]}{}$. \\
		L'ensemble $\{ |f(t)| \;|\; t \in [a;b] \}$ admet une borne supérieur notée $\norminf{f}$.}]
		{Norme uniforme d'une fonction continue par morceaux}
		
		Montrons que sur chaque morceau, $f$ est bornée.
		
		Soit $\sigma = (x_i)_{0 \leqslant i \leqslant N} \in \mathcal{S}([a;b])$ adaptée à $f$.
		Soit $i \in \lient 0; N-1 \rient$. Posons $f_i = f_{|]x_i;x_{i+1}[}$.
		$f$ étant continue par morceaux, $\exists (l_i^+, l_{i-1}^-) \in \R^2 : \textlim{x}{x_i^+} f_i(x) = l_i^+ \wedge \textlim{x}{x_{i+1}^-} f_i(x) = l_{i+1}^-$.
		Nous pouvons donc prolonger $f_i$ en $\tilde{f_i}$ par continuité en $x_i$ et en $x_{i+1}$.
		Comme $f \in \Cont{0}{[a;b]}{}$, le théorème de Weierstrass s'applique : $\Im \tilde{f_i}$ est bornée (donc $f_i$ aussi). Ainsi $\norminf{f_i}$ est bien défini.
		
		\noindent $\{ |f(t)| \;|\; t \in [a;b] \}$ est : \begin{itemize}
			\item une partie de \R
			\item non vide car contenant $|f(x)|$.
			\item majorée par $\max \left( \{\norminf{f_i} | i \in \lient 0; N-1 \rient\} \cup \{\norminf{f_i} | i \in \lient 0; N-1 \rient\} \right)$ (ensemble admettant bien un plus grand élément puisque fini)
		\end{itemize}
		Donc $\norminf{f}$ est bien définie.
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\draw[->] (-0.5, 0) -- (5, 0);
				\draw[->] (0, -0.5) -- (0, 5);
				\draw (4, -0.1) node[anchor=north] {$1$} -- (4, 0.1);
				\draw (2, -0.1) node[anchor=north] {$\nicefrac{1}{2}$} -- (2, 0.1);
				\draw (-0.1, 4) node[anchor=east] {$1$} -- (0.1, 4);
				
				\draw[red] (0, 0) -- (2, 4) -- (4, 0);
				\filldraw[white, draw=red, densely dotted] (2, 3.95) circle (3pt);
				\filldraw[red] (2, 0) circle (2pt);
			\end{tikzpicture}
			\caption{$\norminf{f}$ peut ne pas être atteinte}
		\end{figure}
	\end{question_kholle}
	
	\begin{question_kholle}
		[Soit {$f \in \Cont{0}{[a;b]}{}$}.
		\begin{propositions}
			\item $\forall \varepsilon \in \R_+^*, \
			\exists \chi \in \mathcal{E}({[a;b]}, \R) :
			\norminf{f - \chi} \leqslant \varepsilon$
			\item $\forall \varepsilon \in \R_+^*, \
			\exists (\varphi, \psi) \in \mathcal{E}({[a;b]}, \R)^2 :
			\left\{ \begin{matrix}
				\varphi \leqslant f \leqslant \psi \\
				\norminf{\psi - \varphi} \leqslant \varepsilon
			\end{matrix} \right.$
		\end{propositions}]
		{Lemme d'approximation uniforme d'un fonction continue sur un segment par une fonction en escalier}
		
		Soit $f \in \Cont{0}{[a;b]}{}$. Soit $\varepsilon \in \R_+^*$ \fq. \\
		$(i)$ D'après le théorème de Heine, $f \in \ContU{[a;b]}{}$. Écrivons la définition de uniformément continue pour $\varepsilon$ :
		\begin{equation*}
			\exists \eta \in \R_ +^* : \ \forall (x, y) \in [a;b]^2, \
			|x - y| \leqslant \eta \implies |f(x) - f(y)| \leqslant \varepsilon
		\end{equation*}
		
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}
				\begin{axis}[
					axis lines = center,
					xlabel = $x$,
					ylabel = {$f(x)$},
					width=12cm,
					height=5cm
					]
					\addplot[
					domain=0:20,
					samples=200,
					color=green,
					]
					{2*cos((floor(x/2)*2+1)*45) + floor(x/2)+(1/2)};
					\addplot[
					domain=0:20,
					samples=200,
					color=red,
					]
					{2*cos(x*45) + x/2};
				\end{axis}
			\end{tikzpicture}
			\caption{Fonction en escalier "approximant" une fonction continue}
		\end{figure}
		
		\noindent Cherchons $N$ \tq $\frac{b - a}{N} \leqslant 2 \eta$. C'est-à-dire $N \geqslant \frac{b - a}{2 \eta}$.
		Posons donc $N = \lceil \frac{b - a}{2 \eta} \rceil$ et $\eta' = \frac{b-a}{N}$ de sorte que $\eta' \leqslant 2\eta$.
		
		Définissons $\chi \in \mathcal{E}({[a;b]}, \R)$ par
		\begin{equation*}
			\chi \left| \begin{array}{ccl}
				[a;b] &\rightarrow& \R \\
				x &\mapsto& \left\{ \begin{array}{ll}
					f(x) &\text{ si } \exists n \in \N : \ x = a + n \eta' \\
					f\left(a + \eta' \left( \lfloor \frac{x-a}{\eta'} \rfloor + \nicefrac{1}{2} \right) \right) &\text{ sinon }
				\end{array} \right.
			\end{array} \right.
		\end{equation*}
		Ceci est bien une fonction en escalier car $(a + k \eta')_{0 \leqslant k \leqslant N}$ est une subdivision adaptée. En effet, $\forall k \in \lient 0; N-1 \rient, \ f_{|]a+k\eta';a+(k+1)\eta'[} =f\left( a + \eta' \left( \lfloor \frac{x-a}{\eta'} \rfloor + \nicefrac{1}{2} \right) \right) \cdot \widetilde{1}_{]a+k\eta';a=(k+1)\eta'[}$.
		
		Soit $x \in [a; b]$.
		Si $\exists n \in \N : \ x = a + n \eta'$ alors $|f(x) - \chi(x)| = 0$.
		Sinon $0 \leqslant \frac{x - a} {\eta'} - \lfloor \frac{x-a}{\eta'} \rfloor \leqslant 1$.
		D'où $0 \leqslant (x - a) - \eta' \lfloor \frac{x-a}{\eta'} \rfloor \leqslant \eta'$.
		Donc, en enlevant $\nicefrac{\eta'}{2}$, $- \frac{\eta'}{2} \leqslant a + \eta' \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \leqslant \frac{\eta'}{2}$.
		Par définition de $\eta'$, \ $- \eta \leqslant a + \eta' \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \leqslant \eta$.
		Par définition de $\eta$, on a $|f(x) - f\left(a + 2 \eta \left( \lfloor \frac{x-a}{2 \eta} \rfloor + \nicefrac{1}{2} \right) \right)| \leqslant \varepsilon$.
		
		Ainsi, nous avons bien $\norminf{f - \chi} \leqslant \varepsilon$.
		\bigbreak
		
		$(ii)$ Écrivons la définition de uniformément continue pour $\varepsilon$ :
		\begin{equation*}
			\exists \eta \in \R_ +^* : \ \forall (x, y) \in [a;b]^2, \
			|x - y| \leqslant \eta \implies |f(x) - f(y)| \leqslant \varepsilon
		\end{equation*}
		Définissons $\varphi \in \mathcal{E}({[a;b]}, \R)$ par
		\begin{equation*}
			\left| \begin{matrix}
				[a;b] &\rightarrow& \R \\
				x &\mapsto& \left\{ \begin{matrix}
					f(x) &\text{ si } \exists n \in \N : \ x = a + n \eta \\
					\inf f\left( \; ]a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1 \right) [ \; \right) &\text{ sinon }
				\end{matrix} \right.
			\end{matrix} \right.
		\end{equation*}
		Définissons $\psi \in \mathcal{E}({[a;b]}, \R)$ par
		\begin{equation*}
			\left| \begin{matrix}
				[a;b] &\rightarrow& \R \\
				x &\mapsto& \left\{ \begin{matrix}
					f(x) &\text{ si } \exists n \in \N : \ x = a + n \eta \\
					\sup f\left( \; ] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[ \; \right) &\text{ sinon }
				\end{matrix} \right.
			\end{matrix} \right.
		\end{equation*}
		Ces deux fonctions sont bien définies car $f_{|] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[}$ est continue donc, d'après le théorème de Weiertraß, son image admet une borne inférieure et une borne supérieure.
		Elle sont bien en escalier.
		
		Par définition des bornes inférieures et supérieures, nous avons $\varphi \leqslant f \leqslant \psi$.
		De plus, pour $x \in [a;b]$ \fq, $f_{|] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[}$ se prolonge par continuité et, d'après le théorème de Weiertraß, atteint ses bornes. Notons $f_i$ et $f_s$ les antécédents respectifs des bornes.
		$(f_i, f_s) \in ] a + \eta \lfloor \frac{x-a}{\eta'} \rfloor; a + \eta \left( \lfloor \frac{x-a}{\eta'} \rfloor + 1\right)[ ^2$ donc $\left|f_i - f_s\right| \leqslant \eta$.
		D'où $\left| f(f_i) - f(f_s) \right| \leqslant \varepsilon$.
		
		Ainsi, nous avons bien $\norminf{\psi - \varphi} \leqslant \varepsilon$.
	\end{question_kholle}
\pagebreak\section{Semaine 29}

\begin{question_kholle}[{Le noyau d'un morphisme de groupe étant toujours un sous-groupe du groupe de départ, le groupe alterné d'indice $n \in \mathbb{N}^{*}$ est le sous groupe de $(\mathcal{S}_{n}, \circ)$ obtenu en considérant le noyau du morphisme signature.
    $$
    \mathcal{A}_{n}= \ker \varepsilon
    $$
    $\mathcal{A}_{n}$ est de cardinal $\frac{n!}{2}$
    }]{Définition et cardinal du sous-groupe alternée $\mathcal{A}_n$}
    
    
    Fixons $\tau = (1, 2)$
    Considérons
    $$\Phi \left|\begin{array}{ll} \mathcal{A}_{n} &\to \mathcal{S}_{n} \setminus \mathcal{A}_{n} \\ \sigma &\mapsto \sigma \circ \tau \end{array}\right.$$
    \begin{itemize}
        \item 
        
        $\Phi$ est bien définie: soit $\sigma \in \mathcal{A}_{n}$ fixée quelconque. Par propriété de morphisme de la signature, $\varepsilon(\sigma \circ \tau) = \varepsilon(\sigma) \times \varepsilon(\tau) = 1 \times (-1) = -1$ donc $\sigma \circ \tau \not\in \mathcal{A}_{n}$ donc $\Phi(\sigma) \in \mathcal{S}_{n}\setminus \mathcal{A}_{n}$
        
        \item De plus, $\Phi$ est bijective en considérant 
$$\Psi\left|\begin{array}{ll} \mathcal{S}_{n}\setminus \mathcal{A}_{n} &\to \mathcal{A}_{n} \\ \sigma &\mapsto \sigma \circ  \tau \end{array}\right.$$
$\Psi \circ \Phi = \mathrm{Id}_{\mathcal{A}_{n}}$ et $\Phi \circ \Psi = \mathrm{Id}_{\mathcal{S}_{n}\setminus \mathcal{A}_{n}}$
    \end{itemize}
    Ainsi,
$$
    \lvert \mathcal{A}_{n} \rvert  = \lvert \mathcal{S}_{n}\setminus \mathcal{A}_{n} \rvert  = \lvert \mathcal{S}_{n} \rvert - \lvert \mathcal{A}_{n} \rvert 
$$
    D'où $\lvert \mathcal{A}_{n} \rvert = \frac{\lvert \mathcal{S}_{n} \rvert}{2} = \frac{n!}{2}$
\end{question_kholle}
\begin{question_kholle}{Caractérisation des bases par le déterminant}
    \begin{itemize}[label=$\star$]
        \item Supposons que la famille $\mathcal{B'} =(u_{1}, \dots, u_{n}) \in E^{n}$ est une base de $E$.
        $$\det_{\mathcal{B}}\mathcal{B'}\times \det_{\mathcal{B'}}\mathcal{B} = 1 \implies \det _{\mathcal{B}}\mathcal{B'} \neq 0$$
        
        \item Supposons qu'il existe une base $\mathcal{B}$ telle que $\det_{\mathcal{B}} \mathcal{B}' \neq 0$
        Si $\mathcal{B}'$ était liée, le déterminant serait nul, donc en contraposant, $\mathcal{B}'$ n'est pas liée, et est de cardinal $n$, c'est une base.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{
    Soit $E$ un $\mathbb{K}$-espace vectoriel de dimension finie et $F \in \mathcal{L}_{\mathbb{K}}(E)$
    $$\exists!\lambda \in \mathbb{K} : \forall \mathcal{B} \text{ base de }E, \forall (u_{1}, \dots, u_{n})\in E^{n}, \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n}))=\lambda \times \det_{\mathcal{B}}(u_{1}, \dots, u_{n})$$
    On appelle ce $\lambda$ \underline{le} déterminant de l'endomorphisme $f$.
    }]{Définition du déterminant d'un endomorphisme}
    
    \begin{itemize}[label=$\lozenge$]
        \item \underline{Existence}
        
        Soit $\mathcal{B}_{0}= (e_{1}, \dots, e_{n})$ une base de $E$ fixée.
        L'application
$$
        \varphi \left|\begin{array}{ll} E^{n} &\to \mathbb{K} \\ (u_{1}, \dots, u_{n}) &\mapsto \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) \end{array}\right.
$$
        est
        \begin{itemize}
            \item Une forme n-linéaire :  soient $(u_{1}, \dots, u_{n}) \in E^{n}$ fixés quelconques $(u, v, \lambda) \in E^{2} \times \mathbb{K}$
            
            \begin{align*}
                \varphi (v+\lambda.w, u_{2}, \dots, u_{n})  & = \det_{\mathcal{B}_{0}}(f(v+\lambda.w), f(u_{2}), \dots, f(u_{n})) \\
                &= \det_{\mathcal{B}_{0}}(f(v)+ \lambda.f(w), f(u_{2}), \dots, f(u_{n})) \text{ par linéarité de }f\\
                &= \det_{\mathcal{B}_{0}}(f(v), f(u_{2}), \dots, f(u_{n})) + \lambda \times \det_{\mathcal{B}_{0}}(f(w), f(u_{2}), \dots, f(u_{n}))\\
                & \text{ par linéarité de } \det_{\mathcal{B}_{0}} \\
                &= \varphi(v, u_{2}, \dots, u_{n})+ \lambda \times \varphi(w, u_{2}, \dots, u_{n})
            \end{align*}
            
            Par conséquent, $\varphi$ est linéaire en son premier argument.
            On prouve de même que $\varphi$ est linéaire en ses $n-1$ autres arguments, ce qui montre sa $n$-linéarité.
            
            \item Alternée
            Soient $(u_{1}, \dots, u_{n}) \in E^{n}$ tels qu'il existe $(i, j) \in [ \! [ 1, n ] \!]^{2}$ tels que $i \neq j$ et $u_{i} = u_{j}$ alors on a aussi $f(u_{i}) = f(u_{j})$, si bien que le caractère alterné de $\det_{\mathcal{B}_{0}}$
$$
            \varphi(u_{1}, \dots, u_{n})=\det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) = 0
$$
            Donc $\varphi \in \land_{\mathbb{K}}^{n} = \text{Vect}\{ \det_{\mathcal{B}_{0}} \}$
        \end{itemize}
        Donc
$$
        \exists \lambda_{\mathcal{B}_{0}}\in \mathbb{K}: \varphi = \lambda_{\mathcal{B}_{0}}.\det_{\mathcal{B_{0}}}
$$
        d'où,
$$
        \forall (u_{1}, \dots, u_{n}) \in E^{n}, \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n}))= \lambda_{\mathcal{B}_{0}}\times \det_{\mathcal{B_{0}}}(u_{1}, \dots, u_{n})
$$
        Soit $\mathcal{B}$ une base de $E$ fixée quelconque. Nous savons que
$$
        \det_{\mathcal{B}} = \det_{\mathcal{B}} \mathcal{B}_{0} . \det_{\mathcal{B}_{0}}
$$
        Donc en multipliant la relation précédente par $\det_{\mathcal{B}}\mathcal{B}_{0}$,
$$
        \forall (u_{1}, \dots, u_{n}) \in E^{n}, \underbrace{ \det_{\mathcal{B}}\mathcal{B}_{0} \times \det_{\mathcal{B}_{0}}(f(u_{1}), \dots, f(u_{n})) }_{ \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n})) }= \lambda_{\mathcal{B}_{0}} \times \underbrace{ \det_{\mathcal{B}}\mathcal{B}_{0} \times \det_{\mathcal{B_{0}}}(u_{1}, \dots, u_{n}) }_{ \det_{\mathcal{B}}(u_{1}, \dots, u_{n}) }
$$
        Par conséquent, $\lambda_{\mathcal{B}_{0}}$ convient pour toute base $\mathcal{B}$.
        
        \item \underline{Unicité}
        Soit $\lambda \in \mathbb{K}$ tel que
$$
        \forall \mathcal{B} \text{ base de }E, \forall (u_{1}, \dots, u_{n})\in E^{n}, \det_{\mathcal{B}}(f(u_{1}), \dots, f(u_{n}))=\lambda \times \det_{\mathcal{B}}(u_{1}, \dots, u_{n})
$$
        Particularisons pour $\mathcal{B}\leftarrow \mathcal{B}_{0}$ et $(u_{1}, \dots, u_{n})\leftarrow \mathcal{B}_{0}$
$$
        \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n})) = \lambda \times \det_{\mathcal{B}_{0}}\mathcal{B}_{0} = \lambda \times 1
$$
        Donc $\lambda = \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n}))$
        Or, en particularisant la relation définissant $\lambda_{\mathcal{B}_{0}}$ pour $(u_{1}, \dots, u_{n}) \leftarrow \mathcal{B}_{0}$
$$
        \lambda_{\mathcal{B}_{0}} = \det_{\mathcal{B}_{0}}(f(e_{1}), \dots, f(e_{n}))
$$
        donc $\lambda = \lambda_{\mathcal{B}_{0}}$
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}
    [{\begin{enumerate}
        \item $\forall (f, g) \in \mathcal{L}_{\mathbb{K}}(E)^{2}, \det (f \circ g) = \det f \times \det g$
        
        \item $\forall f \in \mathcal{L}_{K}(E), f \in \mathcal{GL}_{\mathbb{K}}(E) \iff \det f \neq 0$
    \end{enumerate}
    }]{Démontrer que le déterminant est un morphisme de $(\mathcal{L}_{\mathbb{K}}(E), \circ)$ dans $(\mathbb{K}, \times)$, application à la caractérisation des automorphismes}
    Fixons $\mathcal{B}=(e_{1}, \dots, e_{n})$ une base de $E$
    \begin{enumerate} 
        \item Soient $(f, g) \in \mathcal{L}_{\mathbb{K}}(E)^{2}$ fixés quelconques.
        
        \begin{align*}
            \det (f \circ  g) &= \det_{\mathcal{B}}((f \circ g)(e_{1}), \dots, (f \circ  g)(e_{n})) \\
            &= \det_{\mathcal{B}}(f(g(e_{1})), \dots, f(g(e_{n}))) \\
            &= \det f \times \det_{\mathcal{B}}(g(e_{1}), \dots, g(e_{n})) \text{ par définition du déterminant d'un endomorphisme} \\
            &= \det f \times \det g \times \det_{\mathcal{B}}(e_{1}, \dots, e_{n}) \\
            &= \det f \times \det g
        \end{align*}
        
        
        
        \item Soit $f \in \mathcal{L}_{\mathbb{K}}(E)$
        \begin{itemize}
            \item Supposons $f \in \mathcal{GL}_{\mathbb{K}}(E)$
            Appliquons la relation de morphisme pour $g \leftarrow f^{-1}$
$$
            \underbrace{ \det(f \circ  f^{-1}) }_{ = \det \mathrm{Id}_{E} } = \det f \times \det f^{-1}
$$
            Or, $\det \mathrm{Id}_{E}= \det_{\mathcal{B}}(e_{1}, \dots, e_{n}) = 1$ si bien que $\det f \times \det f^{-1} = 1$ on en déduit que $\det f \neq 0$ et d'autre part que $\det (f^{-1}) = \frac{1}{\det f}$
            \item Supposons que $\det f \neq 0$
            Par définition du déterminant d'un endomorphisme
$$
            \det_{\mathcal{B}}(f(e_{1}), \dots, f(e_{n})) = \det f \times \det_{\mathcal{B}}(e_{1}, \dots, e_{n}) = \det f
$$
            Donc $\det_{\mathcal{B}}(f(e_{1}), \dots, f(e_{n})) \neq 0$ si bien que $(f(e_{1}), \dots, f(e_{n}))$ est une base de $E$, donc $f$ envoie une base sur une base : c'est un automorphisme.
        \end{itemize}
    \end{enumerate}
\end{question_kholle}
\begin{question_kholle}[{Soit $A \in \mathcal{M}_{n}(\mathbb{K})$.
    Alors $A \times (\mathrm{com} A)^{T} = (\mathrm{com}A)^{T}\times A = \det A \times I_{n}$
    }]{Produit d'une matrice carrée par la transposée de sa comatrice.}
    
    \begin{itemize}[label=$\lozenge$]
        \item Montrons que $A \times (\mathrm{com}A)^{T}=\det A \times I_{n}$
        Soient $(i, j) \in [ \! [ 1, n ] \!]$ fixés quelconques
        
        \begin{align*}
            [A \times (\mathrm{com}A)^{T}]_{i,j} &= \sum_{k=1}^{n}A_{i,k}[(\mathrm{com}A)^{T}]_{k,j} \\
            &=\sum_{k=1}^{n}A_{i,k}(\mathrm{com}A)_{j,k} \\
            &= \sum _{k=1}^{n}A_{i,k}\times (-1)^{k+j}\Delta_{j,k}
        \end{align*}
        
        \begin{itemize}[label=$\star$]
            \item Supposons que $i = j$ nous obtenons
$$
            [A\times(\mathrm{com}A)^{T}]_{i,i} = \sum_{k=1}^{n}A_{i,k}\times(-1)^{k+i}\Delta_{i,k} = \det A
$$
            D'après la formule du développement du déterminant de $A$ selon la $i$-ième ligne.
            
            \item Supposons que $i \neq j$
            La formule peut être interprétée comme le développement selon la $i$-ième ligne du déterminant de la matrice obtenue à partir de $A$ en remplaçant sa $j$-ième ligne par sa $i$-ième ligne:
            
            \begin{align*}
                \left[ A\times (\mathrm{com}A)^{T} \right] _{i,j} &= \sum_{k=1}^{n}A_{i,k}\times(-1)^{k+j}\Delta_{j,k} \\
                &=\left| \begin{array}{c}
                    L_{1} \\
                    \hline 
                    \vdots \\
                    \hline
                    L_{i-1} \\
                    \hline 
                    L_{i} \\
                    \hline L_{i+1} \\
                    \hline \vdots \\
                    \hline L_{j-1} \\
                    \hline L_{i} \\
                    \hline L_{i+1} \\
                    \hline \vdots \\
                    \hline L_{n}
                \end{array} \right|  \\
                &=0
            \end{align*}
            
            Car les lignes d'indice $i$ et $j$ sont identiques.
            Ainsi, pour tout $(i, j) \in [ \! [ 1, n ] \!]^{2}, [A\times(\mathrm{com}A)^{T}]_{i,j}=\delta_{i,j}\times \det A$
            Donc
$$
            [A\times(\mathrm{com}A)^{T}]_{i,j}=\det A\times I_{n}
$$
        \end{itemize}
        \item On montre de même le produit dans l'autre sens.
    \end{itemize}
\end{question_kholle}
\begin{question_kholle}[{  Le système linéaire $AX=B$ d'inconnue $X \in \mathcal{M}_{n,1}(\mathbb{K})$ et de paramètre $B \in \mathcal{M}_{n,1}(\mathbb{K})$ est dit "de Cramer" s'il admet une unique solution, à savoir si $A$ est une matrice inversible. Dans ce cas, la solution peut être exprimée explicitement par la formule $A^{-1}B$ qui donne la formule dite de Cramer:
    $$
    \left( \frac{
    \bigg| 
    B\mid C_{2}\mid\dots \mid C_{n}
    \bigg| 
    }{\det A}, \dots, \frac{
    \bigg|
    C_{1} \mid
    \dots \mid
    C_{i-1}\mid
    B \mid
    C_{i+1}\mid
    \dots \mid
    C_{n}
    \bigg|
    }{\det A}, \dots, \frac{
    \bigg| 
    C_{1}\mid C_{2}\mid\dots \mid B
    \bigg| 
    }{\det A} \right)
    $$
    où $(C_{1}, \dots, C_{n}) \in \mathcal{M}_{n,1}(\mathbb{K})^{n}$ sont les colonnes de $A$.
    }]{Formule de Cramer}
    
    Partons de l'expression de l'inverse avec la comatrice:
    $$X = A^{-1}B= \frac{1}{\det A}(\mathrm{com}A)^{T} B$$
    Soit $i \in [ \! [ 1, n ] \!]$.
    
    \begin{align*}
        X_{i, 1} &= \frac{1}{\det A}[(\mathrm{com}A)^{T}B]_{i,j} \\
        &= \frac{1}{\det A}\sum_{k=1}^{n}[(\mathrm{com}A)^{T}]_{i,k}B_{k,1} \\
        &= \frac{1}{\det A}\sum_{k=1}^{n}(\mathrm{com}A)_{k,i}B_{k, 1} \\
        &= \frac{1}{\det A}\sum_{k=1}^{n}(-1)^{k+i}\Delta_{k,i}B_{k, 1} \\
        &\text{qui s'interprète comme le développement selon la $i$-ième colonne de la matrice} \\
        &= \frac{1}{\det A}\Bigg|C_{1}\Bigg|\dots\Bigg|C_{i-1}\Bigg|B\Bigg|C_{i+1}\Bigg|\dots\Bigg|C_{n}\Bigg|
    \end{align*}
\end{question_kholle}
\begin{question_kholle}{Calcul du déterminant de Vandermonde}
Posons
    $$
\mathcal{P}(n) : \forall (a_{0},\dots,a_{n}) \in \mathbb{K}^{n+1}, V(a_{0}, \dots, a_{n}) = \prod_{1\leqslant i < j \leqslant n}(a_{j} - a_{i})
$$
\begin{itemize}[label=$\lozenge$]
    \item \underline{Initialisation} $n \leftarrow 2$
Soient $(a_{0}, a_{1}) \in \mathbb{K}^{2}$
$$
\left| \begin{matrix}
1 & 1 \\
a_{0} & a_{1}
\end{matrix}\right| = a_{1} - a_{0}
$$



\item \underline{Hérédité}, soit $n \in \mathbb{N}^{*}$ fixé quelconque tel que $\mathcal{P}(n)$ est vraie.
Soient $(a_{0}, a_{1}, \dots, a_{n+1}) \in \mathbb{K}^{n+2}$ fixés quelconques.
\begin{itemize}
    \item Supposons que les éléments de $\left\{ a_{0}, \dots, a_{n+1} \right\}$ ne sont pas tous deux à deux distincts.
    
    Alors le déterminant à calculer possède deux colonnes identiques donc il est nul, et la formule avec laquelle il doit coïncider s'annule également, donc $\mathcal{P}(n+1)$ est vraie dans ce cas

    \item Supposons que les éléments de $\left\{ a_{0} ,\dots, a_{n+1} \right\}$ sont tous distincts.
Notons
$$
Q(X) = \left| \begin{matrix}
1 & 1 & 1 & \dots  & 1 & 1 \\
a_{0} & a_{1} & a_{2} & \dots & a_{n} & X \\
a_{0}^{2} & a_{1}^{2} & a_{2}^{2} & \dots & a_{n}^{2} & X^{2} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
a_{0}^{n} & a_{1}^{n} & a_{2}^{n} & \dots & a_{n}^{n} & X^{n} \\
a_{0}^{n+1} & a_{1}^{n+1} & a_{2}^{n+1} & \dots & a_{n}^{n+1} & X^{n+1}
\end{matrix} \right| 
$$
Sachant que le déterminant d'une matrice est une somme de produits de coefficients de la matrice, puisque tous les coefficients du déterminant $Q(X)$ sont des polynômes en $X$, $Q(X) \in \mathbb{K}[X]$ (car $\mathbb{K}[X]$ est un anneau et donc stable par produit).
De plus, en développant le déterminant $Q(X)$ selon sa dernière colonne, on observe d'une part que $\deg Q \leqslant n+1$ et d'autre part que le coefficient de $X^{n+1}$ est le cofacteur de $X^{n+1}$ qui est, d'après $\mathcal{P}(n)$
$$
\prod_{0\leqslant i<j \leqslant n}(a_{j}-a_{i})
$$
Et, comme tous les $a_{i}$ sont distincts, ce coefficient est non-nul, donc $\deg Q=n+1$

De plus, $Q(a_{0})=0, Q(a_{1})=0, \dots, Q(a_{n})=0$ car le déterminant présente dans chacun des calculs deux colonnes égales. Nous en déduisons que $Q$ admet au moins $(n+1)$ racines deux à deux distinctes, or son degré est exactement $n+1$ donc
- il n'y a aucune autre racine
- elles sont toutes simples

La forme factorisée de $Q$ est donc
$$
Q(X)=\underbrace{ \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) }_{ \text{coefficient dominant} } \times \underbrace{ \prod_{k=0}^{n}(X-a_{k})^{1} }_{ n+1 \text{ racines simples} }
$$
Donc 
\begin{align*}
V(a_{0}, a_{1},\dots, a_{n+1})&=Q(a_{n+1}) \\
&=  \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) \times  \prod_{k=0}^{n}(a_{n+1}-a_{k})  \\
&= \left( \prod_{0\leqslant i < j \leqslant n} (a_{j}-a_{i})\right) \prod_{\substack{0\leqslant i<j \\ j=n+1}}^{n}(a_{j}-a_{k}) \\
&= \prod_{0\leqslant i < j \leqslant n+1} (a_{j}-a_{i})
\end{align*}

Donc $\mathcal{P}(n+1)$ est vraie
\end{itemize}
\end{itemize}
\end{question_kholle}
\pagebreak\section{Semaine 30}

\begin{question_kholle}{Inégalité de Cauchy-Schwartz dans un Espace préhilbertien réel, cas d'égalité}
	Soit $E$ un $\mathbb{R}$-espace vectoriel, et $\braket{ \cdot | \cdot  }$ un produit scalaire sur $E$.
	Soient $(x, y) \in E^{2}$
	\begin{enumerate}
		\item \begin{itemize}[label=$\star$]
			\item Si $y=0$, l'inégalité est une égalité et est évidente
			\item Sinon, posons
			$$
			P:\left|\begin{array}{ll} \mathbb{R} &\to \mathbb{R} \\ t &\mapsto \braket{ x+t.y | x+t.y } = t^{2} \|y\|^{2} + 2 t \braket{ x | y }  + \|x\|^{2} \end{array}\right.
			$$
			Puisque $\|y\|^{2} \neq 0$, P est un polynôme de degré $2$ à coefficientts réels et positif d'après le caractère positif du produit scalaire (on a donc $\forall t \in \mathbb{R}, P(t)\geqslant 0$)
			Le discriminant de cette fonction polynômiale est $\Delta = 4 \braket{ x | y }^{2} - 4 \|x\|^{2}\|y\|^{2}$, qui est obligatoirement négatif ou nul puisque $P$ admet au mieux une racine double.
			Donc $\braket{ x | y }^{2} - \|x\|^{2}\|y\|^{2} \leqslant 0$ donc en prenant la racine carrée $|\braket{ x | y }| \leqslant \|x\|\|y\|$.
		\end{itemize}
		\item \begin{itemize}[label=$\star$]
			\item Supposons que $(x, y)$ est liée, sans perte de généralité, supposons $y = \lambda.x$ alors
			$$
			\lvert \braket{ x | \lambda.x } \rvert  = \lvert  \lambda \rvert  \braket{ x | x }  = \lvert  \lambda \rvert  \|x\|^{2} = \| x \| \| \lambda.x\|
			$$
			Donc l'inégalité est une égalité.
			
			\item Réciproquement, supposons que $\lvert  \braket{ x | y } \rvert = \| x\| \| y\|$
			\begin{itemize}
				\item Si $y = 0$ alors $(x, y)$ est liée
				\item Sinon, $\Delta = 4(\braket{ x | y }^{2} - \|x\|\|y\|) = 0$
				$P$ est un polynôme de degré $2$ de discriminant nul : il admet une racine double $\lambda$
				Ainsi
				$$
				P(\lambda) = 0 \implies \braket{ x+\lambda.y | x+\lambda.y } = 0
				$$
				Donc $x+\lambda .y = 0_{E}$ d'après le caractère défini du produit scalaire.
			\end{itemize}
		\end{itemize}
	\end{enumerate}
\end{question_kholle}
\begin{question_kholle}[{
	L'application
	$$
	\chi \left|\begin{array}{ll} E &\to E^{*} \\ x &\mapsto \left|\begin{array}{ll} E &\to \mathbb{R} \\ y &\mapsto \braket{ x | y }  \end{array}\right. \end{array}\right.
	$$ est un isomorphisme d'espaces vectoriels. $\chi$ est appelé l'isomorphisme canonique entre un espace vectoriel euclidien et son espace dual.	
	}]{Isomorphisme entre un espace euclidien et l'espace de ses formes linéaires (Théorème de représentation de Riesz).}
	\begin{itemize}[label=$\star$]
		\item $\chi$ est bien définie car, $\forall x \in E$, par linéarité du produit scalaire en sa seconde variable, $\chi(x): \left|\begin{array}{ll} E &\to \mathbb{R} \\ y &\mapsto \braket{ x | y }  \end{array}\right.$ est une forme linaire sur $E$.
		
		
		\item Soient $(x, x') \in E^{2}$ et $\lambda \in \mathbb{R}$ fixés quelconques
		
		
		\begin{align*}
			\forall y \in E,  \chi(x + \lambda .x')(y) &= \braket{ x+\lambda.x' | y }  \\
			&= \braket{ x | y } + \lambda \times\braket{ x' | y }  \\ 
			&= \chi(x)(y) + \lambda \times \chi(x')(y) \\
			&= (\chi(x) + \lambda . \chi(x') )(y)
		\end{align*}
		
		Donc $\chi(x+\lambda x') = \chi(x) + \lambda.\chi(x')$, donc $\chi$ est linéaire.
		
		\item Soit $x \in \ker \chi$ fixé quelconque.
		Alors $\chi (x) = 0_{E^{*}}$
		$$\forall y \in E, \braket{ x | y } = 0$$
		Donc $x \in E^{\perp} = \{ 0_{E} \}$ donc $x = 0_{E}$
		Donc $\chi$ est injective, or $E$ et $E^{*}$ sont de même dimension, donc $\chi$ est bijective.
		Donc $\chi$ est un isomorphisme.
	\end{itemize}
\end{question_kholle}
\begin{question_kholle}{Montrer que si $F$ est un sous-espace vectoriel de dimension finie d'un espace préhilbertien réel, $F^{\perp}$ est son supplémentaire orthogonal}
	
	
	Soient $(E, \braket{ \cdot | \cdot })$ un espace préhilbertien réel, et $F$ un sous espace vectoriel de dimension finie.
	Alors $F$ et $F^{\perp}$ sont supplémentaires orthogonaux, i.e. $E = F  \overset{\perp}{\oplus} F^{\perp}$
	
	
	
	En notant $r = \dim F$, fixons une base orthonormale $(e_{1}, \dots, e_{r})$ de $F$, possible car $F$ est un espace euclidien (dimension finie et muni du produit scalaire induit par $E$).
	\begin{itemize}[label=$\lozenge$]
		\item \underline{Analyse}
		
		Soit $x \in E$ fixé quelconque, supposons que $\exists (x_{\parallel}, x_{\perp}) \in F \times F^{\perp} = x_{\parallel}+x_{\perp}$
		D'abord $$x_{\parallel} \in F \implies \exists (\lambda_{1}, \dots, \lambda_{r}) \in \mathbb{R}^{r}: x_{\parallel}= \sum_{i=1}^{r}\lambda_{i}.e_{i}$$
		Soit $j \in [ \! [ 1, r ] \!]$ fixé quelconque
		
		\begin{align*}
			\braket{ x | e_{j} } &=   \left\langle \sum_{i=1}^{r}\lambda_{i}.e_{i} + x_{\perp} \Bigg| e_{j}  \right\rangle   \\
			&= \sum_{i=1}^{r}\lambda_{i}\times \underbrace{ \braket{ e_{i} | e_{j} } }_{ \delta_{ij} }  + \overbrace{ \braket{ \underbrace{ x_{\perp} }_{ \in F^{\perp} } | \underbrace{ e_{j} }_{ \in F } } }^{ =0 }  \\
			&= \lambda_{j}
		\end{align*}
		
		Ainsi, $$\left\{ \begin{array}{ll}
			x_{\parallel} &= \sum_{i=1}^{r}\lambda_{i}.e_{i}= \sum_{i=1}^{r}\braket{ x | e_{i} } . e_{i} \\
			x_{\perp}  &= x - x_{\parallel}
		\end{array}\right.$$
		
		\item \underline{Synthèse}
		
		Posons donc 
		$$\left\{ \begin{array}{ll}
			x_{\parallel} &= \sum_{i=1}^{r} \braket{ x | e_{i} } .e_{i} \\
			x_{\perp} &= x - x_{\parallel}
		\end{array}\right. $$
		\begin{itemize}[label=$\star$]
			\item $(e_{1}, \dots, e_{r})$ est une base de $F$ donc $x_{\parallel} \in F$
			\item $x_{\parallel}+x_{\perp} = x_{\parallel}+ (x - x_{\parallel}) = x$
			\item Soit $j \in [ \! [ 1, r ] \!]$ fixé quelconque. Calculons $\braket{ x_{\perp} | e_{j} }$
			
			\begin{align*}
				\braket{ x_{\perp} | e_{j} } &= \braket{ x | e_{j} }  - \left\langle  \sum_{i=0}^{r}\braket{ x | e_{i} } .e_{i} \Bigg|  e_{j} \right\rangle \\
				&= \left\langle x | e_{j} \right\rangle - \sum_{i=0}^{r} \left\langle x | e_{i} \right\rangle\underbrace{  \left\langle e_{i} | e_{j} \right\rangle  }_{ \delta_{ij} } \\
				&= \left\langle x | e_{j} \right\rangle  - \left\langle x | e_{j} \right\rangle  = 0
			\end{align*}
			
			
			Donc $x_{\perp} \in \{e_{1}, \dots, e_{r}\}^{\perp}$
			Donc $x_{\perp} \in \text{Vect}\{ e_{1}, \dots, e_{r} \}^{\perp} = F^{\perp}$
			
		\end{itemize}
	\end{itemize}
	Ainsi, $F$ et $F^{\perp}$ sont supplémentaires orthogonaux.
	
	De plus
	$$	\forall x \in E, x = \underbrace{ \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i} }_{ \in F } + \underbrace{x - \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i}  }_{ \in F^{\perp} }$$
	Donc $$p_{F}^{\perp}(x) = \sum_{i=1}^{r}\left\langle x | e_{i} \right\rangle .e_{i} $$
	
\end{question_kholle}
\begin{question_kholle}[{On utilisera le produit scalaire $$
	\left\langle P | Q \right\rangle  = \int_{0}^{1} P(u)Q(u) \, \mathrm du
$$ }]{Orthonormalisation de la base canonique de $\mathbb{R}_2[X]$}
	Partons de la base canonique de $\mathbb{R}_{2}[X]$.
	\begin{itemize}[label=$\star$]
		\item $P_{1} =X^{0}$ est un vecteur unitaire Avec ce produit scalaire
		
		\item Calcul du second vecteur
$$
		P_{2}' = X - \left\langle X | 1 \right\rangle.1 = X - \left( \int_{0}^{1} u \, \mathrm du \right) .1 = X-\frac{1}{2}
$$
		
$$
		P_{2} = \frac{P_{2}'}{\|P_{2}'\|} = \frac{P_{2}'}{\sqrt{ \left\langle P_{2}' | P_{2}' \right\rangle  }}= \frac{P_{2}'}{\sqrt{ \int_{0}^{1} \left( u-\frac{1}{2} \right)^{2} \, \mathrm du }}
		= \frac{P_{2}'}{\sqrt{ \frac{1}{12} }}= \sqrt{ 12 }P_{2}'
$$
		Ce qui donne
$$
		P_{2}' = 2\sqrt{ 3 }X - \sqrt{ 3 }
$$
		
		\item Enfin, 
		
		\begin{align*}
			P_{3}' &= X^{2} - \left\langle X^{2} | 2\sqrt{ 3 }X-\sqrt{ 3 } \right\rangle.(2\sqrt{ 3 }X-\sqrt{ 3 })  - \left\langle X^{2} | 1 \right\rangle .1 \\
			&= X^{2} - \left( \int_{0}^{1} 2\sqrt{ 3 }u^{3} - \sqrt{ 3 }u^{2} \, \mathrm du  \right).(2\sqrt{ 3 }X-\sqrt{ 3 })-\left( \int_{0}^{1} u^{2} \, \mathrm du  \right).1 \\
			&= X^{2} - \frac{\sqrt{ 3 }}{6}(2\sqrt{ 3 }X - \sqrt{ 3 }) - \frac{1}{3} \\ \\
			&=X^{2} -X + \frac{1}{6}
		\end{align*}
		
		
$$
		P_{3} = \frac{P'_{3}}{\|P_{3}'\|}= \frac{P'_{3}}{\sqrt{ \left\langle P_{3} | P_{3} \right\rangle  }}= \frac{P'_{3}}{\sqrt{ \int_{0}^{1} \left( u^{2}-u +\frac{1}{6} \right)^{2} \, \mathrm du }}
		= \frac{P_{3}'}{\sqrt{ \frac{1}{180} }} =6\sqrt{ 5 }P'_{3} = 6\sqrt{ 5 }\left( X^{2}-X + \frac{1}{6} \right)
$$
		
		Donc une base orthonormée de $\mathbb{R}_{2}[X]$ muni de ce produit scalaire est
$$
		\left( 1, 2\sqrt{ 3 }X-\sqrt{ 3 }, 6\sqrt{ 5 }\left( X^{2}-X+\frac{1}{6} \right) \right)
$$
	\end{itemize}
\end{question_kholle}
\begin{question_kholle}[{Soit $(E, \left\langle \cdot | \cdot \right\rangle)$ un espace préhilbertien Réel.
	Soient $F$ un sous espace vectoriel de dimension finie de $E$, et $x \in E$.
	
	L'ensemble $\{ \|x-z\| \mid z \in F \}$ admet une borne inférieure appelée distance de $x$ à $F$ et notée $d(x, F)$, qui est un plus petit élément, atteinte uniquement pour pour $z = p_{F}^{\perp}(x)$
	}]{Distance d'un vecteur à un sous-espace vectoriel de dimension finie}
	
	$\{ \|x - z\| \mid z \in F \}$ est une partie de $\mathbb{R}$, non vide car elle contient $\|x\|$ pour $z \leftarrow 0_{F}$ d'éléments positifs ou nuls. Elle admet donc une borne inférieure
	
	$E$ est un espace euclidien, donc $E = F \overset{\perp}{\oplus} F^{\perp}$ donc $x$ se décompose selon ces supplémentaires orthogonaux
	$$x = \underbrace{ p_{F}^{\perp}(x) }_{ \in F } + \underbrace{ x - p_{F}^{\perp}(x) }_{ \in F^{\perp} }$$
	si bien que, pour tout $z \in F$
	
	\begin{align*}
		\|x - z\|^{2} &=  \|p_{F}^{\perp}(x) - z + x - p_{F}^{\perp}(x)\|^{2} \\
		&= \|p_{F}^{\perp}(x) - z \|^{2} + \|x - p_{F}^{\perp}(x)\|^{2} \text{ d'après le théorème de Pythagore} \\
		& \geqslant \|x - p_{F}^{\perp}(x)\|^{2}
	\end{align*}
	
	En prenant la racine carrée,
	$$\forall z \in F, \| x  - z\| \geqslant \|x - p_{F}^{\perp }(x)\|$$
	D'où $\| x - p_{F}^{\perp}(x)\|$ minore $\{ \| x - z\| \mid z \in F \}$ et donc sa borne inférieure.
	
	Or, en remonant le calcul précédent, il y a égalité pour $z = p_{F}^{\perp}(x)$ si bien que la borne inférieure est un plus petit élément, et vaut $d(x, F) = \|x-p_{F}^{\perp}(x)\|$
	
	De plus, si $z' \in F$ atteint ce plus petit élément on a
	
	\begin{align*}
		\| x- z'\|^{2} &= \|p_{F}^{\perp}(x) - z' + x - p_{F}^{\perp}(x)\|^{2} \\
		\| x - p_{F}^{\perp}(x)\|^{2} &= \|p_{F}^{\perp}(x) - z' \|^{2} + \|x - p_{F}^{\perp}(x)\|^{2} \\
		0 &= \|p_{F}^{\perp}(x) - z' \|^{2}
	\end{align*}
	
	Si bien que $p_{F}^{\perp}(x) - z' = 0_{E}$ d'après le caractère défini du produit scalaire.
	Donc le plus petit élément $d(x, F) = \min\{ \|x-z\| \mid z \in F \}$ est uniquement atteint pour $z = p_{F}^{\perp}(x)$.
\end{question_kholle}
\begin{question_kholle}[{
	Soit $(E, \left\langle \cdot | \cdot \right\rangle)$ un espace vectoriel euclidien, $\mathcal{B}=(e_{1}, \dots, e_{n})$ une base orthonormée de $E$.
	Soit $u = \sum_{i=1}^{n}u_{i}.e_{i}$ un vecteur de $E$.
	Soient $(a_{1}, \dots, a_{n}) \in\mathbb{R}^{n} \setminus \{ 0_{\mathbb{R}^{n}} \}$, $\alpha \in \mathbb{R}$ et $H_{\alpha}$ l'hyperplan affine d'équation
$$
	\sum_{i=1}^{n}a_{i}x_{i}=\alpha
$$
	}]{Distance à un sous-espace affine}
	Posons $a = \sum_{i=1}^{n}a_{i}.e_{i}$ $H_{0}$ est un hyperplan vectoriel et, $H_{0} = a^{\perp}$ et $H_{0}^{\perp}= \text{Vect}\{ a \}$
	Introduisons $h_{\alpha} \in a^{\perp}$ tel que $H_{\alpha}= h_{\alpha}+H_{0}$ et souvenons nous que $h_{\alpha}=\frac{\alpha}{\| a\|^{2}}.a$
	
	Observons que l'égalité $H_{\alpha}=h_{\alpha}+H_{0}$ donne
$$
	\{ \|u-z\| \mid z \in H_{\alpha} \} = \{ \|u-(h_{\alpha}+z')\| \mid z' \in H_{0} \}= \{ \|(u - h_{\alpha})-z'\| \mid z' \in H_{0}\}
$$
	
	or, d'après la caractérisation de la distance à un sous-espace quelconque, on a
	\begin{itemize}[label=$\star$]
		\item L'ensemble $\{ \|(u-h_{\alpha}) -z'\| \mid z' \in H_{0} \}$ admet une borne inférieure donc $\{ \| u - z\| \mid z \in H_{\alpha} \}$ aussi qui vaut $\mathrm{d}(u-h_{\alpha}, H_{0})$, ce qui prouve que $\mathrm{d}(u, H_{\alpha})$ est bien définie
		
		\item $\inf \{  \| (u - h_{\alpha}) - z'\| \mid z' \in H_{0} \}$ est un plus petit élément atteint pour l'unique valeur $z' = p_{H_{0}}^{\perp}(u-h_{\alpha})=p_{H_{0}}^{\perp}(u)$ car $h_{\alpha} \in H_{0}^{\perp}= \ker p_{H_{0}}^{\perp}$, donc $\mathrm{d(u, H_{\alpha})}=\inf \{  \|u-z\| \mid z \in H_{\alpha} \}$ est un plus petit élément atteint pour l'unique valeur $z = h_{\alpha}+p_{H_{0}}^{\perp}(u-h_{\alpha})= h_{\alpha}+ p_{H_{0}}^{\perp}(u)$
$$
		\mathrm{d}(u, H_{\alpha})= \| u - h_{\alpha}-p_{H_{0}}^{\perp}(u)\|
$$
	\end{itemize}
	Or $u - p_{H_{0}}^{\perp}(u) = (\mathrm{Id}-p_{H_{0}}^{\perp})(u) = p_{H_{0}^{\perp}}^{\perp}(u) = \left\langle u | \frac{a}{\|a\|} \right\rangle. \frac{a}{\|a\|}$ car $H_{0}^{\perp} = \text{Vect}\{ a \}$ d'où, sachant aussi que $h_{\alpha}= \frac{\alpha}{\|a\|^{2}}.a$
$$
	\mathrm{d}(u, H_{\alpha})= \|p_{H_{0}^{\perp}}^{\perp}(u)-h_{\alpha}\|= \left\| \left\langle a | \frac{a}{\|a\|} \right\rangle. \frac{a}{\|a\|}- \frac{\alpha}{\|a\|^{2}}.a \right\|
$$
\end{question_kholle}
\begin{question_kholle}
	{Dénombrement des surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$ et dans $\lient 1; 3 \rient$}
	
	Soit $n \in \N^*$.
	
	Il y a ${\left| \lient 1; 2 \rient \right|} ^ {\left| \lient 1; n \rient \right|} = 2^n$ applications de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$.
	Seules les applications constantes $\widetilde{1}$ et $\widetilde{2}$ ne sont pas surjectives.
	Il y a donc $2^n - 2$ surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$.
	
	Il y a ${\left| \lient 1; 3 \rient \right|} ^ {\left| \lient 1; n \rient \right|} = 3^n$ applications de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$. Les applications non surjectives sont celles dont l'image n'est pas $\lient 1; 3 \rient$. C'est-à-dire, celles dont l'image est de cardinal 1 (les fonctions constantes $\widetilde{1}$, $\widetilde{2}$ et $\widetilde{3}$) et celles dont l'image est de cardinal 2. Ces dernières sont les surjections de $\lient 1; n \rient$ dans $\lient 1; 2 \rient$, $\{ 1; 3 \}$ et $\{ 2; 3 \}$. Comme ces trois ensembles ont la même taille, il y a $3 \times (2^n - 2)$ (voir résultat précédent) applications de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$ dont l'image est de cardinal 2. Ainsi, le nombre de surjections de $\lient 1; n \rient$ dans $\lient 1; 3 \rient$ est $3^n - 3 - 3(2^n -2) = 3^n - 3 \times 2^n + 3$.
\end{question_kholle}

\begin{question_kholle}
	[Soient $E, F$ deux ensembles finis non vides et $f : E \rightarrow F$ telle que tout élément de $F$ possède le même nombre $k \in \N^*$ d'antécédents par $f$. \\
	Alors $\left|F\right| = \frac{\left|E\right|}{k}$
	\begin{quotation}
		\textquotedblleft Pour compter les moutons, il faut compter les pattes puis diviser par quatre. \textquotedblright
	\end{quotation}]
	{Lemme des bergers}
	
	Considérons la relation binaire définie sur $E$ par :
	\begin{equation*}
		\forall (x, y) \in E^2, x \sim y
		\iff f(x) = f(y)
	\end{equation*}
	Elle est réflexive, transitive et symétrique donc c'est bien une relation d'équivalence.
	Donc les classes d'équivalence réalise une partition de $E$.
	Nous avons $\displaystyle E = \bigsqcup_{C \in \nicefrac{E}{\sim}} C$ donc, en passant aux cardinaux, $\displaystyle \left|E\right| = \sum_{C \in \nicefrac{E}{\sim}} \left|C\right|$.
	
	Soit $x \in E$ \fq. Alors $\bar{x} = \left\{ y \in E \;|\; f(x) = f(y) \right\} = f^{-1}(f(\{x\}))$. Par hypothèse, tous les éléments de $F$ ont le même nombre $k$ d'antécédents, or $f({x})$ est un singleton d'élément de $F$ donc $\left|\bar{x}\right| = k$.
	Ainsi $\forall C \in \nicefrac{E}{\sim}, \left|C\right| = k$.
	
	Posons $\varphi \left|\begin{array}{ccc}
		\nicefrac{E}{\sim} & \mapsto & F \\
		C & \rightarrow & f(x) \text{ où } x \in C
	\end{array}\right.$.
	$\varphi$ est bien défini car si $(x, y) \in E$ vérifie $\bar{x} = \bar{y}$ alors $f(x) = f(y)$ donc l'image par $\varphi$ ne dépend pas du représentant de classe choisi.
	$\varphi$ est surjective car soit $z \in F$, $f$ est surjective donc $\exists x_z \in E : f(x_z) = z$ et alors $\varphi(\bar{x_z}) = f(x_z) = z$.
	$\varphi$ est injective car soient $(C, C') \in \left( \nicefrac{E}{\sim} \right)^2, \varphi(C) = \varphi(C')$ alors $\exists (x, x') \in C \times C' : x \sim x'$, comme deux classes d'équivalence sont confondues ou disjointes, $C = C'$.
	Ainsi $\varphi$ est une bijection donc $\left|F\right| = \left|\nicefrac{E}{\sim}\right|$.
	
	Ainsi $\displaystyle \left|E\right| = \sum_{C \in \nicefrac{E}{\sim}} \left|C\right| = \sum_{C \in \nicefrac{E}{\sim}} k = \left| \nicefrac{E}{\sim} \right| k = \left|F\right| k$.
\end{question_kholle}
\pagebreak\section{Semaine 31}

Pour cette semaine, $E$ est un ensemble fini de cardianl $n \in \N^*$ et $(\Omega, \proba)$ désigne un espace probabilisé fini.

\begin{question_kholle}
	[Soit $p \in \N^*$. Un $p$-partage de $E$ est un $p$-liste $(A_1, \ldots, A_p) \in \mathcal{P}(E)^p$ de parties de $E$ (éventuellement vide), deux à deux disjointes qui recouvrent $E$ c'est-à-dire \tq+* t:
	\begin{equation}
		\forall (i, j) \in \lient 1 ; p \rient,
		i \neq j \implies A_i \cap A_j = \emptyset
		\qquad \text{et} \qquad
		\bigcup_{i=1}^{p} A_i = E
	\end{equation}
	
	Soient $(n_1, \ldots n_p) \in \N^p$ \tqs $n = n_1 + \ldots + n_p$ est un $p$-partage de $E$ \tq
	\begin{equation*}
		\forall (i, j) \in \lient 1 ; p \rient, \
		\left|A_i\right| = n_i
	\end{equation*}
	Le nombre de $p$-partage de type $(n_1, \ldots, n_p)$ est :
	\begin{equation}
		\frac{n!}{\displaystyle \prod_{i=1}^{p} n_i !}
	\end{equation}
	]
	{$p$-partage d'un ensemble $E$ et leur dénombrement}
	
	Considérons les $p$-partages de type $(n_1, \ldots, n_p)$ et appliquons le principe des choix successifs :
	\begin{equation*}
		\left(
		\underbrace{A_1}_{\binom{n}{n_1} \text{ choix}},
		\underbrace{A_2}_{\binom{n}{n_2} \text{ choix}},
		\underbrace{A_3}_{\binom{n}{n_3} \text{ choix}},
		\ldots,
		\underbrace{A_p}_{\binom{n}{n_p} \text{ choix}}
		\right)
	\end{equation*}
	donc il y a
	\begin{equation*}
		\frac{ n! }{n_1! \cancel{(n-n_1)!} }
		\frac{ \bcancel{(n-n_1)!} }{n_2! \cancel{(n-n_1-n_2)!} }
		\frac{ \bcancel{(n-n_1-n_2)!} }{n_2! \cancel{(n-n_1-n_2-n_3)!} }
		\ldots
		\frac{ \bcancel{(n-(n_1+\ldots+n_{p-1})!} }{n_p! \underbrace{(n_1+\ldots+n_p)!}_{=0!} }
	\end{equation*}
	Donc, au total, il y a $\frac{n!}{n_1! n_2! \ldots n_p!}$ $p$-partages.
\end{question_kholle}

\begin{question_kholle}
	[Soit $B$ un évènement de probabilité non nulle.
	L'application $\proba_B$
	{\begin{equation}
		\proba_B \left| \begin{array}{ccc}
			\mathcal{P}(\Omega) & \mapsto & [0;1] \\
			A & \rightarrow & \displaystyle \frac{\proba( A \cap B )}{\proba(B)}
		\end{array} \right.
	\end{equation}}
	est une probabilité sur sur $\Omega$. ]
	{Une probabilité conditionnelle est une probabilité}
	
	~\\
	\begin{liste}
		\item Soit $A \in \mathcal{P}(\Omega)$ \fq. \\
		On a $\emptyset \subset A \cap B  \subset B$ donc par croissance de la probabilité, $0 = \proba(\emptyset) \leqslant \proba(A \cap B) \leqslant \proba(B)$.
		En divisant par $\proba(B) \neq 0$, $0 \leqslant \proba_B(A) \leqslant 1$. Donc $\proba_B$ est \textit{bien définie}.
		\item $\proba_B(\Omega)
		= \frac{\proba(\Omega \cup B)}{\proba(B)}
		= \frac{\proba(B)}{\proba(B)}
		= 1$
		\item Soient $(A, A') \in \mathcal{P}(\Omega)^2$ \fq* \tq* $A$ et $A'$ sont incompatibles.
		\begin{equation}
			\begin{aligned}
				\proba_B(A \sqcup A')
				&= \frac{ \proba(B \cap (A \sqcup A') ) }{ \proba(B) } \\
				&= \frac{ \proba( (B \cap A) \sqcup (B \cap A') ) }{ \proba(B) } \text{ car } (B \cap A) \cap (B \cap A') \subset A \cap A' = \emptyset \\
				&= \frac{ \proba(B \cap A) + \proba(B \cap A') }{ \proba(B) } \\
				&= \proba_B(A) + \proba_B(A')
			\end{aligned}
		\end{equation}
	\end{liste}
	Ainsi, $\proba_B$ est bien une probabilité sur $\Omega$.
\end{question_kholle}
\begin{question_kholle}{Montrer que si $A$ et $B$ sont des événements indépendants, alors $A$ et $\overline{B}$ aussi}
	Supposons donc que $0 \leqslant \proba(A) \leqslant 1$ et $0 \leqslant \proba(B) \leqslant 1$.
	D'une part, $\{ B, \bar{B} \}$ constitue un système complet donc
	
	\begin{align*}
		\proba(A) &= \proba(A \cap B) + \proba(A \cap \bar{B}) \\
		\iff \proba(A)&= \proba(A) \proba(B) + \proba(A \cap \bar{B}) \\
		\iff \proba(A) - \proba(A) \proba(B) &= \proba(A \cap \bar{B}) \\
		\iff \proba(A)(1-\proba(B))&= \proba(A \cap \bar{B}) \\
		\iff \proba(A) \proba(\bar{B})&= \proba(A \cap \bar{B})
	\end{align*}
	
	Donc $A$ et $\bar B$ sont indépendants
\end{question_kholle}
\begin{question_kholle}{Formule des probabilités composées}
	Soient $(A_{1}, \dots, A_{n})$, $n$ événements tels que $\proba\left( \bigcap_{i=1}^{n} A_{i} \right) \neq 0$
	Pour $k \in [ \! [ 2, n ] \!]$ posons 
	$$\mathcal{H}_{k} : " \proba\left( \bigcap_{i=1}^{n}A_{i} \right) = \proba(A_{1})\proba_{A_{1}}(A_{2})\proba_{A_{1} \cap A_{2}}(A_{3})\proba_{A_{1} \cap A_{2} \cap A_{3}}(A_{4})\dots \proba_{A_{1} \cap\dots \cap A_{k-1}}(A_{k})"$$
	\begin{itemize}[label=$\star$]
		
		
		\item Initialisation, $k \leftarrow 2$
		d'une part, $\bigcap_{i=1}^{n}A_{i} \subset A_{1}$, donc par croissance de $\proba$, $$0<\proba\left( \bigcap_{i=1}^{n}A_{i} \right) \leqslant \proba(A_{1})$$
		Si bien que $\proba(A_{1}) \neq 0$ donc la probabilité conditionnelle $\proba_{A_{1}}$ a un sens.
		D'où, par définition d'une probabilité conditionnelle:
		$$\proba(A_{1} \cap A_{2}) = \proba(A_{1}) \proba_{A_{1}}(A_{2})$$
		Donc $\mathcal{H}_{2}$ est vérifiée.
		
		\item Hérédité Soit $k \in [ \! [ 2, n - 1] \!]$ fixé quelconque tel que $\mathcal{H}_{k}$ est vérifiée.
		
		D'abord, remarquons que $\bigcap_{i=1}^{n}A_{i} \subset \bigcap_{i=1}^{k}A_{i}$ donc par croissance de $\proba$,
		$$0 < \proba\left( \bigcap_{i=1}^{n}A_{i} \right)\leqslant \proba\left( \bigcap_{i=1}^{k}A_{i} \right)$$
		Si bien que $\proba\left( \bigcap_{i=1}^{k} A_{i} \right) \neq 0$ donc la probabilité conditionnelle $\proba_{A_{1}\cap\dots \cap A_{k}}$  a un sens.
		
		
		\begin{align*}
			\proba\left( \bigcap_{i=1}^{k+1}A_{i} \right) 
			&= \proba\left( \left( \bigcap_{i=1}^{k}A_{i} \right) \cap A_{k+1} \right) \\
			&= \proba\left( \bigcap_{i=1}^{k}A_{i} \right)\proba_{\bigcap _{i=1}^{k}A_{i}}(A_{k+1}) \\
			&= \proba(A_{1})\proba_{A_{1}}(A_{2})\proba_{A_{1}\cap A_{2}}(A_{3})\dots \proba_{A_{1} \cap \dots \cap A_{k-1}}(A_{k}) \proba_{A_{1} \cap \dots \cap A_{k}}(A_{k+1})
		\end{align*}
		Donc $\mathcal{H}_{k+1}$ est aussi vérifiée
	\end{itemize}
\end{question_kholle}
\begin{question_kholle}{Formule des probabilités totales et formule de Bayes}
	\begin{itemize}
		\item Formule des probabilités totales
		
		Soit $(A_{1}, \dots A_{n})$ un système complet d'événements.
		Comme ils sont incompatibles
		$$\proba\left( \bigsqcup_{k=1}^{n} A_{k} \right)= \sum_{k=1}^{n}\proba(A_{k})$$
		
		Le système est de plus complet donc $\bigsqcup _{k=1}^{n} A_{k} = \Omega$. Donc $\sum_{k=1}^{n}\proba(A_{k}) = 1$.
		
		$(A_{1}, \dots , A_{n})$ sont aussi deux à deux incompatibles, donc $(B \cap A_{1}, \dots B \cap A_{n})$ aussi.
		De plus $B = B \cap \Omega = B \cap \left( \bigsqcup_{k=1}^{n}A_{k} \right) = \bigsqcup_{k=1}^{n}(B \cap A_{k})$
		Donc
		$$\proba(B)= \proba\left( \bigsqcup_{k=1}^{n}(B \cap A_{k}) \right) = \sum_{k=1}^{n}\proba(B \cap A_{k})$$
		De plus, en passant aux probabilités conditionnelles $(\proba_{A_{i}})_{1\leqslant i\leqslant n}$ on a 
		
		$$\proba(B) = \sum_{k=1}^{n}\proba(A_{k})\proba_{A_{k}}(B)$$
		
		\item Formule de Bayes
		
		Soient $A$ et $B$ deux événements de probabilité non nulle, on a alors :
		$$\proba(A)\proba_{A}(B) = \proba(A \cap B) = \proba(B)\proba_{B}(A)$$
		donc
		
		$$\proba_{A}(B) = \frac{\proba(B)\proba_{B}(A)}{\proba(A)}$$
	\end{itemize}
\end{question_kholle}
\begin{question_kholle}[{
	Soit $X$ une variable alétoire sur $\Omega$ et $g$ une fonction définie sur $X(\Omega)$.
	La loi de probabilité $Y = g(X)$ est donnée par $Y(\Omega) = g(X(\Omega))$ et 
	$$
	\forall y \in Y(\Omega), \proba_{Y}(\{ y \})= \proba(Y=y) = \sum_{x \in g^{-1}(\{ y \})}\proba(X=x)= \sum_{\substack{x \in X(\Omega)\\ g(x)=y}}\proba(X=x)
	$$
	}]{Loi d'une fonction de $X$}
	
	Utilisons le système complet $(X = x)_{x \in X(\Omega)}$ associé à la variable aléatoire $X$ et la formule des probabilités totales
	
	\begin{align*}
		\proba_{Y}(\{ y \}) = \proba(Y=y) &= \sum_{x \in X(\Omega)}\proba((Y=y) \cap (X=x)) \\
		&= \sum_{\substack{x \in X(\Omega)\\ g(x)=y}}\proba((g(X)=y) \cap (X=x)) + \sum_{\substack{x \in X(\Omega)\\ g(x)\neq y}}\proba((g(X)=y) \cap (X=x))
	\end{align*}
	
	
	Remarquons ainsi que
	\begin{itemize}[label=$\star$]
		\item Si $g(x) = y$
		$$
		\omega \in(X=x) \implies X(\omega)=x \implies g(X(\omega)) = g(x) \implies \omega \in (g(X) = y)
		$$
		De plus $(X = x) \subset (g(X)=y)$ donc $(g(X) = y) \cap(X=x)=(X=x)$
		
		\item Sinon, si $g(x) \neq y$
		$$
		\omega \in (X=x) \implies X(\omega) = x \implies g(X(\omega)) = g(x) \neq y \implies \omega \not\in (g(X) = y)
		$$
		Dans ce cas, $(g(X) = y) \cap (X = x) = \emptyset$
		
	\end{itemize}
	Ainsi, 
	
	\begin{align*}
		\proba_{y}(\{ y \}) &= \sum_{\substack{x \in X(\Omega)\\ g(x)=y}}\proba(\underbrace{ (g(X)=y) \cap (X=x) }_{ = (X=x) }) + \underbrace{ \sum_{\substack{x \in X(\Omega)\\ g(x)\neq y}}\proba((g(X)=y) \cap (X=x)) }_{ =0 }\\
		&= \sum_{\substack{x \in X(\Omega)\\ g(x)=y}}\proba(X=x) \\
		&= \sum_{x \in g^{-1}(\{ y \})}\proba(X=x)
	\end{align*}
	
	
\end{question_kholle}
\begin{question_kholle}{Si $X \geqslant 0$ presque sûrement, $\mathbf{E}(X) = 0 \iff X = 0$ presque sûrement}
	Soit $X\geqslant 0$ presque sûrement
	\begin{itemize}
		\item Supposons que $\mathbf{E}(X) = 0$
		Par hypothèse, l'évènement $(X<0)$ est négligeable donc
		
		\begin{align*}
			\mathbf{E} (X) & = \sum_{\omega \in \Omega}X(\omega)\proba(\{ \omega \}) \\
			&= \sum_{\omega \in (X = 0)} \underbrace{ X(\omega) }_{ = 0 } \proba(\{ \omega  \}) + \sum_{\omega \in (X<0)}X(\omega) \underbrace{ \proba(\{ \omega \}) }_{ =0 } + \sum_{\omega \in (X >0)} X(\omega)\proba(\{ \omega \}) \\
			&= \sum_{\omega \in (X >0)} X(\omega)\proba(\{ \omega \})
		\end{align*}
		
		Soit $\omega_{0} \in (X>0)$ fixé quelconque
		La nullité de l'espérance donne
		$$0 \leqslant X(\omega_{0})\proba(\{ \omega_{0} \}) \leqslant \sum_{\omega \in(X>0)}X(\omega) P(\{ \omega \})= \mathbf{E}(X) = 0$$
		donc $X(\omega_{0})\proba(\{ \omega_{0} \}) = 0$, or $X(\omega_{0})>0$ donc $\proba(\{ \omega_{0} \})=0$
		donc 
		$$\proba(X>0) = \sum_{\omega_{0} \in (X>0)} \proba(\{ \omega_{0} \}) = 0$$
		Donc  $(X>0)$ est négligeable, mais $(X<0)$ est négligeable aussi, donc
		$$0\leqslant \proba((X>0) \cup (X<0))\leqslant \proba(X>0) + \proba(X<0)=0$$
		donc l'évènement contraire de $(X>0) \cup (X<0)$, qui est $(X=0)$ est certain
		
		\item Supposons $X=0$ presque sûrement.
		
		\begin{align*}
			\mathbf{E}(X) &= \sum_{\omega \in \Omega}X(\omega)\proba(\{ \omega \}) \\
			&= \sum_{\omega \in (X = 0)} \underbrace{ X(\omega) }_{ =0 } \proba(\{ \omega \}) + \sum_{\omega \in (X \neq 0)} X(\omega) \underbrace{ \proba(\{ \omega \}) }_{ =0 } \\
			&= 0
		\end{align*}
		
	\end{itemize}
\end{question_kholle}
\begin{question_kholle}{Calcul de l'espérance et la variance d'une variable aléatoire suivant une loi binomiale}
	Soit $n \in \mathbb{N}^*$ et $p \in [0, 1]$
	Supposons que $X \hookrightarrow \mathcal{B}(n, p)$
	
	\begin{align*}
		\mathbb{E}(X) &= \sum_{\omega \in X(\Omega)}\omega \proba(X = \omega) \\
		& = \sum_{k=0}^{n}k\proba(X=k) \\
		&= \sum_{k=0}^{n}k \binom{n}{k} p^{k}(1-p)^{n-k} \\
		&= \sum_{k=0}^{n}k \frac{n!}{k! (n-k)!} p^{k}(1-p)^{n-k} \\
		&= n \sum_{k=1}^{n} \frac{(n-1)!}{(k-1)! ((n-1)-(k-1))!} p^{k}(1-p)^{n-k} \\
		&= n \sum_{k=1}^{n} \binom{n-1}{k-1} p^{k}(1-p)^{n-k} \\ \\
		&= n \sum_{j=0}^{n-1} \binom{n-1}{j} p^{j+1}(1-p)^{n-1-j} \\
		&= n p \sum_{j=0}^{n-1} \binom{n-1}{j} p^{j}(1-p)^{n-1-j} \\ \\
		&= np (p + (1-p))^{n-1} = np
	\end{align*}
	Pour la variance, calculons d'abord $\mathbf{E}(X^{2})$
	
	\begin{align*}
		\mathbf{E}(X^{2}) &= \sum _{k=0}^{n}k^{2} \proba(X=k^{2}) \\
		&=  \sum_{k=1}^{n} k \underbrace{ k \binom{n}{k} }_{ n \binom{n-1}{k-1} }p^{k}(1-p)^{n-k} \\
		&= n \sum_{k=1}^{n} \underbrace{ k }_{ (k-1)+1 } \binom{n-1}{k-1} p^{k}(1-p)^{n-k} \\
		&= n \sum_{k=1}^{n}(k-1)\binom{n-1}{k-1}p^{k}(1-p)^{n-k}+ n \sum_{k=1}^{n}\binom{n-1}{k-1}p^{k}(1-p)^{n-k} \\
		&= n \sum_{k=2}^{n}\underbrace{ (k-1)\binom{n-1}{k-1} }_{ (n-1)\binom{n-2}{k-2} } p^{k}(1-p)^{n-k} + n \sum_{k=1}^{n}\binom{n-1}{k-1}p^{k}(1-p)^{n-k} \\
		&= n(n-1) \underbrace{ \sum_{i=0}^{n-2}\binom{n-2}{i}p^{i+2}(1-p)^{(n-2)-i}  }_{ \text{ en posant } i = k-2 }+ n \underbrace{ \sum_{i=0}^{n-1}\binom{n-1}{i}p^{i+1}(i-p)^{(n-1)-i} }_{ \text{en posant }i=k-1 } \\
		&= n(n-1)p^{2}(p+(1-p))^{n-2} + np(p+(1-p))^{n-1} \\
		&= n(n-1)p^{2}+np \\
		&= np((n-1)p+1)
	\end{align*}
	
	D'où,
	$$
	\mathbb{V}(X) = \mathbf{E}(X^{2}) - \mathbf{E}(X)^{2}=np((n-1)p+1)- n^{2} p ^{2}=np(1-p)
	$$
	
	\textbf{Calcul alternatif de $\mathbf{E^2}$} En utilisant la formule de transfert pour $f \leftarrow (x \mapsto x(x-1))$
	
	
	\begin{align*}
		\mathbf{E}(X(X - 1)) &= \sum_{k=0}^{n}k(k-1)\proba(X=k) \\
		&= \sum_{k=0}^{n}k(k-1)\binom{n}{k}p^{k}(1-p)^{n-k} \\
		&= n(n-1) \sum_{k=2}^{n}\binom{n-2}{k-2}p^{k}(1-p)^{n-k} \\
		&= n(n-1)p^{2} \sum_{j=0}^{n-2} \binom{n-2}{j}p^{j}(1-p)^{n-2-j} \\
		&= n(n-1) p^{2} (p + (1-p))^{n-2} \\
		&= n(n-1)p^{2}
	\end{align*}
	
	Donc en remarquant que $$\mathbf{E}(X^{2}) = \mathbf{E}(X(X-1)+X) = \mathbf{E}(X(X-1))+\mathbf{E}(X)= n(n-1)p^{2} + np$$
	Donc
	$$\mathbb{V}(X) = \mathbf{E}(X^{2})- \mathbf{E}(X)^{2} = n(n-1)p^{2} + np - n^{2}p^{2} = np(1-p)$$
	
	
\end{question_kholle}
\end{document}
